\documentclass[
    iict, % Saisir le nom de l'institut rattaché
    il, % Saisir le nom de l'orientation
    %confidential, % Décommentez si le travail est confidentiel
]{heig-tb}

\usepackage{float} % pour forcer l'empalcement d'une image
% https://tex.stackexchange.com/questions/8625/force-figure-placement-in-text

\usepackage{titlesec}

\usepackage[nooldvoltagedirection,european,americaninductors]{circuitikz}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{pdfpages}

\usepackage{dirtree}
\usepackage{caption}
\usepackage{subcaption}


%https://www.overleaf.com/project/62b06d8b8257fff1974999df
\signature{va_signature.svg}

\makenomenclature
\makenoidxglossaries
\makeindex

\addbibresource{bibliography.bib}

\input{nomenclature}
\input{acronyms}
\input{glossary}
\input{meta}

%\surroundwithmdframed{minted}

%% Début du document
\begin{document}
\selectlanguage{french}
\maketitle
\frontmatter
\clearemptydoublepage

%% Requis par les dispositions générales des travaux de Bachelor
\preamble
\let\cleardoublepage\clearpage
\authentification
\let\cleardoublepage\clearpage

%% Résumé / Version abbrégée
\begin{abstract}
    \input{abstract}
\end{abstract}

%% Sommaire et tables
\listoffigures
\addcontentsline{toc}{chapter}{\listfigurename}
\listoflistings
\addcontentsline{toc}{chapter}{Liste des codes sources}

\tableofcontents

\printnomenclature
\clearemptydoublepage
\pagenumbering{arabic}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{1pt}

\fancyhead[R]{Support du langage UON}
\fancyhead[L]{\itshape\nouppercase{\leftmark}}

\renewcommand{\chaptermark}[1]{\markboth{\MakeUppercase{#1}}{}}

\renewcommand\footrulewidth{1pt}

\fancypagestyle{plain}{%
    \fancyfoot[R]{Page \thepage/\pageref{LastPage}}
}
\fancyfoot[R]{Page \thepage/\pageref{LastPage}}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\titlespacing{\chapter}{0pt}{-40pt}{20pt}

%% Contenu
\mainmatter
\chapter{Introduction}

%"Contexte" qui explique comment s'insère ce travail de Bachelor. C'est quoi UON est-ce que quelqu'un a déjà travaillé dessus, ce qu'il a fait, ce qu'il reste à faire.

Ce projet de Bachelor consiste à fournir du support pour le nouveau langage de sérialisation UON, sous VS Code.
UON est un format de sérialisation qui vise à rassembler les meilleures caractéristiques des principaux formats de sérialisation disponibles sur internet.

Ce support aura donc rôle de fournir à l'utilisateur, les outils pour lui permettre la rédaction la plus optimale d'un fichier UON dans l'éditeur VS Code.

C'est un nouveau langage complexe et une spécification détaillée existe, mais elle n'est pas terminée. Il reste maintenant à construire les éléments permettant son utilisation.
Un parser en Python a déjà été implémenté par Stéphane Selim durant son travail de bachelor "Parser for a serialization language UON" en 2020.
Ce travail s'était concentré sur trois aspects : la sérialisation et déserialisation binaire et textuelle d'objet en Python, la coercition entre les valeurs et la validation de fichiers.

Cepedendant ce travail de bachelor n'en est pas la suite directe.
Il consiste à fournir une autre composante qui est de permettre l'utilisation du langage UON dans un éditeur pour qu'il soit utilisé par un plus grand nombre.
Et à étudier les différentes approches et les mettre en oeuvre.
Les alternatives et améliorations possibles seront spécifiées quand cela est pertinent.
Le projet se veut aussi être expérimental et ouvert à la discussion.
Aucun cadre précis sur sa réalisation n'a été établi et les approches choisies sont libres tant qu'elles respectent le cahier des charges.

Ce travail se focalise uniquement sur VS code dans un premier temps. Le choix de cet éditeur sera expliqué plus tard.

Dans les chapitres suivants, nous allons :

\begin{itemize}
    \item Se renseigner concernant ce qui est fourni par VS Code pour élaborer et déployer une extension.
    \item Se renseigner sur les éléments à prendre en compte pour pouvoir fournir du support pour un langage.
    \item Choisir les aspects du langage UON à traiter dans le cadre de ce projet.
    \item Documenter et réaliser l'implémentation d'une extension, son intégration continue ainsi que ses fonctionnalités.
    \item Documenter et mettre en place les tests effectués pour les différents composants du projet.
    \item Documenter également des pistes de réflexion, les alternatives et améliorations possibles.
\end{itemize}

\let\cleardoublepage\clearpage

\chapter{Cahier des charges}

L'objectif de ce travail de Bachelor est l'élaboration d'une extension VsCode pour la prise en charge du langage de sérialisation UON.

Cela consistera dans un premier temps, à implémenter un certain nombre de fonctionnalités considérées intéressantes à avoir initialement pour un nouveau langage dans un éditeur de code.

Puis si le temps le permet, de rajouter des fonctionnalités supplémentaires.

\subsection*{VSCode Extension}
\begin{itemize}
    \item Réaliser une extension dans le code source se trouve sur Github.
    \item Publier l'extension sur \href{https://marketplace.visualstudio.com/}{le marketplace} de Visual Studio.
    \item Documenter la mise en place de l'extension et l'implémentation des fonctionnalités.
    \item Le système doit disposer d'une intégration continue.
\end{itemize}

\subsection*{Grammaire}
\begin{itemize}
    \item Indiquer le "scope" de la grammaire UON utilisée.
\end{itemize}
\textbf{Remarque} : toutes les fonctionnalités se baseront sur cette grammaire.

\subsection*{Fonctionnalités à implémenter}
\subsubsection*{Syntax highligting}
\begin{itemize}
    \item Chaque élément du texte doit être affiché dans l'éditeur avec une coloration et un style en fonction de son type.
\end{itemize}

\subsubsection*{Auto-complétion}
\begin{itemize}
    \item Des propositions de complétions doivent apparaitre pendant la saisie d'une chaine de texte si des complétions possibles existent pour une telle chaine.
\end{itemize}

\subsubsection*{Document Outlining}
\begin{itemize}
    \item Fournir le "Symbol tree" du document.
\end{itemize}

\subsubsection*{Hover Information}
\begin{itemize}
    \item Lorsque l'on passe le curseur sur un élément du texte, des informations le concernant doivent apparaitre s'ils existent pour celui-ci.
\end{itemize}

\subsection*{Si le temps le permet}

\subsubsection*{Lint}
\begin{itemize}
    \item Détecter les erreurs de syntaxe pendant la saisie et les afficher.
    \item Proposer des solutions de corrections.
\end{itemize}

\subsubsection*{Formatter (outil permettant de formater le code)}
\begin{itemize}
    \item Il est possible de formater le code au format minimal.
    \item Il est possible de formater le code au format canonique.
\end{itemize}

\subsubsection*{Converter (outil permettant de convetir du code)}
\begin{itemize}
    \item Du code JSON est capable d'être converti en UON et inversement.
    \item Du code YAML est capable d'être converti en UON et inversement.
\end{itemize}

\chapter{Pré-étude}

\section{Parsing}\label{parsing}

Le résultat de l'analyse syntaxique (parsing) nous permet d'obtenir une représentation de la structure d'un texte. Ce qui est très utile dans le cadre d'un support de langage.

Il est donc important de rappeler le mécanisme général. Car avant la phase de l'analyse syntaxique, il y a habituellement une analyse lexicale.
À cette étape, un input (du code ou texte) est décomposé en une liste tokens à l'aide d'un outil appelé \emph{lexer}.
Un token correspond à un segment de texte avec un type qui lui a été associé.
Ce sont ces tokens qui seront ensuite utilisés lors de l'analyse syntaxique.

L'analyse syntaxique consiste à regarder quelles sont les règles qui sont applicables pour une suite de tokens définie dans une grammaire.
Une grammaire étant habituellement un fichier qui décrit formellement le langage.

Le résultat de cette analyse est un \emph{parse Tree} (aussi nommé CST) qui représente toutes les dérivations conrète de notre input en fonction de la grammaire.
Elle est souvent représentée sous forme d'un arbre hiérarchique.

Généralement c'est cette structure finale qui sera interprétée ou compilée.

Ce processus peut être représenté par la figure \ref{Analayse du code source}.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/interpreter-flow.PNG}}
    \end{center}
    \caption[Analayse du code source]{\label{Analayse du code source} Analayse du code source}
\end{figure}

Dans le cadre de ce projet, cette arbre sera utilisé lorsque cela sera nécessaire.

\subsection{AST}
Le parse tree contient toutes les informations et beaucoup d'entre elles ne nous intéressent pas.
Typiquement les informations grammaticales et structurelles.
C'est pourquoi on en extrait généralement un \Gls{ast}.
Il ne contient généralement que les informations les plus pertinentes \cite{cst}.

Par exemple, le parse tree d'une opération arithmétique simple pourrait être représenté par la figure \ref{parse-tree}.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/parse-tree.PNG}}
    \end{center}
    \caption[Exemple d'un parse tree]{\label{parse-tree} Exemple d'un parse tree}
\end{figure}

Son AST pourrait être, par exemple, ce que l'on voit à la figure \ref{ast}.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=6cm]{assets/figures/ast.PNG}}
    \end{center}
    \caption[Exemple d'un AST]{\label{ast} Exemple d'un AST}
\end{figure}

\section{VS Code}
Cet éditeur a été choisi, car il fait partie des éditeurs les plus populaires. C'est également l'éditeur que j'utilise le plus personnellement et donc je suis déjà familier de son environnement.
D'autres alternatives existent comme VIM et Atom pour ne citer qu'eux. Et chacun d'eux fournit également de quoi implémenter une extension.
C'est donc un choix personnel.

Un éditeur de code est un logiciel destiné à la création et l'édition de fichiers textes. Plus généralement des fichiers statiques, comme un langage de sérialisation. Contrairement à un IDE (Environnement de développement) qui permet en plus la compilation et le debugging de programme plus complexe.

Visual Studio Code (ou VS Code) est un éditeur de code multi-plateforme, open source et gratuit.

Il est pensé pour être extensible (de L'UI à l'expérience utilisateur), presque tout peut être customisé et amélioré à travers de L'Extension API \cite{extension-api}.
On peut mentionner le theming, les fonctionnalités de langages, la publication de l'extension et les tests.

VS Code fournit des documentations détaillées sur plein de sujets : étapes pour la création d'une extension, son déploiement continue ainsi que le support de langage en lui-même.
Il est aussi facilement possible de trouver des guides, ainsi que \href{https://github.com/microsoft/vscode-extension-samples}{des exemples de codes} concernant les extensions.
Il s'agit donc évidemment de la source principale des explications données dans ce document concernant ce qui touche à cet éditeur.
La communauté est aussi très active (stack overlow, gitter, issue github, etc.).

Le code source de VS Code de Microsoft est open source, sous licence MIT permissive.
Il a été écrit en TypeScript et JavaScript. Une extension peut être également écrite dans ces deux langages.
Nous choisirons le TypeScript car VS Code recommande ce langage. Mais certaines composantes externes (langage server) peuvent être écrites dans n'importe quel langage. Tant qu'elles fournissent l'API adéquate pour communiquer.

\subsection{Command palette}\label{command-palette}

La \href{https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette}{command palette}
est le point d'accès à toutes les fonctionnalités de VS Code. Et est un élément important à connaitre.

Pour l'ouvrir il faut exécuter la combinaison suivante : "CTRL + SHIFT + p".


\subsection{Extension Anatomy}
Il y a trois concepts cruciaux à comprendre pour réaliser une extension \cite{extension-anatomy} .

\begin{itemize}
    \item \href{https://code.visualstudio.com/api/references/activation-events}{\textbf{Activation Events}}: des événements à partir desquels l'extension devient active.
    \item \href{https://code.visualstudio.com/api/references/contribution-points}{\textbf{Contribution Points}}: des déclarations statiques qui sont faites dans l'Extension Manifest \emph{package.json} pour étendre l'extension. Il s'agit d'un ensemble de déclarations JSON faites au travers du champ \emph{contributes}.
    \item \href{https://code.visualstudio.com/api/references/vscode-api}{\textbf{VS Code API}}: un ensemble d'API JavaScript qui peut être invoqué dans le code.
\end{itemize}

En général, l'extension est une combinaison de plusieurs \emph{Contribution Point} et l'utilisation de l'API pour étendre les fonctionnalités de VS Code.

\subsubsection{Extension File Structure}\label{Extension File Structure}

La squelette de base d'une extension est illustrée par la figure \ref{basic-structure}.

% https://tex.stackexchange.com/questions/98703/framebox-and-subfigure-with-dirtree-package
\begin{figure}[H] % \begin{figure}[!h]
    \centering
    \framebox[\textwidth]{%
        \begin{minipage}{0.9\textwidth}
            \dirtree{%
                .1 ..
                .2 vscode.
                .3 launch.json     // Config for launching and debugging the extension.
                .3 tasks.json      // Config for build task that compiles TypeScript.
                .2 .gitignore.
                .2 README.md.
                .2 src.
                .3 extension.ts   // Extension source code.
                .2 tsconfig.json  // TypeScript configuration.
            }
        \end{minipage}
    }
    \caption[Squelette de base d'une extension VS Code]{\label{basic-structure} Squelette de base d'une extension VS Code}
\end{figure}

\textbf{Extension Manifest} :
chaque extension doit contenir un fichier \emph{package.json}. En plus des champs propres à Node.js, on peut spécifier des scripts, des dépendances de développement et des champs spécifiques à VS Code.

\textbf{Extension Entry File} :
il s'agit du fichier principal de l'extension (Extension.js).

De base, il contient deux fonctions : \emph{activate} et \emph{deactivate}.
La fonction activate est exécutée à l'activation de notre extension par un \emph{Activation Event}. On initialisera ici notre extension.
La fonction \emph{deactivate} est exécutée lorsque l'application devient inactive et sert principalement à nettoyer le code avant la désactivation de l'extension

\textbf{Remarque} : le niveau de customisation est assez élevé. La seule limitation indiquée et qu'il n'est pas possible d'accéder au \Gls{dom} de l'éditeur.

Pour se renseigner davantage sur l'élaboration d'une extension, la page \href{https://code.visualstudio.com/api}{suivante} peut être consultée.
Concernant les fonctionnalités générales de l'éditeur VS Code, les informations se trouve sur \href{https://code.visualstudio.com/docs}{la documentation officielle}.

\section{Support de langage}
VS Code fournit la possibilité d'ajouter du support pour un nouveau langage de programmation au travers d'implémentation de fonctionnalités. Ces fonctionnalités peuvent être classées en deux catégories détaillées
dans les sections \hyperref[Declarative language features]{Declarative language features} et \hyperref[Programmatic language features]{Programmatic language features}.

\subsection{Declarative language features}\label{Declarative language features}
Elles ajoutent un support d'édition de texte de base pour un langage de programmation.
Par exemple, les éléments suivants :

\begin{itemize}
    \item syntax highlighting
    \item snippet completion
    \item bracket matching
    \item bracket autoclosing
    \item bracket autosurrounding
    \item comment toggling
    \item auto indentation
    \item folding (by markers)
\end{itemize}

Il s'agit de fonctionnalités implémentées à l'aide de fichier de configuration.

Puis, elles doivent être enregistrées comme \emph{Contribute Point}.

\subsection{Programmatic language features}\label{Programmatic language features}
Il s'agit de fonctionnalité plus riche et plus complexe à mettre en place.
Par exemple, les éléments suivants :

\begin{itemize}
    \item hover information
    \item jump to Definition
    \item bracket matching
    \item error checking
    \item bracket autosurrounding
    \item auto completion
\end{itemize}

Il y a deux approches possibles pour les implémenter. Elles sont détaillées ci-dessous.

\subsubsection{VS Code API (Direct implementation)}
La première solution est d'utiliser \href{https://code.visualstudio.com/api/references/vscode-api#languages}{l'API de VS Code} pour les fonctionnalités de langage.
Cette API permet d'implémenter directement ces fonctionnalités sans devoir mettre en place l'infrastructure les supportant.

Beaucoup de fonctionnalités se font en s'inscrivant à des providers depuis notre application.
L'éditeur de code fera ensuite les requêtes à ces providers lorsque cela sera nécessaire.

\subsection{Language server}

La seconde solution est de fournir nous-mêmes ces méthodes en respectant la spécification LSP \cite{lsp-specification} au travers d'un langage serveur.
Les avantages souvent mentionnés de cette approche sont que le langage server peut être écrit avec le langage que l'on souhaite et
que cela permet aussi à d'autres éditeurs de texte compatibles avec le langage server d'utiliser ces fonctionnalités sans devoir les implémenter de nouveau.

Pour être utilisé sur VS Code, un langage server à deux parties :
\begin{itemize}
    \item \textbf{Un client} : c'est une extension écrite en Javascript ou Typescript qui à accès à tous les endpoints de VS Code.
    \item \textbf{Langage server} : un outil d'analyse linguistique fonctionnant dans un processus séparé.
\end{itemize}

\vspace{\parskip}

Le client et le serveur communiquent à l'aide du protocole LSP (pour "language server") dès que des informations devraient être fournies à l'éditeur.

L'implémentation d'un tel serveur peut être libre en respect avec la spécification, mais des implémentations existent déjà. Telle que \emph{LSP4} écrit en Java ou \emph{VS Code-languageserver-node}
écrit en Typescript qui n'est pas exclusivement réservé à VS Code comme son nom pourrait l'indiquer.

Un langage server peut être utilisé sur d'autre éditeur compatible, mais le client devra être implémenté de nouveau pour chaque éditeur.
Cette approche est aussi plus complexe à implémenter.

\subsection{API vs LSP}\label{api vs lsp}

La figure \ref{API de VS Code vs méthodes LSP} montre la correspondance des méthodes entre la première et seconde approche.

Cette liste est non exhaustive, mais permet de montrer l'équivalence des deux solutions.

\begin{figure}[!ht]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/api-vscode.png}}
    \end{center}
    \caption[API de VS Code vs méthodes LSP]{\label{API de VS Code vs méthodes LSP} API de VS Code vs méthodes LSP}
\end{figure}

\subsection{Fault tolerant Parser}
Pour fournir des fonctionnalités de langue, un point souvent mentionné est l'utilisation d'un parser.
Car l'arbre généré par celui-ci contient des informations concernant la structure du document utiles pour certaines fonctionnalités.

Des outils permettant la génération d'un parser existent à partir d'une grammaire et seront privilégiés.

Cependant dans le cadre de son utilisation dans un éditeur ou IDE, il devrait être "fault tolerant".
Car la plupart du temps, le code dans l'éditeur est incomplet et syntaxiquement incorrect.
Mais l'on s'attend tout de même que certaines fonctionnalités continuent à fonctionner (ex : la outline view).

Le parseur doit donc pouvoir générer au mieux un parse tree qui à du sens depuis du code erroné.

On pourrait également se demander s'il est possible de se passer d'une telle strucutre et de n'utiliser à la place uniquement des expressions régulières.
Cependant le problème majeur avec cette approche et que les imbrications deviennent extrêmement difficile à gérer. \cite{antlr-mega-tutorial}

\section{Choix technologiques}
Nous allons maintenant mentionner les technologies et approches choisies et expliquer les raisons.

\subsection{Direct implementation}
L'implémentation d'un langage server n'étant pas une priorité et pour se concentrer sur la réalisation des fonctionnalités, la solution de contacter directement l'API de VS Code a été privilégiée.
Les fonctionnalités riches (voir section \ref{Programmatic language features}) pouvant toujours être déplacées dans un langage server par la suite.

Beaucoup de possibilités existent comme mentionné au point \hyperref[api vs lsp]{API vs LSP}, les providers utilisés sont les suivants :
\begin{itemize}
    \item \textbf{registerCompletionItemProvider} : pour afficher des suggestions de complétions.
    \item \textbf{registerHoverProvider} : pour gérer le hover.
    \item \textbf{registerDocumentSymbolProvider} : pour afficher les éléments dans la Outline View.
\end{itemize}

\subsection{ANTLR}

\Gls{antlr} est un générateur de parser qui est actuellement dans sa quatrième version.
Il a été créé par Terence Parr à l'Université de San Francisco et est très utilisé, autant dans le monde académique que professionnel \cite{antlr}.

Le générateur prend en entrée une grammaire. La grammaire d'ANTLR est décrite en utilisant la notation \Gls{ebnf}.

ANTLR est composé de deux parties principales. Il y a l'outil en java qui permet de générer le lexer et le parser. Il s'agit donc du code permettant de reconnaitre un langage comme mentionné dans la section \ref{parsing}.
Ainsi que l'environnement d'exécution (runtime) qui permet leur exécution.

Il y a seulement un outil qui permet de générer le code dans les langages cibles. Et il est écrit en Java.
Pour pouvoir utiliser cet outil dans un environnement autre que Java, il est nécessaire de l'utiliser dans un runtime (environnement d'exécution) propre au langage souhaité.

Le runtime ainsi que les fichiers générés doivent être dans le même langage. La liste se trouve \href{https://github.com/antlr/antlr4/blob/master/doc/targets.md}{ici}.
Malheureusement, le typscript n'en fait pas partie.

\subsubsection{antlr4ts}
C'est \emph{antlr4ts} qui sera finalement choisi. C'est un outil permettant d'utiliser ANTLR dans environnement d'exécution en typescript et donc parfaitement adapté pour VS Code.
Il a été conçu par Sam Harwell indépendamment de l'organisation ANTLR.

% https://github.com/tunnelvisionlabs/antlr4/blob/master/doc/targets.md

C'est un outil qui semble populaire est très utilisé.

La figure \ref{Utilisation du parser d'ANTLR} illustre un exemple simple d'utilisation.

\begin{listing}[!ht]
    \begin{minted}{typescript}
        let inputStream = new ANTLRInputStream("var c = a + b()");
        let lexer = new ExprLexer(inputStream);
        let tokenStream = new CommonTokenStream(lexer);

        let parser = new ExprParser(tokenStream);
        let errorListener = new ErrorListener();
        parser.addErrorListener(errorListener);
        let tree = parser.expression();
    \end{minted}
    \caption{Utilisation du parser d'ANTLR}
    \label{Utilisation du parser d'ANTLR}
\end{listing}

On remarque que le code suit le processus d'écrit dans la section \ref{parsing}.

\subsubsection{ATN}

La version 4 de ANTLR a comme avantage sur son ancienne version (ANTLR3) d'avoir la structure de la grammaire directement disponible dans le parser via
un mécanisme de machine à état \Gls{atn}.

La figure \ref{ATN d'une séquence} montre un atn d'une règle de parser.

\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=12cm]{assets/figures/seq_ATN.png}
    \end{center}
    \caption[ATN d'une séquence]{\label{ATN d'une séquence} ATN d'une séquence}
\end{figure}

Cela permet de comprendre que le parser passe par plusieurs états lors d'une analyse syntaxique.
Cette image a été généré en utilisant le plugin \href{https://marketplace.visualstudio.com/items?itemName=mike-lischke.vscode-antlr4}{ANTLR4 grammar syntax support}.

\subsubsection{Gestion des erreurs}

ANTLR est capable de signaler des erreurs et se rétablir d'erreurs plus graves qui pourraient faire échouer le processus de parsing.
ANTLR définit l'interface \emph{ANTLRErrorStrategy} pour résoudre ce problème.
Elle sert à définir une stratégie concernant les erreurs rencontrées.

On peut distinguer trois types d'erreurs \cite{ANTLRErrorStrategy} :
\begin{itemize}
    \item L'analyseur syntaxique n'a pas pu déterminer le chemin à prendre dans l'ATN (aucune des alternatives disponibles ne pouvait correspondre).
    \item L'input actuel ne correspond pas ce que nous attendons.
    \item Un prédicat évalué comme faux.
\end{itemize}

Par défaut c'est l'implémentation \emph{DefaultErrorStrategy} qui est appliquée et qui consiste à supprimer un token qui poserait problème ou en rajouter un s'il manquerait, avant de
se resynchronisation à l'ATN et de continuer le parsing. Cette resynchronisation permet de continuer dans la règle courante que traverse le parser. \cite{antlrs-error-handling-strategy}

S'il ANTLR n'a pas pu se rétablir et qu'il a encore des erreurs, il va consommer tout les tokens qui suit celui qui a posé problème.
Puis, ce mécanisme s'arrêtera quand il réussira à trouver un token qui lui permettra de quitter la règle de parser courante sans la valider, mais permettant de continuer le processus de parsing.
Au pire des cas il s'agira d'un token de retour à la ligne \Gls{eof}.

C'est grâce à ce mécanisme qui fait que si nous utilisons le \emph{DefaultErrorStrategy} nous obtiendrons un parse tree dans tous les cas.
Le résultat du parsing n'a pas besoin d'être correct. Il est possible de travailler avec du code incomplet ou malformé, ce qui est relativement fréquent dans un éditeur.

Malheureusement, je n'ai pas pu explorer plus en profondeur ce mécanisme complexe de resynchronisation.
Cela pourrait se révéler intéressant pour tolérer plus d'erreurs au lieu d'une seul pour l'instant.

\section{UON}\label{UON}
UON est essentiellement un langage de sérialisation qui est un superset de JSON et un superset partiel de YAML.
Il cherche à regrouper les meilleures caractéristiques des formats de sérialisation les plus utilisés sur internet en un seul format.
Il fournit également des fonctionnalités supplémentaires utiles pour augmenter l'interopérabilité entre différents types de dispositifs, valider les données ainsi que pour diminuer le payload.

Uniquement les points essentiels dans le cadre d'un support de langage seront abordés.

La spécification se trouve sur le lien suivant : \url{https://github.com/uon-language/specification/blob/master/spec.md}.

\subsection{Format de sérialisation}

Lorsqu'on sérialise des données, elles se retrouvent soit sous forme binaire ou soit textuelle.
Ce projet se focalisera naturellement uniquement sur le format textuel, car c'est le seul format, humainement interprétable, que l'on manipulera dans un éditeur.
Le rôle d'un format de sérialisation et de pouvoir représenter des données sous une autre forme pour être ensuite manipulé.

UON est un langage comprenant plusieurs composantes est particularités qui ne peuvent pas toutes être traité dans un simple éditeur de texte.
Un éditeur ne traite généralement que des fichiers statiques.
Les étapes de sérialisation et désérialisation ne se feront donc pas à notre niveau.
Car pour pouvoir profiter de ces fonctionnalités, cela nécessite d'avoir un environnement de programmation (ex : Python).

\subsection{Pourquoi ?}
Son rôle est d'être utilisé dans l'industrie 4.0. Plus particulièrement dans la communication \Gls{m2m} ainsi que pour l'\Gls{IoT}.
Ces communications nécessitent souvent de communiquer entre de petits appareils dont la puissance de calcul est très limitée.

\subsection{Communication}
Lorsque deux appareils veulent échanger des informations en UON, ils disposent de deux canaux de communication :
\begin{itemize}
    \item Un canal de communication en ligne utilisé pour la transmission des données de contenu (payload). Les données peuvent être représentées sous forme humaine et binaire.
    \item Un canal de communication contractuel utilisé pour l'accord sur la description des données (schema).
\end{itemize}

\vspace{\parskip}

À la figure \ref{data-exchange}, on a à gauche deux dispositifs : un capteur de température à faible consommation
et une puissante passerelle domotique. Pour réduire la taille du payload, les deux dispositifs peuvent convenir d'un schéma qui décrit le format des données.

\begin{figure}[H]
    \begin{center}
        \frame{\includegraphics[width=15cm]{assets/figures/data-exchange.png}}
    \end{center}
    \caption[Échange de données en UON]{\label{data-exchange}Échange de données en UON}
\end{figure}

À droite on affine le schéma. Pour réduire davantage les données transmises.
Par exemple, en disant que la température exprimée avec un nombre est maintenant une valeur non signée de huit bits exprimée en degrés Celsius.

\subsection{Design}
UON est complètement conforme avec JSON et partiellement avec YAML.

Il peut être décomposé au travers de 4 niveaux de complexités :
\begin{itemize}
    \item \textbf{UON:0} est entièrement compatible avec JSON en ce qui concerne le RFC8259.
    \item \textbf{UON:1} est partiellement conforme à YAML.
    \item \textbf{UON:2} fournit des propriétés de type, la coercition et les chaînes de caractères multilignes.
    \item \textbf{UON:3} offre des types et des références riches.
\end{itemize}

\subsection{Modèle d'information}

Le contenu peut être soit un scalaire, soit une collection d'éléments, comme montré à la figure \ref{information-model}.

\begin{figure}[H]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/information-model.png}}
    \end{center}
    \caption[Le modèle d'information]{\label{information-model}Le modèle d'information}
\end{figure}

Chaque élément qui compose l'arbre d'objets UON est un \emph{Type} qui peut avoir plusieurs propriétés qui lui sont attachées.

\subsection{Types}
Une caractéristique importante en UON est que tout valeur appartient à un type. Les types sont organisés en différentes catégories.
Ces catégories sont illustrées par la figure \ref{type-base}.

\begin{figure}[H]
    \begin{center}
        \frame{\includegraphics[width=12cm]{assets/figures/type-base.png}}
    \end{center}
    \caption[Les types de base en UON]{\label{type-base}Les types de base en UON}
\end{figure}

La liste complète des types supportés se trouve dans la spécification.

\subsection{Propriétés}\label{properties}
C'est là que UON devient plus complexe que les autres langages de sérialisation.
Il offre des propriétés au type permettant de contraindre, documenter ou affiner une valeur. Les propriétés sont associées à un type et pas directement à une valeur.

3 types de propriétés existent :
\begin{itemize}
    \item \textbf{Presentation properties} : elles influencent la présentation d'une valeur.
    \item \textbf{Validation propeties} : elles sont utilisées pour valider, contraindre et décrire un fichier UON.
    \item \textbf{Application properties} : elles ne peuvent être lues que depuis UON en utilisant le type !prop. Elles sont accessibles depuis l'application (Python, JavaScript, etc.). Elles sont utilisées pour générer un fichier sérialisé (binaire, ou UON), mais elles ne sont jamais explicitement transmises.
\end{itemize}

\subsection{Syntaxe et grammaire}

Tout est une valeur avec un type et des propriétés associés. Une clé est une valeur de type \emph{!key}. Les types peuvent être imbriqués.

\begin{figure}[H]
    \begin{center}
        \frame{\includegraphics[width=12cm]{assets/figures/syntax.png}}
    \end{center}
    \caption[Règle de syntaxe en UON]{\label{syntax} UON syntax}
\end{figure}

\subsection{Validation}
Dans de telles communications, la validation des données transmises entre des machines dont la puissance de calcul est très limitée est un atout majeur.
Une des propriétés uniques de UON est l'usage de schéma directement intégré dans le langage.

Ce schéma est une structure qui décrit comment un fichier ou type doit être représenté.

UON propose 4 différents types de stratégies pour le définir.

\begin{itemize}
    \item \textbf{Embedded schema} : inclus dans le dataset, les informations sont mélangé avec les contraintes.
    \item \textbf{Included schema} : inclus dans le dataset, les informations et contraintes sont séparés.
    \item \textbf{Linked schema} : inclus dans le dataset en précisant la signature du schéma.
    \item \textbf{Separated schema} : n'est pas inclus dans le dataset, les informations et contraintes sont séparés et non liés.
\end{itemize}

Chaque solution présente des avantages et des inconvénients.

\subsection{Liens}
Le dépôt du projet se trouve à l'adress suivante : \url{https://github.com/uon-language/specification}

\chapter{Scope de la grammaire}\label{grammar scope}
Pour pouvoir proposer du support pour un nouveau langage, il est nécessaire de savoir sur quoi celui-ci portera exactement.
Il est donc important de préciser sur quels éléments du langage nous nous focaliserons.

Une spécification d'UON existe, elle couvre énormément d'aspects. Cependant, tout n'est pas détaillé et des points sont sujets à interprétation.
C'est un langage complexe et qui pourrait encore, être amener à évoluer.

Heureusement, une grammaire concrète avait déjà été écrite en Lark par l'ancien élève Stéphane Selim.
Cette grammaire se trouve à l'adresse suivante : \url{https://github.com/uon-language/uon-parser/tree/master/grammar}.

Elle a été convertie en format ANTLR. Des modifications ont été apportées et seront expliquées par la suite à la section \ref{modification}.

Les fonctionnalités se baseront sur celle-ci. Dans le but de rester cohérents entre elles.

La version finale est écrite dans le fichier \emph{UON.g4}.

\section{Résumé}
Pour commencer, il est nécessaire de préciser ce que permet cette grammaire initialement.

Ci-dessous se trouveront des exemples de ce qu'il est possible de faire.

\textbf{Remarque} : La grammaire qui a été adaptée dans le format d'ANTLR sera utilisée pour illustrer nos exemples.

\subsection{Éléments racines}

Un fichier UON peut contenir comme élément racine trois structures différentes : les collections en json et yaml, ainsi qu'un schéma.

\begin{listing}[!ht]
    \begin{minted}{antlr}
    uon: root_value;

    root_value: json_collection | yaml_collection | schema;

    json_collection: json_map | json_seq;

    yaml_collection: yaml_map | yaml_seq;
    \end{minted}
    \caption{Valeur racine de la grammaire UON}
    \label{uon-root-value}
\end{listing}

\subsection{Collections}
Ces collections sont des mapping et séquences dans un format proche de json ou de yaml.

UON est par nature proche du format JSON. Mais UON étant "YAML compliant" à un certain degré (reduced set). Il est naturel de penser qu'un format YAML soit supporté, même si son utilisation est moins recommandé que son alternative.

Ces collections peuvent être imbriqué seulement par format, on ne peut pas avoir dans un fichier une collection en format json et yaml en même temps.

\begin{lstlisting}[frame=single,caption={Exemple d'un mapping en format JSON},captionpos=b,label={json-map}]
    {
        "name" ( description : a12 ) :  !str(optional: False) paul,
        age: !float !float128 !float32 +12.2 C,
        hobbys ( description : "what i like the most" ) : !seq [
            "cinema", "sport", "etc..."],
        license  :  !bool  true,
        car:   !bool false,
        sensor ( optional : false , description : ok ) : !map {
            type: "temperature",
            parameters1: !seq [ 1, 2,3,4,5]}
    }
\end{lstlisting}

\begin{lstlisting}[frame=single,caption={Exemple d'un mapping en format YAML},captionpos=b,label={yaml-map}]
key :
    -  "text"
    - !float32 12
    -
         - tes
key2 : -23
\end{lstlisting}

\begin{lstlisting}[frame=single,caption={Exemple d'une séquence en format JSON},captionpos=b,label={json-seq}]
[1,2, paul, jean, { key : 10}]
\end{lstlisting}

\begin{lstlisting}[frame=single,caption={Exemple d'une séquence en format YAML},captionpos=b,label={yaml-seq}]
    -
        - !float !float128(comment : "temperature") 12 C
        - !str text
\end{lstlisting}

\subsection{Schéma}
On peut définir également un schéma de validation.

Ce schéma doit se trouver dans un autre document que les deux structures ci-dessus. Cela correspond à la stratégie \emph{Separated schema}.

Cette séparation permet d'éviter des ambiguïtés. On incite comme ça à l'utilisateur à séparer la logique entre différents fichiers.

\begin{lstlisting}[frame=single,caption={Exemple d'un schéma de validation},captionpos=b,label={schema}]
    !!mySensor: !schema ( description : "provide great infos") {
        power( optional: true) : !float(min : 5, max : 50),
        name : !str( max: 6, min :12),
    }
\end{lstlisting}

\subsection{Propriétés}
Les clés des mapping peuvent posséder des propriétés de présentation.
Les types dans un schéma de validation peuvent posséder des propriétés de validation.

\subsection{Terminaux}
Les terminaux traité par la grammaire couvrent certains types, des nombres, des strings et des unités (temperature, kilogram, etc...).

\section{Modification}\label{modification}

La structure de départ a été gardé, car cette grammaire regroupe déjà des éléments importants du langage UON.

Cependant quelques points concernant la structure générale et les règles concernant les terminaux (string, nombre, etc.) ont été modifiés.

Pour commencer il a fallu convertir la grammaire à la main car aucun convertisseur n'existe. Heureusement, ANTLR et Lark se basent sur une notation EBNF.
Ce qui fait que la structure de la grammaire reste relativement semblable.

Le seul point de divergence a été sur l'utilisation d'expressions régulières car la syntaxe de celle-ci varie légèrement. Et sur la gestion de l'indentation.

Concernant la structure d'un fichier de grammaire ANTLR \emph{.g4}, il y a quelques subtilités à connaitre :

\begin{itemize}
    \item Le fichier doit comporter le même nom que la règle de départ.
    \item Les règles de lexer définissent comment une chaine de texte doit être tokenifier. Elles doivent être écrites en majuscule.
    \item Les règles de parser définissent comment une suite de tokens sera interprétée. Elles doivent être écrites en minuscule.
\end{itemize}

\subsection{YAML}

La grammaire permet deux notations différentes pour les collections : une proche de json et l'autre de yaml.

En JSON les retours à la ligne et indentations ne sont pas nécessaires à la compréhension du langage, contrairement à YAML.
Le chapitre suivant explique ce qui a été fait pour pouvoir gérer ce problème.

\subsubsection{L'indentation}

Contrairement au parser lark, la gestion des indentations n'est pas gérée automatiquement à l'aide d'une librairie externe.
Ce mécanisme doit malheureusement être traité par nos soins.

Mais avant de résoudre ce problème, il est nécessaire de rappeler que notre grammaire n'est pas uniforme concernant la gestion des retours à la ligne.
Et c'est là où se trouve la vraie problématique.
Le format proche de JSON ne tient pas compte des indentations contrairement à celle de YAML.
Et la gestion des indentations nécessite de prendre en compte les retours à la ligne.

Il y a donc trois solutions qui ont été envisagées :

La première est de rajouter des tokens de retour à la ligne ou il serait jugé possible de les avoir dans les règles propre au format JSON, et de les rendre optionnels.
Mais ce n'est pas très propre et on risque d'en placer énormément.

Le deuxième serait d'ignorer tout simplement les erreurs.  C'est-à-dire que si on à un token de retour à la ligne qui ne devrait pas être là, alors l'ignorer tout simplement.
On pourrait penser ce mécanisme simple se révélerait efficace. Cependant cela pose problème pour le moteur de complétion qui a besoin d'avoir un input correct, comme expliqué dans la section \ref{error handle}

La troisième solution trouvée et celle actuellement utilisée. Elle consiste à prendre par défaut les retours à la ligne et de les ignorer dès qu'on détecte les tokens
qui ne sont jamais utilisés dans un format YAML (les accolades et virgules).

Dès lors que les retours à la ligne existent, nous pouvons maintenant placer les tokens INDENT et DEDENT depuis le lexer
pour pouvoir gérer les imbrications correctement.

Pour ce faire, à chaque fois qu'un espace qu'un retour à la ligne est détecté par le lexer, nous prenons compte de la taille de l'espacement qu'il contient (ex : "\\r\\n    ")
Cet espacement nous permettra de savoir si l'on doit rajouter des INDENT ou DEDENT.

Il est possible de placer le code permettant ce mécanisme dans le fichier de grammaire, en l'écrivant dans le block \emph{lexer:@member{}}.
Lors de la génération, le code est placé dans le lexer.

Le code permettant est une adaptation d'une solution déjà existante pour du YAML dans une grammaire ANTLR, elle se trouve \href{https://github.com/umaranis/FastYaml}{ici}

Il est maintenant possible d'avoir par exemple la règle suivante :

\begin{listing}[!ht]
    \begin{minted}{antlr}
        yaml_seq : INDENT (SEQUENCE_TYPE)? seq_item+ DEDENT;
    \end{minted}
    \caption{Règle de grammaire gérant les indentations}
    \label{ex-indentation-rule}
\end{listing}

Les tokens INDENT et DEDENT seront créés donc par le lexer.

\subsubsection{Minus}

Le caractère "-" est utilisé pour définir une séquence en format yaml.
Il a fallu forcer le caractère directement dans l'expression régulière définissant un nombre.

\begin{listing}[!ht]
    \begin{minted}{antlr}
    NUMBER:
        (('+'|'-')? INT ('.' [0-9]*)? EXP?) // +1.e2, 1234, 1234.5
        | '.' [0-9]+ EXP? // -.2e3
        | (('+'|'-')? '0' [xX] HEX+) // 0x12345678
        | (('+'|'-')? '0' [oO] HEX+) ; // 0o12345678

    NUMERIC_LITERAL: 'inf' | 'nan' | '-inf' | '-nan' | '+inf' | '+nan';
    \end{minted}
    \caption{Règle de grammaire concernant les nombres}
    \label{number-rule}
\end{listing}

\subsection{String}

\begin{listing}[!ht]
    \begin{minted}{antlr}
        string :
        literal
        | QUOTED_STRING
        | UNQUOTED_STRING;
    \end{minted}
    \caption{Règle de grammaire concernant les strings}
    \label{string-rule}
\end{listing}

Chaque valeur textuelle peut écrite directement ou être placé entre guillemets ou accolades.

C'est aussi le cas pour la clé d'une map.
Cela se rapproche par exemple, un peu plus de ce que fait \href{https://json5.org/}{JSON5}.
Cela permet d'avoir des clés qui peuvent contenir exactement ce que l'on veut.
Car encapsuler entre des caractères spécifiques, il n'y a pas d'ambiguïtés possibles. Alors que sans, on est contraint de limiter la forme de la clé.

Par contre un cas spécial a dû être traité. Si l'on veut pouvoir utiliser directement un mot sans guillemet, mais qu'il existe dans la grammaire.
C'est pourquoi nous avons la règle de parser \emph{literal} pour pouvoir utiliser un de ces tokens comme string.

Par exemple :

\begin{lstlisting}[frame=single]
    {
        temperature : 10 C
    }
\end{lstlisting}

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=6cm]{assets/figures/literal-key-uon.PNG}}
    \end{center}
    \caption[Utilisation d'un token de type literal comme clé]{\label{literal-key-uon}Utilisation d'un token de type literal comme clé}
\end{figure}

Sur la figure \ref{literal-key-uon}, on voit que la clé et bien un token de type \emph{literal}.
Ce token \emph{temperature} existe comme token car c'est une valeur utilisée comme quantité, mais si on ne précise pas explicitement que l'on souhaite avoir cette valeur
comme possible clé alors on aura une erreur.

\subsubsection{Unquoted string}

\begin{listing}[H]
    \begin{minted}{antlr}
    UNQUOTED_STRING: IDENTIFIER+;

    IDENTIFIER
        : [\p{L}] //matches a single code point in the category "letter"
        | [\p{M}] //a character intended to be combined with another character
                  //(e.g. accents, umlauts, enclosing boxes, etc.)
        | [\p{N}] //matches any kind of numeric character in any script.
        | [\p{Pc}]//punctuation character such as an underscore that connects words.
        | '!';
    \end{minted}
    \caption{Règle concernant les strings sans guillement}
    \label{unquoted-rule}
\end{listing}

\subsubsection{Quoted string}

\begin{listing}[H]
    \begin{minted}{antlr}
    QUOTED_STRING:
    '"' DOUBLE_QUOTE_CHAR* '"'
    | '"''"''"' DOUBLE_QUOTE_CHAR* '"''"''"'
    | '\'' SINGLE_QUOTE_CHAR* '\'';

    fragment DOUBLE_QUOTE_CHAR: ~["\\\r\n];

    fragment MULTILINE_QUOTE_CHAR: ~[];

    fragment SINGLE_QUOTE_CHAR: ~['\\\r\n];
    \end{minted}
    \caption{Règle concernant les strings avec guillement}
    \label{quoted-rule}
\end{listing}

N'importe quelle caractère peut se trouver entre guillement sauf un autre guillement et pareillement pour les apostrophes.
Il est possible d'avoir des multilines strings.

\subsection{Propriétés}

Des propriétés de présentation pour les types ont été rajoutées.

\begin{listing}[!ht]
    \begin{minted}{antlr}
    types_properties: OPEN_PAR(types_propertie (COMMA types_propertie)*)?CLOSE_PAR;
    types_propertie: comment | description | optional;
    comment: COMMENT COLON string;
    string_scalar: (STR_TYPE (types_properties)?)? string;
    \end{minted}
    \caption{Règle concernant les propriétés de présentation des types}
    \label{types_properties-rule}
\end{listing}

\section{Debugging}

Il n'est pas simple de debugger une grammaire sans aucun outil ou seulement à l'aide d'un terminal, cela peut même se révéler extrêmement difficile.
Heureusement, il existe une \href{https://plugins.jetbrains.com/plugin/7358-antlr-v4}{extension} sur intellij qui nous permet de visualiser en temps réel, le parse tree du texte que l'on est en train de saisir.

\chapter{Implémentation}
Dans ce chapitre, nous allons explorer les aspects techniques concernant l'implémentation de fonctionnalités. Nous allons voir également plus en détail les outils utilisés ainsi que le déploiement de l'extension sur le marketplace.
Ce travail reflète uniquement mon approche personnelle sur la matière. Et ne devrait pas être considérée comme l'unique manière de procéder.

\section{Code}

Par rapport à la figure \ref{basic-structure}
les points suivants au été rajouté à la figure \ref{project-structure}

\textbf{Remarque} : Les "..." signifie qu'il y a des sous fichiers et/ou répertoire mais ont été ignoré pous ne pas surcharger
la figure.

\begin{figure}[!h]
    \centering
    \framebox[\textwidth]{%
        \begin{minipage}{0.9\textwidth}
            \dirtree{%
                .1 ..
                .2 github.
                .3 workflows.
                .4 pipeline.yml.
                .2 vscode.
                .3 launch.json.
                .3 tasks.json.
                .2 .gitignore.
                .2 README.md.
                .2 src.
                .3 completion.
                .4 completion.ts.
                .3 error.
                .4 ErrorListener.ts.
                .4 UonCompletionErrorStrategy.ts.
                .3 generated.
                .4 ....
                .3 grammar.
                .4 .antlr.
                .4 UON.g4.
                .3 outline.
                .4 UonASTVisitor.ts.
                .4 UONDocumentSymbolProvider.ts.
                .3 test.
                .4 ....
                .3 extension.ts.
                .2 tsconfig.json.
            }
        \end{minipage}
    }
    \caption{Structure du projet}
    \label{project-structure}
\end{figure}

Le code de du projet est disponible sur le dépôt à l'adresse suivante : \url{https://github.com/vitorva/vscode-uon}.

\section{Extension}
Nous allons voir maintenant, l'essentiel de la procédure pour créer et publier son extension.

La procédure détaillée se trouve \href{https://code.visualstudio.com/api/get-started/your-first-extension
}{ici}

\subsection{Génération de l'extension}

VS Code permet de générer un squelette (boilerplate) pour la réalisation d'une extension.
Pour ce faire il faut avoir Node.js et Git installés.

Ceci dans le but d'installer Yeoman et VS Code Extension Generator.

\href{https://yeoman.io/}{Yeoman} est un outil permettant de créer un générateur et de l'exécuter dans un environnement adapté.
\href{https://www.npmjs.com/package/generator-code}{VS Code Extension Generator} et le générateur implémenté par VS Code pour générer la structure de l'extension.

La commande pour l'installation est la suivante :

% ,caption={generator-code},label={generator-code}
\begin{lstlisting}[frame=single]
npm install -g yo generator-code
\end{lstlisting}

Puis il suffit de lancer le générateur :

\begin{lstlisting}[frame=single]
yo code
\end{lstlisting}

Et de compléter ce qui est attendu dans le terminal pour créer notre extension.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=8cm]{assets/figures/yo-code.png}
    \end{center}
    \caption[Générateur Yo Code]{\label{yo-code}Générateur Yo Code}
\end{figure}

Cela créera l'arborescence mentionnée précédemment au point \hyperref[Extension File Structure]{Extension File Structure}.

\subsection{Lancement de l'extension}
On peut lancer notre application en mode debug avec la touche F5. Cela va ouvrir une nouvelle fenêtre VS Code qui va contenir notre extension.

Par défaut, l'application générée vient avec du code permettant d'afficher un message de bienvenu. On l'affiche au travers du command palette. On peut s'assurer comme cela que l'implémentation de base fonctionne correctement.
On constate donc que par défaut, l'application doit être lancée au travers d'une commande, mais pour notre cas il sera plus intéressant que VS Code nous propose les fonctionnalités à l'ouverture d'un fichier UON.
Pour cela, il faut modifier \emph{l'activation events} dans le fichier package.json par :
\begin{lstlisting}[frame=single]
    "activateEvents" : [
	"onLanguage:uon"
    ]
\end{lstlisting}

\subsection{Déploiement sur le Marketplace}

% https://code.visualstudio.com/api/working-with-extensions/publishing-extension
% https://code.visualstudio.com/api/working-with-extensions/continuous-integration
% https://marketplace.visualstudio.com/manage/publishers/test2publish?noPrompt=true
% https://dev.azure.com/Stev03/_usersSettings/tokens

% delete publisher : https://stackoverflow.com/questions/54179509/how-do-you-delete-a-visual-studio-marketplace-publisher

% https://github.com/Microsoft/vscode-vsce/issues/249
% https://docs.npmjs.com/downloading-and-installing-packages-globally
% https://github.com/clangd/clangd/issues/159
% https://github.com/microsoft/vscode-vsce/issues/238

\textbf{Prérequis} : Posséder un compte Azure.

Il faut d'abord récupérer un \Gls{pat}. Pour ce faire, il faut créer une \href{https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/create-organization?view=azure-devops}{organisation}.
Puis, créer un access token avec ces paramètres :
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/access-token.png}}
    \end{center}
    \caption[Access Token]{\label{access-token}Access Token}
\end{figure}

Ce token va nous permettre de publier l'extension sur un publisher. Celui-ci permettra de gérer l'extension en ligne.

Pour créer un publisher, il faut se connecter avec le même compte qui a été utilisé pour créé l'organisation et le token via la \href{https://marketplace.visualstudio.com/manage/publishers/}{management page}.

Quand cela est fait, il faut ensuite ajouter la propriété "publisher" et saisir l'id de celui-ci dans le package.json de notre extension.

Puis finalement, il suffit dans un terminal de saisir les commandes suivantes :
\begin{lstlisting}[frame=single]
    vsce login [le nom du publisher]
    Vsce publish
\end{lstlisting}

\textbf{Remarque} : Attention au terminal utilisé. Dans mon cas, le terminal de node (git.bash) sur windows ne fonctionne pas lorsque des interactions avec le terminal sont nécessaires.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=12cm]{assets/figures/manage-publisher.png}}
    \end{center}
    \caption[Interface web pour manager l'extension]{\label{manage-publisher} Interface web pour manager l'extension}
\end{figure}

Il est ensuite facile de manager l'extension au travers de la page web.
L'extension apparaitra automatiquement dans le marketplace si elle est publiée.

\subsection{Intégration continue (CI)}

Le squelette de base de l'extension ayant été créé en utilisant le générateur Yeoman de VS Code, la structure pour écrire nos tests d'intégrations
existe déjà \cite{testing-extension}.

Il suffit de lancer la commande "npm run test" ou "yarn test" pour lancer tous les tests se trouvant dans le répertoire \emph{test} (voir figure \ref{project-structure}).

C'est l'API \href{https://mochajs.org/api/}{mocha} qui est utilisé pour exécuter ces tests.

Actuellement on teste de cette façon les fonctionnalités de la complétion et de l'outline view.

Concernant les tests pour la coloration syntaxique, le module \href{https://www.npmjs.com/package/vscode-tmgrammar-test?activeTab}{vscode-tmgrammar-test} a été choisi.
Car depuis mocha, on a pas moyen de le faire.

Quelques tests unitaires simples ont été implémenté, mais qui permet toutefois de s'assurer qu'un changement ne vient pas modifier le comportement d'une manière inattendue.

\subsection{Deployement automatisé (CD)}

L'extension est actuellement automatiquement déployée lorsqu'une release sur la branche main est réalisée.
L'étape de mise en place est détaillée par VS Code à cet \href{https://code.visualstudio.com/api/working-with-extensions/continuous-integration#github-actions}{emplacement}.

Il s'agit d'une étape supplémentaire dans la pipeline après que les tests soient effectués.

Elle consiste à exécuter la commande vsce publish depuis un github action pour publier l'extension.

\href{https://www.npmjs.com/package/vsce}{vsce} est le gestionnaire d'extension de VS Code.

Il est requis d'ajouter le token (PAT) comme secret dans notre dépôt github.

\textbf{Remarque } :
À chaque nouvelle publication de l'extension, il est nécessaire d'incrémenter la valeur du champ \emph{version} du package.json
On peut redéployer une extension uniquement avec une version supérieure que la précédente.

\section{Génération du parser ANTLR}
\subsubsection{Grammaire}

Quand le fichier de grammaire est écrit, il suffit de lancer le code suivant :

\begin{lstlisting}[frame=single]
    npm run antlr4ts
\end{lstlisting}

Il nous génère ces fichiers :
\begin{itemize}
    \item UON.interp
    \item UON.tokens
    \item UONLexer.interp
    \item UONLexer.tokens
    \item UONLexer.ts
    \item UONParser.ts
\end{itemize}

Il faut exécuter cette commande à chaque fois que la grammaire est modifiée, pour répercuter les changements.

\section{Syntax highlighting}

La coloration syntaxique permet que chaque élément du texte soit affiché dans l'éditeur avec une coloration et un style en fonction de son type.
Il y a deux composantes principales.
\begin{itemize}
    \item La tokenisation qui consiste à séparer le texte en une liste de token.
    \item La thémisation (Theming) qui permet d'attribuer à un token une couleur et un style.
\end{itemize}

VS Code utilise Textmate grammars comme le moteur de tokénisation de la syntaxe.
Cette grammaire a été inventée par l'éditeur TextMate est a été adopté par de nombreux éditeurs et IDE.
Elle contient une liste structurée d'expressions régulières qui permet d'associer un scope
à un ou plusieurs tokens qui déclencherait l'activation d'une de ces expressions. \cite{syntax-highlight-guide}

La grammaire TextMate du langage UON se trouve dans le fichier \emph{uon.tmLanguage.json} dans le dossier \emph{syntaxes} à la racine du projet.
Ce fichier doit être référencé à travers du point de contribution \emph{"grammar"} dans le package.json.

\subsection{Scope}
Un scope défini le contexte d'un groupe de token et peut-être considéré comme son type.

Il s'agit d'un identifiant, c'est grâce au scope d'un token, qu'un fichier de thème peut attribuer une couleur avec un mécanisme de "clé-valeur".

TextMate définit une convention de nommage que de nombreux thèmes ciblent déjà et recommande de l'utiliser.
Ceci dans le but de permettre à notre langage d'être supporté le plus possible. \cite{textmate-grammars}

Pour respecter au maximum les conventions attendues et s'assurer qu'une coloration automatique soit appliquée.
La stratégie suivante a été adoptée : partir depuis un fichier de thème déjà créé (Thème par défaut de VS Code : Dark + ).
Et de réutiliser les scopes définis dans ce fichier.

Ce fichier se trouve sur le dépôt \href{https://github.com/microsoft/vscode/blob/main/extensions/theme-defaults/themes/dark_vs.json}{suivant}.
Mais peut être également généré depuis le command palette.

Ce procédé nous permet de profiter des thèmes de VS Code implémenté par défaut dans l'éditeur. Par exemple, illustrée par les figures \ref{vscode-dark-theme} et \ref{vscode-kimbi-dark-theme}.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/vscode-dark-theme.PNG}
    \end{center}
    \caption[UON en Thème vscode dark]{\label{vscode-dark-theme} vscode-dark-theme}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/vscode-kimbi-dark-theme.PNG}
    \end{center}
    \caption[UON en Thème vscode kimbi dark]{\label{vscode-kimbi-dark-theme} vscode kimbi dark theme}
\end{figure}

\subsubsection{Example}
Prenons un exemple simple du fichier \emph{uon.tmLanguage.json}, le scope pour les nombres : "constant.numeric.uon"

La convention est d'avoir des mots clés séparés par un point pour spécifier chaque niveau de précision supplémentaire
Il est aussi possible d'avoir plusieurs mots clés dans le scope d'un élément : "string.uon support.type.property-name.uon". Ils sont séparés par un espace.
C'est celui le plus à droite qui est prioritaire. Cela peut-être utile si l'on souhaite maximiser les chances qu'un thème nous fournisse une coloration.
\cite{scopes-selectors}

Les scopes s'imbriquent également et c'est le plus spécifique qui est utilisé pour le choix du thème.
Il est possible d'analyser notre code à l'aide du \emph{Scope Inspector}, qui est un outil disponible dans l'éditeur depuis la command palette de VS Code.
pour visualiser la hiérarchie des scopes pour un token. Comme illustré par la figure \ref{scope-inspector}.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/scope-inspector.png}
    \end{center}
    \caption[Scope inspector]{\label{scope-inspector} Le scope inspector sur un exemple de code en UON}
\end{figure}

La section \emph{textmate scopes} affiche la liste des scopes utilisés pour le token qu'on séléctionne. Le scope le plus spécialisé se trouve au sommet.

La coloration syntaxique n'est pas considérée comme un "programmatic language feature" et donc n'est pas gérée par la spécification LSP et par l'API de VS Code non plus.
Une des raisons évoquées et que ce mécanisme doit être le plus rapide possible. Et que le faire directement depuis le client garantit une latence faible
\cite{syntax-highlighting-editor}.

\subsection{Fonctionnement}

La figure \ref{Textmate grammar} illustre une représentation très simplifié du fichier \emph{uon.tmLanguage.json}.

\begin{listing}[!ht]
    \begin{minted}{json}
       1. {
       2.   "name": "UON",
       3.   "scopeName": "source.uon",
       4.   "patterns": [
       5.       {
       6.           "include": "#root_value"
       7.       },
       8.       {
       9.           "include": "#comments"
      10.       }
      11.    ],
      12.    "repository": {
      13.       "unquotedObjectKey": {
      14.           "match": "[\\p{L}]([\\p{L}]|[\\p{N}]|[\\p{Pc}])*",
      15.            "name": "entity.name.tag.unquotedObjectKey.uon"
      16.        },
      17.        "comments": {
      18.           "begin": "#",
      19.           "beginCaptures": {
      20.               "0": {
      21.                       "name": "punctuation.definition.comment.begin.uon"
      22.                }
      23.            },
      24.            "end": "\\n",
      25.            "endCaptures": {
      26.               "0": {
      27.                       "name": "punctuation.definition.comment.end.uon"
      28.               }
      29.             },
      30.            "name": "comment.line.number-sign.uon"
      31.            // "patterns": [ ... ]
      32.        }
      33.    }
      34. }
    \end{minted}
    \caption{Exemple simple d'une grammaire Texmate}
    \label{Textmate grammar}
\end{listing}

Dans cette exemple, la coloration n'est appliqué que sur les commentraires et les clés sans guillemets du langage UON.
Tout n'est pas couvert dans cette exemple, mais cela permet de bien comprendre le mécanisme général utilisé.

Les points importants sont les suivants :

\textbf{"scopeName"} (ligne 3) : Scope racine du fichier. C'est le scope de base appliqué sur chaque élément.

\textbf{"patterns"} (ligne 4) :
C'est un tableau contenant les règles actuelles utilisées pour parser le document en sein du scope courrant.
Elles sont appliquées dans l'ordre.
Ces règles peuvent être directement écrites à cet emplacement, mais il est aussi possible de les définir dans un objet devant être nommé "repository". Puis de les inclure, comme fait actuellement.

\textbf{"repository"} (ligne 12) : Un dictionnaire (clé-valeur) de règles.

\textbf{"name"} : le nom d'un scope qui sera attribuer.

On constate qu'il y a globalement deux manières pour appliquer un scope.

La première manière de faire et d'appliquer le scope à un élément qui déclenche directement l'expression régulière. ("match")
La seconde et de définir un caractère de départ et un autre de fin. Puis définir les patterns que l'on appliquera aux éléments se trouvant à l'intérieur.

\subsection{Capture}

On peut attribuer un scope à une partie d'un élément capturé avec une regex à l'aide des propriétés de capture.
Ce niveau de précision permet de gérer certains cas. Par exemple, quand il n'y a pas de délimiteur et que l'on voudrait appliquer une coloration différente aux éléments qui composent le groupe de caractère qu'une expression régulière
a déclenché.

C'est le cas pour les nombres en UON. Pour la séquence : 12 kg on voudrait que les deux composantes soient coloriées différemment.

On aura donc les capture illustré par la figure \ref{capture}.

\begin{listing}[!ht]
    \begin{minted}{json}
        "number": {
            "match": "[+-]?((0|[1-9]\\d*)(\\.\\d*)?([Ee][+-]?\\d*)?|
            (.\\d+([Ee][+-]?\\d*)?)|(0[xX][0-9a-fA-F]+)|(0[oO][0-9a-fA-F]+))( *)
            (kg|min|m|k|m|g|s|C|K)?",
            "captures": {
                "0": {
                    "name": "constant.numeric.uon"
                },
                "10": {
                    "name": "constant.language.uon"
                }
            }
        }
    \end{minted}
    \caption{Capture}
    \label{capture}
\end{listing}

Il n'est pas facile pour des expressions assez complexes de trouver ce nombre mais on peut s'aider du site \href{https://regex101.com/}{suivant}

\subsubsection{Approches}
La coloration essaye globalement se se calquer à la grammaire d'ANTLR. Mais ce n'est pas forcément toujours possible.
Des composants ont donc des structures et imbrications différentes.

Il y a coloration spécifique pour les propriétés appartenant à un type.
Cela rend la coloration syntaxique un peu plus liée à la grammaire.
C'est un choix d'implémentation qui peut rapidement modifié.

Mais cela permet de savoir qu'une propriété n'est pas valide (En complément avec les erreurs).

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/uon-unkown-type.PNG}
    \end{center}
    \caption[Propriété non connue pour un type]{\label{example-uon-coloration} Propriété non connue pour un type}
\end{figure}


\subsection{Semantic Highlight}

Un point important et qu'il ne faut pas confondre la coloration syntaxique avec ce que l'on nomme la coloration sémantique. Car il s'agit d'une couche que l'éditeur rajoute sur la précédente pour l'améliorer si nécessaire.
La tokénisation sémantique permet de fournir des informations supplémentaires sur les tokens en se basant sur une compréhension profonde du langage.
De manière à résoudre les symboles dans le contexte d'un projet.

Il s'agit donc d'obtenir des informations contextualisées afin fournir une coloration plus précise pour un token.

Par exemple avec du code Typescript :

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=15cm]{assets/figures/semantic-coloration-without.png}
    \end{center}
    \caption[Exemple sans coloration sémantique]{\label{semantic-coloration-without} Sans coloration sémantique}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=15cm]{assets/figures/semantic-coloration-with.png}
    \end{center}
    \caption[Exemple avec coloration sémantique ]{\label{semantic-coloration-with} Avec coloration sémantique }
\end{figure}

On voit à la ligne 10 que la colorisation de la fonction a changé et correspond maintenant à un paramètre d'une fonction.
Cela est intéressant pour des langages plus complexes dans leur nature qu'un langage de sérialisation. Une coloration syntaxe suffit amplement dans notre cas.
Car se baser sur la structure du document nous donne déjà toutes les informations nécessaires.

\section{Auto-completion}

Cette fonctionnalité fait parti de ce qu'on appelle plus communément "IntelliSense" qui un terme général désignant diverses fonctions d'édition de code \cite{intelliSense}.
Le terme "auto-complétion" existe également sous d'autres appellations (ex : autocomplétion, complétion automatique, complétion, etc.).

L'intérêt est de proposer des suggestions de saisie au fur est à mesure qu'un utilisateur écrit une chaine de caractères.
Ces suggestions sont des compléments qui pourraient convenir à la chaine de caractère saisit. Cela va donc dépendre principalement du langage traité.
Car en JSON ou en YAML, l'auto complétion n'existe pas vraiment dû à la syntaxe même de ces langages de sérialisation.
Il n'y a pas de mots clés comme on pourrait en trouver dans un langage de programmation.
Il n'aurait donc rien de base à proposer pour ces deux langages, sans utiliser une source d'infomation externe comme les "JSON-schéma".
À la limite, seules les clés ou valeurs précédemment saisies peuvent éventuellement être reproposées mais cela est fait par défaut sous VS Code pour n'importe quel type de langage
avec le type de complétion nommé "word based completion" \cite{word-based-completions}.

C'est pourquoi lorsqu'il faut implémenter une telle fonctionnalités, il faut savoir quoi proposer et quand le faire.
Pour cela, Il est nécessaire d'analyser le langage UON et de voir ce qu'il est pertinent de proposer comme suggestion de complétion.
La syntaxe UON peut être exprimée par la figure \ref{syntax} dans la section \hyperref[UON]{UON}.

On constate que : toute valeur est un type, et un type peut avoir des propriétés associées.

Les suggestions se baseront donc sur les types et leurs propriétés.

\subsection{antlr4-c3}

Une approche typique pour implémenter de la complétion est d'utiliser un parser du langage.
Mais en complément, nous allons aussi utilisé \href{https://github.com/mike-lischke/antlr4-c3}{"antlr4-c3"} (également simplement nommé c3).
Il s'agit d'un moteur de complétion pour les parsers basés sur ANTL4. Le moteur c3 est capable de fournir des candidats de complétions.
Les suggestions proposées dans l'éditeur sera majoritairement des candidats fournis par le moteur. C'est donc un outil relativement puissant, qui simplifie grandement la tâche du développeur.

Il a été conçue par \href{https://github.com/mike-lischke}{Mike Lischke}. Cet outil est écrit en Typescript et est disponible comme node module. Il existe aussi en version \href{https://github.com/mike-lischke/antlr4-c3/tree/master/ports/java}{JAVA} et \href{https://github.com/mike-lischke/antlr4-c3/tree/master/ports/c%23}{C\#}.

Le moteur de complétion n'utilise pas le parse tree pour proposer des candidats. Mais l'exécution du parser est nécessaire pour remplir le token stream. Pour rappel, un exemple d'utilisation du parser est illustré précédement par la figure \ref{Utilisation du parser d'ANTLR}.
Ce flux de tokens contient tous les tokens qui ont été reconnu lors de l'analyse lexicale produit par le lexer.
Le moteur s'appuyera sur ce flux de tokens et également du mécanisme de machine à état d'ANTLR4 disponible dans le parser \cite{antlr-mega-tutorial}.

Pourquoi un tel moteur ?
Car connaitre tous les types de tokens qu'un utilisateur serait amené à saisir ne suffit pas. Pour que la complétion soit réellement intéressante, nous voudrions connaitre quelle règle de la grammaire pourrait être valide à une position donnée.
Et cet outil apporte une solution pratique est élégante pour résoudre ce cas.

\subsection{Candidats}\label{candidates}

Pour recevoir des candidats, il suffit d'avoir un environnement comme présenté par la figure \ref{Utilisation du parser d'ANTLR}, d'installer le module npm \emph{antrl4ts} et
de donner au moteur, une instance du parser et un indice.

\begin{listing}[!ht]
    \begin{minted}{typescript}
    let core = new c3.CodeCompletionCore(parser);
    let candidates = core.collectCandidates(index);
    \end{minted}
    \caption{Initialisation du moteur de complétion c3}
    \label{c3-setup}
\end{listing}

Un candidat est un token qui pourrait se situer à une position donnée dans l'input.
Cette position correspond à un indice donné au moteur.

\subsubsection{Indexation}
L'indexation est gérée de deux manières différentes comme illustré par la figure \ref{candidat-index}, tiré de la documentation de c3.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/candidat-index.png}}
    \end{center}
    \caption[Indexation des candidats]{\label{candidat-index} Indexation des candidats}
\end{figure}

Cela va dépendre si on décide d'ignorer ou non les espaces depuis la grammaire g4.
Ce qui est fait actuellement c'est de récupérer ces tokens dans un canal mais sans le considérer dans le langage.

\begin{listing}[H]
    \begin{minted}{antlr}
        WS: [ \n\r\t] -> channel(HIDDEN);
    \end{minted}
    \caption{Règle pour ignorer les espaces}
    \label{space-rule}
\end{listing}

C'est ce qui est recommandé dans la documentation du projet c3. Car cela permet de gérer plus de cas particuliers \cite{c3-example-soql}.
% https://neuroning.com/post/implementing-code-completion-for-vscode-with-antlr/

Même si l'on n'en profite pas actuellement, cela pourrait permettre une meilleur extensibilité du code si nécessaire.

% La documentation du projet indique bien que ce n'est pas un problème trivial à résoudre.

\subsubsection{Trouver l'indice}

L'indice devrait correspondre à l'emplacement du curseur dans l'input.

À priori, il serait nécessaire de convertir la position du curseur dans un indice.
Mais cela est n'est pas réelement nécessaire, si on ne prend pas en compte tout le code.

La méthode actuellement appliquée et que l'on va extraire le code saisie jusqu'à la position du curseur.
C'est ce code qui sera l'input que l'on va donner au parser lorsque la complétion sera trigger par VS Code.

Il sera ensuite possible de déduire l'indice du prochain candidats en se basant sur le tableau de tokens que le token stream contient ainsi que sa taille.

Car on remarque que chaque indice à la figure \ref{candidat-index} représente l'emplacament d'un token et un espace (si gardé). Et c'est ce que l'on retrouve dans le token stream.

L'indice sera donc un élément en fin de token stream.

Par exemple sur la figure \ref{token-stream} qui représente l'état d'un stream de token après une demande de suggestion pour obtenir des propriétés dans une clé.
un indices valide serait le 10.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=2cm]{assets/figures/tokenstream.PNG}}
    \end{center}
    \caption[Représentation textuelles du token stream]{\label{token-stream} Représentation textuelles du token stream}
\end{figure}

L'indice est calculé en soustrayant simplement la taille du tableau par 3 lorsque l'indentation n'est pas gérée. Car il a été constaté que le token stream ajoute systématiquement les mêmes deux éléments de fin (\textbackslash n et <EOF>)
Lorsque l'indentation est gérée le traitement est différent. Des tokens de type DEDENT pouvant se trouver après un \textbackslash n. On va donc prendre l'indice du premier élément qui n'est pas un token de type DEDENT ou un \textbackslash n en commençant depuis la fin du token stream.

\subsection{CandidatesCollection}

Le résultat du moteur à l'appel de la fonction \emph{collectCandidates} est un objet de type \emph{CandidatesCollection}.

% https://github.com/lark-parser/lark/issues/684 -> api ?
\begin{listing}[!ht]
    \begin{minted}{typescript}
    class CandidatesCollection {
        tokens: Map<number, TokenList>;
        rules: Map<number, RuleList>;
    }
    \end{minted}
    \caption{CandidatesCollection}
    \label{CandidatesCollection}
\end{listing}

Cette collection contient deux map : \emph{tokens} et \emph{rules}.

La map \emph{tokens} fait le lien entre le prochain token possible et une liste contenant des tokens qui doit le suivre immédiatement, si elle existe.

Nous allons principalement utiliser les clés de cette map pour proposer nos suggestions.
Cette clé est juste l'id d'un token. On va utiliser cet id et le parseur pour retrouver la valeur du token qu'il représente. C'est ce qui sera finalement affiché dans l'éditeur comme suggestion.

La liste est aussi utilisé à une moindre mesure pour obtenir par exemple des suggestions suivantes : "descripton : "
On obtient deux tokens d'un coup "description" et ":"

Cependant cela nécessite que les tokens se suivent obligatoirement et sans alternatives possibles dans la grammaire.

La map \emph{rules} fait le lien entre la règle du parser actuelle et la liste de règles utilisé pour arriver à ce stade.
Cette map nous permet de questionner la liste des symboles pour retourner des informations supplémentaires.

\subsubsection{Symbol tables}\label{Symbol tables}

C'est un mécanisme de c3 mais qui ne sera pas exploité, car je n'ai pas trouvé d'usage dans le cadre du cahier des charges de ce projet et par manque de temps.
Cependant il reste intéressant de détailler brièvement son utilité pour d'éventuelles améliorations.

On pourrait penser qu'avoir à disposition un parser nous permettrait de récupérer toutes les informations nécessaires au travers d'un AST.
Cependant ce n'est pas le cas pour la plupart des scénarios.
Nous pouvons savoir quels sont les tokens valide à un emplacement (ce qui est suffisant pour nous). Mais pour certains langages, des informations manquent.
Et c'est ce que la table de symboles essaye de résoudre.

Une table de symboles est une structure qui contient des informations complémentaires sur des symboles, cela peut s'agir de noms, leur scopes et d'autres caractéristiques.
Cependant, un langage de sérialisation est relativement moins complexe à gérer qu'un langage de programmation.
Nous n'avons par exemple pas à mémoriser des variables et leur visibilité au sein du code.

Mais il est mentionné dans la spécification qu'un schéma pourrait être défini au même niveau que le payload.
Avec ce scénario en tête, une table de symboles pourrait se révéler utile. Car on pourrait se servir de ce schéma de validation qui spécifierait un nouveau type pour proposer de la complétion appropriée
lorsque l'on saisira ce type dans le payload. Typiquement des propriétés.

\subsection{Tokens ignorés}

Il est possible d'indiquer au moteur les tokens qu'il peut ignorer, pour ne pas les récupérer dans la liste des candidats fournis par le moteur.
Cela est nécessaire pour nous, car tous les tokens ne sont pas intéressant.

Les tokens peuvent être classé en deux catégories.

\begin{enumerate}
    \item Ceux dans leur valeur sont fixes.
    \item Ceux dans leur valeur dépendent de la saisie d'un utilisateur au travers d'expression régulière.
\end{enumerate}

Les tokens qui sont utiliser comme suggestions sont ceux issue de a première catégorie. Mais quelques tokens sont ignorés : comme les parenthèse, accolades car ce n'est pas vraiment pertinent. % et les deux-points
La seconde n'est pas intéressante, car le moteur ne fera que retourner le nom du token ce qui est inutile. Par exemple "NUMBER", lorsqu'il est possible d'avoir un nombre.

\subsection{Gestion des erreurs }\label{error handle}

On aurait pu penser naïvement que le mécanisme avancé de récupération d'erreur \emph{Parser Error Recovery Strategy} permettrait également de rendre la complétion plus tolérante.
Mais ce mécanisme n'est utilisé que pour générer un parse tree, cela est donc inutile pour ce cas.

%Donc comme déjà mentionner, pour que le moteur puisse retourner des candidats, la syntaxe se trouvant avant la position du curseur doit être valide.
Cependant une contrainte pour que le moteur propose des candidats est qu'il est nécessaire que tout le code se trouvant avant le curseur soit correct.
Car sinon on pourrait avoir dans le stream de token des éléments qui ne seront pas compris par le moteur et l'on n'obtiendra pas de candidats.

Une explication fournit par le créateur du moteur dans la rubrique "issues" du dépot du \href{ https://github.com/mike-lischke/antlr4-c3/issues/29}{projet}.
% https://github.com/mike-lischke/antlr4-c3/issues/29

est que dès le moment où le moteur est confronté à une erreur de syntaxe (token inattendu ou invalide), il n'a plus moyen de trouver un chemin valide.
Et que parcourir toutes les possibilités jusqu'à trouver le token qui suit celui qui pose problème est trop long et peut mener à des erreurs d'ambiguïtés si le token suivant se trouve dans plusieurs règles de la grammaire.

Mais l'on pourrait s'en contenter, car finalement, il est logique qu'une erreur puisse entrainer un tel comportement.
Si dans un langage de programmation un utilisateur saisit du code erroné, il fort possible que la complétion cesse de fonctionner.

\subsubsection{Correction custom}
Cependant comme nous travaillons avec une syntaxe beaucoup moins complexe l'on pourrait modifier le texte depuis l'application pour corriger voir supprimer le problème.
Par exemple, dans le cas d'un mapping de clé valeur, une ligne pourrait être omis sans nuire à la compréhension du document.

C'est ce qui peut être fait de la manière suivante :
\begin{enumerate}
    \item Étape 1 : créer une nouvelle classe de listener qui détecte les erreurs lors du parsing.
    \item Étape 2 : stocker la position des tokens qui posent problème dans la classe du listener.
    \item Étape 3 : modifier le texte récupérer dans l'éditeur localement en conséquence.
    \item Étape 4 : relancer la phase de la création du parser en lui donnant le texte modifié.
\end{enumerate}

Mais cette approche nécessite de traiter énormément de cas particulier et le problème et que l'on couple la logique du moteur avec l'application et donc n'est pas une très bonne pratique.
Car si on modifie la grammaire, il faudra répercuter les changements dans l'application.
C'est pourquoi actuellement on ne supprime que la première ligne qui pose problème et qu'il n'a pas été jugé nécessaire d'aller plus loin.

\subsection{Snippets}
Il est possible d'ajouter des snippets sous VS Code. C'est-à-dire des templates qui nous permettent de générer du code prédéfini. Puis de les ajouter aux suggestions.
C'est utile si on a envie d'avoir des suggestions un peu plus personnalisées.
Typiquement on pourrait vouloir avoir comme suggestion directement un ensemble de propriétés comme illustré par la figure \ref*{snippet-suggestion}.
Ce qui n'est pas possible avec le moteur.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/snippet-suggestion.png}
    \end{center}
    \caption[Suggestion d'un snippet]{\label{snippet-suggestion} Suggestion d'un snippet}
\end{figure}

Ce modèle de snippet permet d'afficher une chaine de caractères avec des case vides à remplir.
Il est possible de passer à la case suivante à l'aide de la touche \emph{tab}.

Cependant il est nécessaire de proposer ces suggestions au bon moment. La solution envisagée et de vérifier que le token du type possédant
ces propriétés ce trouve dans les candidats du moteur et qu'il ne s'agit pas de schéma en controlant dans le stream de token que le token \emph{!schema} ne s'y trouve pas.

Les snippets sont simples et utiles, mais liés à l'application. Il ne faudrait donc pas trop en utiliser.

\subsection{Triggers}
Par défaut, il Il aussi possible de l'activer manuellement en exécutant la commande "ctrl + space".

Il est possible d'ajouter au provider des symboles (triggerCharacters) qui vont forcer son activation. Les symboles espace " " et de retour à la ligne (\\rn) ont été définis comme triggers, car cela semble assez naturel dans ce langage.
Il s'agit uniquement d'une appréciation personnelle.

\subsection{Pas de suggestion}

Le message "no suggestion" peut survenir lorsque le provider n'arrive pas à retourner des suggestions.
Mais, il est possible d'avoir ce message alors même que le moteur retourne des candidats. Cela vient du fait
que si le curseur se trouve collé à du texte, VS Code n'a pas la capacité de savoir qu'elle est l'étendu de la chaine de texte que le curseur touche,
qui doit être remplacer et s'il doit même le faire.

Par exemple : { key(|) }.
On pourrait recevoir la suggestion "description:" mais l'on ne recevra rien. Car VS Code ne sait pas comment réagir.

Il est possible de résoudre ce problème mais cela nécessite de définir la position pour la suggestion (range).
Cela implique de gérer pleins de cas particulier de la grammaire depuis l'application. Il n'a pas été jugé pertinent de traiter ce cas, car n'apportant pas réellement un intérêt.

\subsection{Mécanisme automatique}

Il est parfois difficle de savoir ce que VS Code fourni par défaut ou à quel point un comportement peut être modifié.
C'est le cas pour la complétion. Il est nécessaire d'aller regarder dans les paramètres (le fichier \emph{settings.json} dans VS Code) pour voir d'ou vienne certain comportement.

\subsubsection{Quick suggest}

Le \emph{quick suggest} contrôle si les suggestions doivent s'afficher automatiquement pendant la saisie.

Il est activé en dehors des strings et commentaires. Comme illustré par la figure \ref{quickSuggestions}.

\begin{listing}[!ht]
    \begin{minted}{json}
        "editor.quickSuggestions": {
        "other": "on",
        "comments": "off",
        "strings": "off"
        }
    \end{minted}
    \caption{Quick Suggestions}
    \label{quickSuggestions}
\end{listing}

Ce n'est donc pas un comportement qui peut être paramétré depuis le provider de VS Code, mais seulement depuis l'éditeur lui-même. \cite{quick-suggestion}

\subsubsection{Fuzzy search}

VS Code va automatiquement filtrer les suggestions pendant la saisie à l'aide d'un algorithme de fuzzing search.
Pour rappel le fuzzy search (ou recherche approximative) permet de trouver une chaine de caractère en fournissant une chaine qui n'a besoin d'avoir une correspondance exacte
avec le texte que l'on cherche.

Par défaut la complétion proposée par VS Code porte sur les symboles déjà existants dans le code.
Ce type de complétion est nommé "word based completion" dans VS Code.

\subsection{Résultat}

Voici un liste non exhaustive de complétions possibles.

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=8cm]{assets/figures/completion.PNG}
    \end{center}
    \caption[Complétion d'unités]{\label{unit-completion} Complétion d'unités}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=6cm]{assets/figures/property-completion.PNG}
    \end{center}
    \caption[Complétion de propriétés]{\label{property-completion} Complétion de propriétés}
\end{figure}

\begin{figure}[H]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/type-completion.PNG}
    \end{center}
    \caption[Complétion de types]{\label{type-completion} Complétion de types}
\end{figure}


\section{Outline view}

La outline view est une représentation de la structure d'un fichier actuellement ouvert par l'éditeur. On appelle généralement cette structure le "document outlining" ou "code outlining".

Elle est actualisé en continu pour refléter les changements.

En analysant la spécification \href{https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/}{LSP} ou le \href{https://code.visualstudio.com/api/references/vscode-api#DocumentSymbol}{DocumentSymbolProvider} de l'api VS Code, on constate que l'outline peut être représenté sous deux formes :

\begin{itemize}
    \item SymbolInformation qui est une flat list de tous les symboles trouvés dans un document.
    \item DocumentSymbol qui est une hiérarchie de symboles trouvés dans un document.
\end{itemize}

La structure peut être donc représentée de manière hiérarchique ou seulement lister des éléments.

On a accès à cette représentation dans la sidebar gauche par défaut.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/outline-view-sidebar.PNG}}
    \end{center}
    \caption[Outline view disponible depuis la sidebar]{\label{outline-view-sidebar} outline-view-sidebar}
\end{figure}

ou depuis la command palette en saisissant la combinaison "CTRL + SHIFT + o" :

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/outline-view-searchbar.PNG}}
    \end{center}
    \caption[Outline view disponible depuis la searchbar]{\label{outline-view-searchbar} outline-view-searchbar}
\end{figure}

Il est possible obtenir la structure d'un fichier en analysant manuellement le texte (par exemple ligne par ligne).
Mais cette approche n'est pas optimale et il serait difficile de représenter la structure hiérarchique du document à l'aide de regex \cite{antlr-mega-tutorial}.

Nous allons donc utiliser notre parser.

Donc pour générer une telle structure, il va falloir parcourir le parse tree pour récupérer un AST qui contiendra uniquement les informations utiles pendant le parcours.

Pour parcourir un arbre, deux approches sont possibles : Le pattern \emph{Listener} ou \emph{Visitor}.
La première consiste à écouter les types de noeuds traversés qui nous intéressent. Le problème et que cette approche ne nous permet pas de manipuler un objet en cours de route ni gérer le flux d'exécution.
Car il n'est pas possible de communiquer entre les noeuds. Cela peut être tout de même utile si l'on veut afficher la structure de l'arbre dans un terminal par exemple.
Contrairement à la seconde approche, qui nous permet de gérer ces deux cas. Et c'est donc tout naturellement que cette solution a été privilégiée.

\section{DocumentSymbol}

Cependant on ne va pas d'abord créer un AST puis construire la outline view à l'aide de celui-ci.
L'outline view sera plutôt crée à la volé, pendant le parcours du parse tree. L'outline view sera une représentation directe de notre AST.

L'arbre sera donc un objet VS Code de type \href{https://code.visualstudio.com/api/references/vscode-api#DocumentSymbol}{DocumentSymbol}.

Il y a quelques points intéressants à commenter concernant cet objet.
La variable \emph{children} permet d'avoir des encapsulations d'objets \emph{DocumentSymbol}. C'est ce qui permettra de créer une hiérarchie.
Un noeud doit être représenté par un \emph{Symbolkind}. Nous sommes donc limités sur la représentation et devons faire les ajustements nécessaires.

Il y a deux paramètres de ranges à fournir à la création de l'objet. Leur différence n'est pas forcément très claire.

Le premier se veut plus global que le second.
La première range indique l'entièreté de la définition d'un objet. Par exemple, pour une classe en Typescript en sélectionnerait le mot \emph{class} si on clique sur le nom de la classe.
Le deuxième range (selectionRange) se veut plus spécifique en ciblant uniquement un seul token.
Dans notre cas les deux peuvent représenter la même chose et représenter la range du token affiché dans l'éditeur.

\section{Visitor}

Lors de la génération des fichiers ANTLR, il est indiqué de générer également un fichier visiteur \emph{UONVisitor} en précisant le tag \emph{- visitor}.

Cela va nous créer un fichier qui contiendra une interface définissant une fonction pour chaque type de noeud de note parseur et retournera un résultat de type générique.
Chacune de ses fonctions prend comme paramètre son contexte.

La classe UonASTVisitor implémente cette interface qui étend la classe \href{https://www.antlr.org/api/Java/org/antlr/v4/runtime/tree/AbstractParseTreeVisitor.html}{AbstractParseTreeVisitor}
pour obtenir le comportement par défaut du pattern visitor. Elle gère le mécanisme de parcours et d’agrégation des résultats.

Pour obtenir l'AST, on doit donner le parse tree créé par le parser à la fonction d'entrée \emph{visit} de notre visitor.

\begin{listing}[!ht]
    \begin{minted}{typescript}
        const ast = uonASTVisitor.visit(tree);
    \end{minted}
    %\caption{Utilisation du parser d'ANTLR}
    %\label{Utilisation du parser d'ANTLR}
\end{listing}

De plus note classe \emph{UONVisitor} étant générique, c'est ce qui nous permet de manipuler un objet VS code.
Actuellement la classe surchargée est de type \emph{any} car pendant le parcours un élément peut être de deux natures différentes (un objet représentant un noeud terminal ou un DocumentSymbol).
Ce noeud terminal contient simplement la valeur textuelle du noeud et sa position.
Cette séparation est faite pour simplifier des étapes de constructions et de filtrage des éléments.

\subsection{visitChildren}
La fonction \emph{visitChildren} permet la traversée de l'arbre.

Elle est illustrée par la figure \ref{visitChildren}.

\begin{listing}[!ht]
    \begin{minted}{typescript}
    visitChildren(node: RuleNode) {
        let result: any = this.defaultResult();
        let n = node.childCount;
        for (let i = 0; i < n; i++) {
            if (!this.shouldVisitNextChild(node, result)) {
                break;
            }
            let c = node.getChild(i);
            let childResult = c.accept(this);
            result = this.aggregateResult(result, childResult);
        }
        return result;
    }
    \end{minted}
    \caption{Fonction visitChildren }
    \label{visitChildren}
\end{listing}

Le résultat par défaut est une liste vide. Lors du parcours de la remontée dans un noeud parent, nous allons rassembler les enfants dans une liste.

Pourquoi manipuler une liste au lieu d'un objet \emph{DocumentSymbol} directement ?
Tout simplement, car sinon on aurait du le faire dans la fonction d'agrégation et l'on préféra ici déléguer la logique dans le noeud parent concerné
qui pourra utiliser les valeurs récupérer pour construire l'information voulue par exemple dans une séquence ou un mapping.

C'est un choix d'implémentation pour faciliter la construction d'un objet \emph{DocumentSymbol}.

\subsection{Parcours}

Le parcours est effectué selon un algorithme \Gls{dfs}.

Pour rappel un parcours DFS est effectué dans l'ordre affiché à la figure \ref*{Algorithme de parcours en profondeur (DFS)}

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=6cm]{assets/figures/DFS.PNG}}
    \end{center}
    \caption[Algorithme de parcours en profondeur (DFS)]{\label{Algorithme de parcours en profondeur (DFS)} Algorithme de parcours en profondeur (DFS)}
\end{figure}

Ce qui est agréable en utilisant cet algorithme et qu'il nous permet de commencer à construire l'information à partir des noeuds terminaux
de l'arbre et de remonter naturellement au parent le plus proche.

Un parent va pouvoir obtenir la liste des enfants dans un ordre que l'on s'attendrait à recevoir.
Cela rend la manipulation dans un parent plus simple concernant des modifications que l'on voudrait effectuer pour représenter ces informations
dans un objet de type \emph{DocumentSymbol}.

\subsection{La position}

Généralement un noeud nous permet de nous rediriger sur l'élément du fichier lorsqu'on le sélectionne depuis la sidebar ou depuis la search bar.
Il faut donc pouvoir être capable de récupérer la position d'un élément du fichier.

Heureusement on peut récupérer cette information lors d'un parcours d'un noeud terminal du parse tree.
Il suffit d'observer le token associé au symbole de ce noeud et plus particulièrement les
propriétés \emph{line} et \emph{charPositionInLine} (qui est la position que l'on qualifierait généralement de colonne).
Il faut toutefois juste corriger la position de la ligne en soustrayant la valeur d'une unité, car l'indice d'une ligne commence à 0 sur VS Code et 1 pour ANLTR.

\subsection{Résultat}
L'aspect visuel de l'outline view a été conçu pour ressembler à celle de JSON et de YAML.
Suivant cet exemple de code UON basique :

\begin{lstlisting}
    {
        name : "paul",
        age: 18,
        hobbys: {
            music: "rock"
        },
        parents : ["steve", "jeanne"]
    }
\end{lstlisting}

Nous obtiendrons l'outline suivant :

% capture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=4cm]{assets/figures/uon-payload-outline-without-properties.PNG}}
    \end{center}
    \caption[Outline UON sans propriétés]{\label{uon-payload-outline-without-properties} Outline UON sans propriétés}
\end{figure}

Mais UON prenons en compte également les propriétés, il a été jugé pertinents de les rajouter dans la outline.
Pour ne pas changer la structure et ne pas surcharger l'arbre,
nous rajoutons simplement les propriétés des clés et des types comme enfant de l'attribut parent du noeud de l'outline.

Par exemple, si nous rajoutons des propriétés à la clé est un type :
\begin{lstlisting}
    {
        name : "paul",
        age(optional : false): !int( comment : "a comment") 18,
        hobbys: {
            music: "rock"
        },
        parents : ["steve", "jeanne"]
    }
\end{lstlisting}

Nous obtiendrons l'outline suivante :

%capture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=4cm]{assets/figures/uon-payload-outline-with-properties.PNG}}
    \end{center}
    \caption[Outline UON avec propriétés]{\label{uon-payload-outline-with-properties} Outline UON avec propriétés}
\end{figure}

La représentation d'un schéma de validation est légèrement différente :

\begin{lstlisting}
    !!mySensor: !schema ( description : "provide great infos", name : ok) {
        name : !str,
        power( optional : true) : !int( min : 5, max : 50)
    }
\end{lstlisting}

Dans un fichier de validation, on ne peut avoir comme valeur uniquement des types ayant ou non des propriétés. Ce sont ces éléments qui nous intéressent le plus.
Donc on a plus de propriétés \emph{value props}, mais on liste ces propriétés directement comme enfant de l'objet. Cela pour gagner en visibilité.

Nous aurons donc l'outline suivante :

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=6cm]{assets/figures/uon-schema-outline.PNG}}
    \end{center}
    \caption[Outline d'un schéma]{\label{uon-schema-outline} Outline d'un schéma}
\end{figure}

\section{Hover Information}

%https://stackoverflow.com/questions/57438198/typescript-element-implicitly-has-an-any-type-because-expression-of-type-st

C'est une fonctionnalité assez triviale qui consiste à afficher une explication lorsque l'on passe le curseur sur un élément.
Cela consiste à aller chercher dans un dictionnaire si la valeur sélectionnée est bien une clé du dictionnaire et de récupérer la valeur si c'est bien le cas.

Ce dictionnaire est un fichier.json qui est importé comme module. Il a été nécessaire de rajouter la ligne \emph{"resolveJsonModule": true} dans le fichier tsconfig.json pour autoriser ce type d'import.

Même si son implémentation est relativement simple, il n'en reste pas moins un élément fortement utile et pouvant être étendu par la suite.
Le code est déjà réutilisé pour fournir la documentation des types lors des suggestions de complétions.

Les informations affichées au survol de la souris d’un élément sont directement tirées de la spécification.

\section{Lint}
Un Linter est un outil d'analyse de code qui permet de détecter les erreurs et les problèmes de syntaxe.

Il est donc nécessaire de pouvoir exploiter correctement les erreurs.

Il est possible de créer un listener qui aura pour rôle d'observer les erreurs lors du parsing avec ANLTR.
Pour cela il faut créer une classe qui implémente l'interface "ANTLRErrorListener" et attribuer ce listener à notre parser
avec la commande :

Cette interface définit la fonction \emph{syntaxError}.

Cette fonction sera trigger comme sans nom l'indique lors d'une erreur de syntaxe.
Une erreur de syntaxe peut survenir quand un élément et mal orthographié ou manquant.

Il est bien de rappeler que certaines erreurs de syntaxes sont plus graves que d'autres.
Même si ANTLR propose des solutions pour se récupérer d'un processus de parsing défectueux, il reste limité et ne peut pas faire de miracle.

Elle contient des paramètres intéressants à exploiter pour informer l'utilisateur.
Ces paramètres sont illustrés par la figure \ref{syntax}.

\begin{listing}[!ht]
    \begin{minted}{typescript}
    syntaxError?: <T extends TSymbol>(recognizer: Recognizer<T, any>,
    offendingSymbol: T | undefined,line: number, charPositionInLine: number,
    msg: string, e: RecognitionException | undefined) => void;
    \end{minted}
    \caption{Fonction syntaxError}
    \label{syntaxError}
\end{listing}

Comme pour la outline, il est possible de récupérer la position du token. Il suffit de récupérer les valeurs des propriétés \emph{line} et \emph{charPositionInLine}.
Il s'agira ici d'un token qui a posé problème et l'on voudra afficher à son emplacement l'indication visuelle (soulignement ondulé rouge) ainsi que le message d'erreur associé, lorsque l'utilisateur passe
sa souris dessus.

Le message d'erreur peut-être récupéré à partir de la propriété \emph{msg}. Même s'il est davantage destiné au debuggage.
Les messages restent relativement simples. Ils indiquent à l'utilisateur quel token pose problème et quels sont les caractères qui pourrait y être substituer.

Pour afficher ces informations sur l'éditeur, il faut utiliser un objet VS Code de type \emph{DiagnosticCollection}.

Elle peut afficher une liste d'erreur.

On va garder en mémoire cette liste dans la classe que l'on mettra à jour à chaque fois que le listener détecte une erreur.
Et on actualisera à chaque fois l'objet \emph{DiagnosticCollection} avec cette liste.

Cependant, le parser ne pas se rendre compte si ce n'est pas plutôt le token précédent qui pourrait amener à une erreur.
Cela ne prend non plus pas en compte ce qu'il vient après et donc remplacer un token par un de ceux proposés peut se se révéler contre-productif.

\subsection{Résultat}

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/lint.PNG}}
    \end{center}
    \caption[Signalement d'erreurs]{\label{lint} Signalement d'erreurs}
\end{figure}

\subsection{Suggestion}
À l'heure actuelle  il n'y a pas encore de réelles suggestions.
Certains messages d'erreurs nous informes parfois des tokens qui devrait se situer à la place de celui qui a causé l'erreur.
Une idée serait donc de pouvoir remplacer ce token erronées par ceux suggérés.
Le problème est que certains des ces tokens ne corrige pas toujours les problèmes, il faudrait donc appliquer un traitement particulier.

\subsubsection{Quick fick}

L'implémentation des suggestions peut se faire à l'aide d'un quickfick qui va permettre à l'utilisateur d'intéragir avec l'éditeur.

Un petit quick fix a été implémenter pour en tester la mise en place. Il permet de supprimer le token qui pose problème en se basant sur les ranges fourni lors de la création d'un objet Diagnostic depuis le listener.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=6cm]{assets/figures/quickfix.PNG}}
    \end{center}
    \caption[Quick fix]{\label{quick-fix} Quick fix}
\end{figure}

\textbf{Remarque } : le comportement n'est pas complètement operationnelle. Il s'affiche pour chaque erreur (même quand il manque un token).

\subsection{Outline view}
Le listener se trouve au niveau du code de la outline. Étant donné que c'est une fonctionnalité qui est souvent solliciter il a été jugé interessant
de placer ce listener à cet emplacement.

\section{Fonctionnalités triviales}
Il s'agit de fonctionnalités couramment disponibles dans un support de langage. Leur implémentation est relativement simple et rapide et n'a donc pas été mentionnée dans le cahier des charges.
Il s'agit concrètement du fichier \emph{language-configuration.json}. % https://github.com/redhat-developer/vscode-yaml/blob/main/language-configuration.json

Les suivantes ont été implémentées :

\textbf{Comment}
\begin{itemize}
    \item Il est possible de commenter du code sous forme de ligne.
    \item Il est possible de commenter et décommenter du code (Toggling)
\end{itemize}

\textbf{Symbol pairs}
\begin{itemize}
    \item Il est possible de faire la correspondance pour certaines paires de symboles à l'aide d'une indication visuelle. (Matching)
    \item Certains symboles du langage doivent être automatiquement complétés si l'utilisateur saisit le premier élément de celle-ci. (Autoclosing)
\end{itemize}

\textbf{Code folding}
\begin{itemize}
    \item Cacher un bloc de code sur une ligne en fonction de son niveau d'indentation.
\end{itemize}


\chapter{Planning}

\section{Diagramme de gantt}
Voici le diagramme de Gantt pour ce projet.

\includepdf[page=1]{assets/planning/gantt.pdf}

\section{Discussion}
% Ajouter discussion de l'avancée du travail. Êtes-vous à jour ? Avez-vous pris du retard ?

Le projet s'est déroulé de la manière suivante :
\begin{enumerate}
    \item Analyser ce qu’il était possible de faire puis fournir un support minimal du langage.
    \item Améliorer ensuite progressivement ce support.
\end{enumerate}

Le planning a donc été modifié à partir de la 15ème semaine pour tenir compte des tâches qui n'avaient pas été prévues.

Il s'agissait de :

\begin{itemize}
    \item Se renseigner davantages sur VS Code et ANTLR pour comprendre certains comportements.
    \item Étendre la grammaire au format YAML et donc la gestion des indentations.
    \item Gérer les propriétés dans la outline.
\end{itemize}

Concernant les tâches, ce qui a concentré le plus de temps ont été : la complétion, l'outline view et la gestion des indentations.

\section{État actuel du projet}

L'état actuel du projet sur Github correspond au commit sur la branche main, avec le hash :
195d7eb6bdeee2ed8f67ea196adae874f2b1f443

Pour rappel, le projet est disponible à l'adresse suivante : \url{https://github.com/vitorva/vscode-uon}.

L'extension est \href{https://marketplace.visualstudio.com/items?itemName=vitorva.vscode-uon}{disponible} sur le marketplace de VS Code.

Passons maintenant en revue les tâches réalisées en lien avec le cahier des charges:

\textbf{VSCode Extension} :
\begin{itemize}
    \item L'extension est publiée sur le marketplace
    \item Des tests sont effectué lors d'un push sur les branches \emph{dev} ou \emph{main} et lors de la publication d'une release.
    \item Lors d'une release, l'extension et publiée automatiquement.
\end{itemize}

\textbf{Grammaire} :
\begin{itemize}
    \item Permet de créer des mappings et des séquences de type en un format proche de JSON et YAML, ainsi qu'un schéma de validation.
\end{itemize}

\textbf{Syntax highlighting} :
\begin{itemize}
    \item Une coloration syntaxique est appliqué aux éléments.
\end{itemize}

\textbf{Auto-complétion} :
\begin{itemize}
    \item Des suggestions sont proposés si elles peuvent être écrit à l'emplacement souhaité.
    \item Les suggestions proposés à l'utilisateur sont des mots-clés (Les types et leurs propriétés associées, ainsi que des unités).
    \item Les suggestions peuvent aussi être des snippets.
\end{itemize}

\textbf{Outline view} :
\begin{itemize}
    \item Les éléments sont affichés dans l'outline d'une manière similaire à une outline de JSON ou de YAML.
    \item Les propriétés des clés et des types sont affichés comme enfant de l'élément.
\end{itemize}

\textbf{Lint} :
\begin{itemize}
    \item Les erreurs de syntaxes sont affichés : elles indiquent si un élément est faux ou manquant.
    \item Un quick fix permet de supprimer l'élément qui cause l'erreur.
\end{itemize}

\chapter{Conclusion}

L'objectif de ce Travail de Bachelor était le support du langage UON au travers de l'implémentation d'une extension VS Code.

L'idée principale de ce support était qu'il devait mettre à disposition dans l'éditeur, les éléments pour permettre aux mieux la rédaction d'un fichier UON.
Pour éviter par exemple de devoir consulter la documentation régulièrement.

Il a donc fallu découvrir et analyser un nouveau langage.
Définir quelles sont les fonctionnalités à implémenter dans un éditeur pouvant s'avérer utile pour ce langage.
Découvrir les technologies permettant leur implémentation et ensuite pouvoir les utiliser en sein de notre application.

Lors de la phase de recherche, il a été appris qu'un parser est un élément important pour fournir du support de langage.
Heureusement, des outils existent pour en générer depuis une grammaire. Cela a pour intérêt majeur de faire gagner du temps de développement en accélérant grandement la mise en place d'un support de langage.
ANTLR s'est donc avéré être une composante dominante du projet. De plus sa gestion des erreurs fournit par défaut permet d'avoir une meilleure tolérance concernant celles-ci.
Ce qui est avantageux, car les erreurs sont relativement fréquentes dans un éditeur.

Nous avons fait le choix de travailler uniquement dans un environnement VS Code mais fournissant toutes les clés pour des extensibilités futures.
VS Code est un environnement riche et complexe, mais qui est extrêmement bien documenter et agréable à utiliser.
Cependant, Il est quand même parfois difficile de savoir ce que peut faire cet éditeur par défaut ou non.

La grammaire a aussi été un élément important.
Pour pouvoir fournir du support pour un langage, on est obligé de connaitre l'étendue du langage sur laquelle ces fonctionnalités porteront.
Nous avons pu reprendre une implémentation concrète et l'avons adaptée pour couvrir nos besoins.

L'outil de complétion c3 a aussi été une très bonne découverte et se couplait bien avec notre environnement.

Chaque langage ayant ses spécificités, les besoins peuvent varier les uns des autres.
Les mêmes fonctionnalités entre deux langages peuvent avoir des comportements relativement différents.
L'avantage d'avoir travaillé sur un langage de sérialisation comme UON est que cela est relativement moins complexe à gérer qu'un langage de programmation.
Mais, cela nous oblige à découvrir et prendre conscience ce qu'il est possible de faire plus généralement.
UON étant plus complexe que ces concurrents sur plein de points.

Pour finir, je dirais que c'est un des plus grands projets sur lequel j'ai été amené à travailler seul.
J'ai donc beaucoup appris et me servirais de ces leçons pour la suite.

Pour toutes ces raisons, travailler sur ce projet s'est révélé être très intéressant.

%passer trop de temps sur la coloration....

\section{Difficultés rencontrées}

%  Des implémentations pas compliqué mais peu de documentation sur antlrts dans le cadre d'une extension + faut trouver comment faire en testant parfois
%    Difficulté :
%    Regrouper toutes les sources.
%    Grand projet personnel....
%    Analyse des resultat

Même si des extensions fournissant du support de langage existent et sont facilement disponibles, j'ai remarqué en analysant leur code source qu'il n'y a pas de consensus sur la manière d'implémenter une fonctionnalité (ex : complétion).
C'est donc difficile de savoir si on effectue la "bonne" méthodologie.

Concernant le générateur de parser ANTLR, c'est un outil puissant, mais assez complexe à prendre en main.
Au début du projet, certaines règles de la grammaire étaient malformées est causaient des erreurs qui n'était pas facilement perceptibles.
De plus, comme c'est une implémentation en Typescript qui est utilisée, peu d'exemples existent et cela a compliqué le travail lorsque des erreurs survenaient.

Adapter la grammaire a été un challenge. Il a été nécessaire de modifier certaines règles, car le comportement par défaut posait parfois problème. ANTLR étant différent de Lark.
Un point contraignant lorsque l'on développe un support de langage, et que la grammaire est intrinsèquement liée à certaines fonctionnalités.
Un changement peut donc avoir des répercussions sur l'ensemble de celles-ci et j'ai malheureusement perdu du temps à corriger ce genre d'erreurs.
La gestion des deux formats (JSON et YAML) s’est aussi révélée être assez complexe, due à la gestion des indentations à traiter.

Les fonctionnalités qui ont requis le plus de travail ont été la complétion et l'outline view.
Pour la complétion, il a été nécessaire d'analyser le fonctionnement du moteur pour comprendre ce qu'il était possible de faire.
Même si un usage assez basique s'est révélé adapté pour notre utilisation, il est important de savoir comment étendre une solution.
Un autre point complexe a été la manière de trouver le bon indice, car des situations particulières pouvaient fausser le résultat attendu.
Pour l'outline view, il a fallu comprendre comment construire un objet en cours d'un parcours DFS. Et ajouter la gestion des propriétés avait rajouté une couche de complexité supplémentaire.

Plus globalement, implémenter des fonctionnalités s'est avéré assez complexe. Car on est constamment confronté à des cas que l'on n’a pas pensé, il est compliqué de tout prévoir.
Il est aussi difficile de savoir quel comportement est acceptable ou non et quand il faudrait s'arrêter d'apporter des ajustements.

Du temps a également été passé pour essayer d'améliorer le comportement des erreurs à l'aide du fichier \emph{UonCompletionErrorStrategy.ts} sans succès. Le comportement par défaut n'étant jamais réellement amélioré.

La rédaction du rapport a aussi représenté un challenge. Il s'agit d'un travail conséquent étant donné l'ampleur d'un tel projet.

\section{Améliorations possibles}

Le linter a partiellement été implémenté et se base sur la détection des erreurs d'ANTLR.
Les messages d'erreurs sont ceux qu'ANTLR affiche en cas d'erreur et ne sont très "user-friendly". Il faudrait donc adapter ces messages.
Il faudrait aussi pouvoir proposer des corrections d'erreurs (ex : quickfix) plus avancées.

Le fichier \emph{UonCompletionErrorStrategy.ts} ne fait que surcharger le comportement par défaut. Si le comportement doit être amélioré, c'est ce fichier qui doit être modifié.

\section{Erreurs existantes}

La liste des erreurs connues est la suivante :

\begin{itemize}
    \item Imbriquer des mapping ou sequences en format YAML ne fonctionne pas correctement. Cela vient de la gestion des indentations qu'il faudrait corriger.
    \item L'éditeur affiche un caractère erroné (ondulation rouge) au début du document lorsque le fichier est vide. Cela vient aussi de la gestion des indentations.
\end{itemize}


%% \vfil
%% \hspace{8cm}\makeatletter\@author\makeatother\par
%% \hspace{8cm}\begin{minipage}{5cm}
%%if
% Place pour signature numérique
%%\printsignature
%%fi
%% \end{minipage}
%% \clearpage

%% \let\cleardoublepage\clearpage
%% \backmatter

\label{glossaire}
\printnoidxglossary
\addcontentsline{toc}{chapter}{Glossaire}

\printbibliography
\addcontentsline{toc}{chapter}{Bibliographie}

%% \label{index}
%% \printindex

\end{document}
