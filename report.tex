\documentclass[
    iict, % Saisir le nom de l'institut rattaché
    il, % Saisir le nom de l'orientation
    %confidential, % Décommentez si le travail est confidentiel
]{heig-tb}

\usepackage{float} % pour forcer l'empalcement d'une image
% https://tex.stackexchange.com/questions/8625/force-figure-placement-in-text

\usepackage{titlesec}

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\usepackage[nooldvoltagedirection,european,americaninductors]{circuitikz}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{pdfpages}

\usepackage{dirtree}
\usepackage{caption}
\usepackage{subcaption}


%https://www.overleaf.com/project/62b06d8b8257fff1974999df
%\signature{mbernasconi.svg} TODO : Gérer la taille
\signature{va_signature.png}

\makenomenclature
\makenoidxglossaries
\makeindex

\addbibresource{bibliography.bib}

\input{nomenclature}
\input{acronyms}
\input{glossary}
\input{meta}

\surroundwithmdframed{minted}

%% Début du document
\begin{document}
\selectlanguage{french}
\maketitle
\frontmatter
\clearemptydoublepage

%% Requis par les dispositions générales des travaux de Bachelor
\preamble
\let\cleardoublepage\clearpage
\authentification
\let\cleardoublepage\clearpage

%% Résumé / Version abbrégée
\begin{abstract}
    \input{abstract}
\end{abstract}

%% Sommaire et tables
\listoffigures
\addcontentsline{toc}{chapter}{\listfigurename}
\listoflistings
\addcontentsline{toc}{chapter}{Liste des codes sources}

\tableofcontents

\printnomenclature
\clearemptydoublepage
\pagenumbering{arabic}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{1pt}

\fancyhead[R]{Support du langage UON}
\fancyhead[L]{\itshape\nouppercase{\leftmark}}

\renewcommand{\chaptermark}[1]{\markboth{\MakeUppercase{#1}}{}}

\renewcommand\footrulewidth{1pt}

\fancypagestyle{plain}{%
    \fancyfoot[R]{Page \thepage/\pageref{LastPage}}
}
\fancyfoot[R]{Page \thepage/\pageref{LastPage}}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\titlespacing*{\chapter}{0pt}{-40pt}{20pt}

%% Contenu
\mainmatter
\chapter{Introduction}

%"Contexte" qui explique comment s'insère ce travail de Bachelor. C'est quoi UON est-ce que quelqu'un a déjà travaillé dessus, ce qu'il a fait, ce qu'il reste à faire.

Ce projet de Bachelor consiste à fournir du support pour le nouveau langage de sérialisation UON, sous VS Code.
UON (abréviation de Unified Object Notation) est un format de sérialisation qui vise à rassembler les meilleures caractéristiques de tous les formats de sérialisation disponibles sur le marché, pour créer celui qui les unifie tous.

Ce projet consiste donc à fournir à l'utilisateur, les outils pour lui permettre la rédaction la plus optimale d'un fichier UON dans l'éditeur VS Code.

C'est un nouveau langage complexe et une spécification détaillé mais non terminé existe. Il reste maintenant à construire les éléments permettant son utilisation.

Un parser en Python à déja été implémenté par Stéphane Selim.
Le travail précédent s'était concentrer sur trois aspects : La sérialisation/deserialisation binaire et textuelle d'objet en python, la cohércient et la validation.
Cepedendant ce travail de bachelor n'en est pas la suite directe.

Il consite à founir une autre composante qui est de permettre l'utilisation du langage UON dans un éditeur pour qu'il soit utilisé par un plus grand nombre.

Nous nous focaliserons uniquement sur VS code dans un premier temps et expliquerons pourquoi plus tard nous avons choisi cet éditeur.

Ce projet consite à étudier les différentes approches et les mettre en oeuvre. Nous spécifierons également les alternatives et améliorations possible quand cela est pertinent.

Il se veut aussi être expérimental et aussi ouvert à la discussion.
Aucun cadre précis sur sa réalisation n'a été établi et les approches choisies sont libres tant qu'elles respectent le cahier des charges.

Dans les chapitres suivants, nous allons :

\begin{itemize}
    \item Se renseigner concernant ce qui est fourni par VS Code pour élaborer et déployer une extension.
    \item Se renseigner sur les éléments à prendre en compte pour pouvoir fournir du support pour un langage.
    \item Chosir les aspects du langage UON à traiter dans le cadre de ce projet.
    \item Documenter et réaliser l'implémentation d'une extension, son intégration continue ainsi que ses fonctionnalités.
    \item Documenter et mettre en place les tests effectués pour les différents composants du projet.
    \item Documenter également des pistes de réflexions, les alternatives,et améliorations possible.
\end{itemize}

\let\cleardoublepage\clearpage

\chapter{Cahier des charges}
Voici ci-dessous un résumé du cahier des charges :

\textbf{Extension}
\begin{itemize}
    \item Code source publié sur Github
    \item Fournir une intégration continue
    \item Déployer l'extension sur \href{https://marketplace.visualstudio.com/}{le marketplace} de Visual Studio
\end{itemize}

\textbf{Fonctionnalités à implémenter}
\begin{itemize}
    \item Syntax highligting
    \item Auto-complétion
    \item Document Outlining
    \item Hover Information
\end{itemize}

\textbf{Si le temps le permet}
\begin{itemize}
    \item Lint
    \item Formattage : Outil permettant de formatter le code
    \item Converter : Outil permettant de convetir du code
\end{itemize}

\vspace{\parskip}

Concernant les fonctionnalités à implémenter. Il n'y avait pas de contraintes de choix.
Les fonctionnalités choisis ont donc été ceux paraissant être les plus utiles à avoir dans un premier temps, en prenant en considération la contrainte de temps.

\chapter{Pré-étude}

\section{Parsing}
Le travail ne portant pas sur cette élément en particulier, nous n'allons pas rentrer en détail concernant ce sujet.
Mais il est important de rappeler les concepts prinicipaux, car ces éléments présentés ci-dessous nous seront très utile et il est important de bien en comprendre le fonctionnement.

Afin de pouvoir intérpréter du code, on passe par plusieurs étapes.

La première consite à une analyse lexical à l'aide d'un Lexer. On va décomposer un input (notre code ou texte) en une liste de token.
Ces tokens seront ensuite utilisés lors de l'analyse syntaxique. L'analyse syntaxique consiste à regarder quelles sont les règles qui sont applicables pour une suite de tokens définie dans une grammaire.

Le résultat de cette analyse est un "Parse Tree" qui représente toutes les dérivations de notre input en fonction de la grammaire.
Elle est représenté sous forme hiérarchique.

C'est cette strucutre final qui sera intérpté ou compilé.

Le flow d'exécution standard est représenté par la figure \ref{interpreter-flow}.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=12cm]{assets/figures/interpreter-flow.PNG}}
    \end{center}
    \caption[interpreter-flow Anatomy]{\label{interpreter-flow} interpreter flow}
\end{figure}


\section{VS Code}
% TODO :
% Peut être élaborer un peu plus :
% c'est quoi VS Code, pourquoi a-t-il été choisi ?
% Est-il populaire ?
% En quel langage a-t-il été écrit. Est-il entièrement open-source ?
% est-ce qu'une extension peut être complètement indépendante de la base de code ?
% Pour le rapport final j'attends plus de détails

Il a été choisi car il fait partie des éditeurs les plus populaires. C'est également l'éditeur que j'utilise le plus personnellement et donc je suis déja familier de son environement.
D'autres alternatives existes comme VIM et Atom pour ne citer qu'eux. Et chacun d'eux fourni également de quoi implémenter une extension.
C'est donc un choix personnel.

Visual Studio Code (ou VS Code) est un éditeur de code multi-plateforme, open source et gratuit.

Il est pensé pour être extensible (de L'UI à l'expérience utilisateur), presque tout peut être customisé et amélioré à travers de L'Extension API \cite{extension-api}.
On se focalisera sur le theming, les fonctionnalités de langages, la publication de l'extension et les tests.

VS Code fournit des documentations détaillées sur plein de sujets :  étapes pour la création d'une extension, son déployement continue ainsi que le support de langage en lui-même.
Il est aussi possible facilement de trouver des guides, ainsi que des exemples de codes.
Il s'agit donc évidemment de la source principale des explications données dans ce document concernant ce qui touche à cette éditeur.
% TODO : code sample https://github.com/microsoft/VS Code-extension-samples
La communauté est aussi très active (stack overlow, gitter, issue github, etc.).

Le code source de VS Code de Microsoft est open source (sous licence MIT permissive).
Il a été écrit en Typscript et Javascript. Une extension peut être également écrite dans ces deux langages.
Nous choisirons le Typescript car VS Code recommande ce langage. Mais certaines composantes externes (langage server) peuvent être écrit dans n'importe quel langage. Tant qu'elle fournisse l'API adéquate pour communiquer.

% est décris selon wikipédia
Un éditeur de code est un logiciel destiné à la création et l'édition de fichiers textes.
Contrairement à un IDE (Environnement de développement) qui permet en plus la compilation et le debuggage de programme plus complexe.
Nous manipulerons des fichiers statiques par la nature même de UON qui est un langage de sérialisation.

\subsection{Extension Anatomy}
Il y a tois concepts cruciaux à comprendre pour réaliser une extension.

\begin{itemize}
    \item \textbf{Activation Events}: Des événements à partir desquels l'extension devient active \cite{activation-events}.
    \item \textbf{Contribution Points}: Des déclarations statiques qui sont faites dans l'Extension Manifest "package.json" pour étendre l'extension. Il s'agit d'un ensemble de déclarations JSON faites au travers du champ "contributes" \cite{contribution-points}.
    \item \textbf{VS Code API}: Un ensemble d'API JavaScript que nous pouvons invoquer dans le code \cite{vs-code-api}.
\end{itemize}

En général, l'extension est une combinaison de plusieurs \emph{Conntribution Point} et l'utilisation de l'API pour étendre les fonctionnalités de VS Code.

\subsubsection{Extension File Structure}\label{Extension File Structure}

La structure de base d'une extension est illustré par la figure \ref{basic VS Code extension structure}.

% https://tex.stackexchange.com/questions/98703/framebox-and-subfigure-with-dirtree-package
\begin{figure}[H] % \begin{figure}[!h]
    \centering
    \framebox[\textwidth]{%
        \begin{minipage}{0.9\textwidth}
            \dirtree{%
                .1 ..
                .2 vscode.
                .3 launch.json     // Config for launching and debugging the extension.
                .3 tasks.json      // Config for build task that compiles TypeScript.
                .2 .gitignore.
                .2 README.md.
                .2 src.
                .3 extension.ts   // Extension source code.
                .2 tsconfig.json  // TypeScript configuration.
            }
        \end{minipage}
    }
    \caption[basic VS Code extension structure]{\label{basic VS Code extension structure} basic VS Code extension structure}
\end{figure}

\textbf{Extension Manifest} :
chaque extension doit contenir un fichier \emph{package.json}. En plus des champs propres à Node.js, on peut spécifier des scripts, dépendances de développement et des champs spécifiques à VS Code.

\textbf{Extension Entry File} :
il s'agit du fichier prinicipal de l'extension (Extension.js).
De base il contient 2 fonctions : \emph{activate} et \emph{deactivate}.
La fonction activate est executée à l'activation de notre extension par un \emph{Activation Event}. On initialisera ici notre extension.
La fonction \emph{deactivate} est exécutée lorsque l'application devient inactive et sert principalement à nettoyer le code avant la désactivation de l'extension

\textbf{Remarque} : le niveau de customisation est assez élevé. La seule limitation indiquée et qu'il n'est pas possible d'accéder au DOM de l'éditeur.

Nous avons donc vu les briques de bases pour la conception d'une extension. Mais cette base est suffisante pour commencer à travailler.

Si vous souhaitez vous renseigner d'avantages sur l'élaboration d'une extension vous pouvez consulter la page \href{https://code.visualstudio.com/api}{suivante}.
Concernant les fonctionnalitls générales de l'éditeur VS Code vous pouvez consulter consulter \href{https://code.visualstudio.com/docs}{la documentation officielle}.

\section{Support de langage}
VS Code fournit la possibilité d'ajouter du support pour un nouveau langage de programmation au travers d'implémentation de fonctionnalités. Ces fonctionnalités peuvent être classées en deux catégories détaillées
dans dans les sections \hyperref[Declarative language features][Declarative language features] et \hyperref[Programmatic language features][Programmatic language features] ci-dessous.

\subsection{Declarative language features}\label{Declarative language features}
Elles ajoutent un support d'édition de texte de base pour un langage de programmation.
Par exemple, les éléments suivants :

\begin{itemize}
    \item syntax highlighting
    \item snippet completion
    \item bracket matching
    \item bracket autoclosing
    \item bracket autosurrounding
    \item comment toggling
    \item auto indentation
    \item folding (by markers)
\end{itemize}

Il s'agit de fonctionnalités implémentées à l'aide de fichier de configuration.

% TODO : faire ref à l'explication ci-dessus ?
Puis, elles doivent être enregistrées comme \emph{Contribute Point}.

\subsection{Programmatic language features}\label{Programmatic language features}
Il s'agit de fonctionnalité plus riche et plus complexe à mettre en place (p.ex : "Hovers", "Go to Definition", "diagnostic errors", "IntelliSense" \space et "CodeLens".
Il y a deux approches pour les implémenter que nous allons voir ci-dessous.

\subsubsection{VS Code API (Direct implementation)}
La première solution est d'utiliser \href{https://code.visualstudio.com/api/references/vscode-api#languages}{l'API de VS Code} pour les fonctionnalités de langage.
Cette API permet d'implémenter directement les fonctionnalités du langage.
% TODO : s'assurer que l'api vscode une implementation de LSP ?
% Comment faire la transition entre mon implémentationa avec celle d'un langage server

Beacoup de fonctionnalités se font en s'inscrivant à des providers depuis notre application.
L'éditeur de code fera ensuite les requêtes à ces providers lorsque cela sera nécessaire.

\subsection{Language server}

La seconde solution est de fournir nous-mêmes ces méthodes en respectant la spécifiation LSP \cite{lsp-specification} au travers d'un langage serveur.
Les avantages souvent mentionnés ce cette approche sont que le langage server peut être écrit avec le langage que l'on souhaite et
que cela permet aussi à d'autres éditeurs de texte compatibles avec le langage server d'utiliser ces fonctionnalités sans devoir les implémenter de nouveau.

Pour être utilisé sur VS Code, un langage server à deux parties :
\begin{itemize}
    \item \textbf{Un client} : c'est une extension écrite en Javascript ou Typescript qui à accès à tous les endpoints de VS Code.
    \item \textbf{Langage server} : un outil d'analyse linguistique fonctionnant dans un processus séparé.
\end{itemize}

\vspace{\parskip}

Le client et le serveur communiquent à l'aide du protocole LSP (pour "language server") dès que des informations devraient être fournies à l'éditeur.

L'implémentation d'un tel serveur peut être libre en respect avec la spécification, mais des implémentations existent déjà. Telle que \emph{LSP4} implémenté en Java ou \emph{VS Code-languageserver-node}
implémenté en Typescript qui n'est pas exclusivement réservé à VS Code comme son nom pourrait l'indiquer.

Un langage server peut être utilisé sur d'autre éditeur compatible mais le client devra être implémenté de nouveau pour chaque éditeur.
Cette approche est aussi plus complexe à implémenter.

\subsection{API vs LSP}\label{api vs lsp}

Le schéma suivant \ref{api vs lsp} montre la correspondance des méthodes entre la première et seconde approche.

Cette liste est non exhaustive, mais permet de montrer l'équivalence des deux solutions.

\begin{figure}[!ht]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/api-vscode.png}}
    \end{center}
    \caption[API vs LSP]{\label{api vs lsp} VS Code API vs LSP method}
\end{figure}

\subsection{Parser}
% TODO : bcp de fonctionnalités, pas clair...

Pour fournir des fonctionnalités de langue, un point souvent mentionnée est l'utilisation d'un parser.
Car l'arbre généré par celui-ci contient des informations concernant la structure du document utiles pour certaines fonctionnalités.
% TODO arbre / parse tree / ast ?

Cependant dans le cadre de son utilisation dans un éditeur ou IDE, il devrait être "fault tolerant".
Car la plupart du temps, le code dans l'éditeur est incomplet et syntaxiquement incorrect.
Mais l'on s'attend tout de même que certaines fonctionnalités continuent à fonctionner (ex : la outline view).

Le parseur doit donc pouvoir générer au mieux un parse tree qui à du sens depuis du code erroné.

On pourrait également se demander s'il est possible de se passer d'une telle strucutre et de n'utiliser à la place uniquement des expression régulières.
Cependant un problème majeur survient rapidement si on choisit cette approche, la récursivité devient extrèment difficle à traiter.

"you cannot find a (regular) expression inside another one, unless you code it by hand for each level. Something that quickly became unmaintainable."

Example : % TODO

\subsubsection{Génration de parser}
Des outils permettant la génération d'un parser existent à partir d'une grammaire.

% TODO

\section{Choix technologiques}
Nous allons maintenant mentionner les technologies et approches choisies et expliquer les raisons.

\subsection{Direct implementation}
L'implémentation d'un langage serveur n'étant pas une priorité et pour se concentrer sur la réalisation des fonctionnalités, la solution de contacter directement l'API de VS Code sera choisi.
Si le temps le permet, toute la logique du code concernant les fonctionnalités riches\ref{Programmatic language features} pourrait être déplacée dans un langage server.

%%Beaucoup de possibilités existent comme mentionné au point \hyperref[api vs lsp]{API vs LSP}, mais nous utiliserons uniquement les providers suivants :
%%\begin{itemize}
%%    \item \textbf{registerCompletionItemProvider} : pour afficher des suggestions de complétions.
%%    \item \textbf{registerHoverProvider} : pour gérer le hover.
%%    \item \textbf{registerDocumentSymbolProvider} : pour afficher les éléments dans la Outline View.
%%\end{itemize}

\subsection{ANTLR}

ANTLR (ANother Tool for Language Recognition) est un générateur de parser, il est actuellement dans sa quatrième version.

Il a été créé par Terence Parr à l'Université de San Francisco.

Il est très utilisé, autant dans le monde académique que profesionnel.

L'algorithme de parsing utilisé dans sa version la plus récente est le "Adaptive LL(*)" qui est décrit comme amélioration de l'algorithme
"LL(*)" utilisé dans ssa version précédente.

% https://en.wikipedia.org/wiki/LL_parser
% https://www.antlr.org/papers/allstar-techreport.pdf

Depuis une grammaire, qui est un fichier décrivant formellement le langage, ANTLR génère le code permettant de reconnaitre ce langage.
C'est à dire en pratique un lexer et un parser. % TODO ref ?

La grammaire est décrite en utilisant la notation EBNF (Extended Backus–Naur Form).
% lien, citation ?

Il n'y a seulement qu'un seul outil qui permet de générer le code des lexer et parsers pour tous les langages cibles, et il est écrit en Java.

Pour pouvoir utiliser cet outil dans un environnement autre que java, il est nécessaire de l'utiliser dans un runtime (environnement d'éxécution) propre au langage shouhaité.
%TODO : citer target.md de antlr et repo antlr4ts + mega thread

Lorsque que la grammaire a été défini, on peut demander à antlr de générer des parsers dans différents langages.

ANTLR est composé de deux parties principales : l'outil en java qui permet de générer le lexer et le parseur, aisni que l'environnement d'éxecution (runtime) qui permet leur exécution

%The tool will be needed just by you, the language engineer, while the runtime will be included in the final software created by you.
%Typical Workflow

Nous utilisons la version 4 de ANTLR. Elle a comme avantage sur son ancienne version (ANTL3) d'avoir la structure de la grammaire directement disponible dans le parser via
un mécanisme de machine à état ATN (Augmented Transition Network).

\subsubsection{antlr4ts}
Nous utiliserons antlr4ts, qui est un environement d'éxécution en typescript et donc parfaitement adapté pour VS Code.

Il a été conçu par Sam Harwell indépendamment de l'organisation ANTLR.
L'organisation ANTLR ne proposant pas de runtime en typescript. %lien ? ou citation
% https://github.com/antlr/antlr4/blob/master/doc/targets.md
% https://github.com/tunnelvisionlabs/antlr4/blob/master/doc/targets.md

% https://github.com/sharwell
% https://github.com/tunnelvisionlabs

% lecture
%https://stackoverflow.com/questions/41427905/how-many-ways-are-there-to-build-a-parser

C'est un outil populaire est très utilisé.

L'initialisation de base est la suivante :

\begin{lstlisting}[frame=single,caption={antlr-setup},label={antlr-setup}]
    let inputStream = new ANTLRInputStream("var c = a + b()");
    let lexer = new ExprLexer(inputStream);
    let tokenStream = new CommonTokenStream(lexer);

    let parser = new ExprParser(tokenStream);
    let errorListener = new ErrorListener();
    parser.addErrorListener(errorListener);
    let tree = parser.expression();
\end{lstlisting}

\textbf{Remarque} :
Comme mentionné plus loin au chapitre \hyperref[grammar scope]{4}. Le précédent travail de Bachelor s'était concentré sur l'utilisation d'un parseur généré depuis une spécifiation Lark.
L'implémentation de son parser n'a pas été privilégié car il aurait été nécessaire de créer l'infrastructure permettant de l'utiliser.
Et antlr4ts semble être un choix plus naturel dans notre environnement VS Code.


\subsubsection{Gestion des erreurs}

% L'idée gérnéral et d'insérer de faux token ou de supprimer les tokens non valides jusqu'à ce qu'il en trouve un qui corresponde à set de tokens connu.

% https://www.antlr.org/api/Java/org/antlr/v4/runtime/ANTLRErrorStrategy.html
Antlr définit une interace pour définir une startégie concernant les erreurs recontrés durant une analyse syntaxique nommé "ANTLRErrorStrategy".
On peut distinguer trois types d'erreurs :
\begin{itemize}
    \item L'analyseur syntaxique n'a pas pu déterminer le chemin à prendre dans le ATN (aucune des alternatives disponibles ne pouvait correspondre).
    \item L'input actuel ne correspond pas ce que nous attendons.
    \item Un prédicat évalué comme faux
\end{itemize}

% TODO
Nous utiliserons le "DefaultErrorStrategy". Il a la particularité d'ignorer un token invalide pour passer au suivant.

% TODO : expliquer la classe qui nous permet de recevoir des informations en cas d'erreur
Puis on peut être notifié de ces erreurs en liant notre parser à un listener "notifyErrorListener".

% TODO
%Proposition de correction à l'aide de antlr qui nous fourni la position dans le document
%Regarder pourquoi ici on la ligne + colonne et pourquoi on pouvait pas l'avoir dans la outline view
%On utilise également une fonction et non pas un provider pour afficher les erreurs



%citation ?
ANTLR4 est capable de signaler des erreurs et se rétablir d'erreurs plus grave qui pourrait faire échouer le processus de parsing.

ANTLR défnit l'interface \emph{ANTLRErrorStrategy} pour ce problème.

Par défaut c'est l'implémentation \emph{DefaultErrorStrategy} qui est appliqué et qui consite à supprimer un token qui poserait problème ou en rajouter un s'il manquerait, avant de
se resyncroniser à l'ATN et de continuer le parsing.


À chaque changement d'état  il lance le processur de syncronisation et applique le \emph{defaultErrorStrategy} si besoin.

Comme mentionné ici \dots
Le parser va appliquer des règles.
Cette resyncronisation permet de continuer dans la règle courante.

S'il ANTLR n'a pas pu se rétablir et qu'il a encore des erreurs, on va consommer tout les tokens qui suit celui qui a posé problème. 
Le mécanimse s'arretera quand il réussira à trouver un token qui lui permettra de quitter la règle de parser courrante sans la valider mais permettant de continuer le processus de parsing.
Au pire des cas il s'agira d'un token EOF (retour à la ligne).

C'est grâce à ce mécanisme qui fait que si nous utilisons le DefaultErrorStrategy nous obtiendrons un parse tree dans tous les cas.

Malheursement je n'ai pas pu explorer plus en profonceur le mécanisme complexe de resyncronisation.
Cela pourrait se revélerr intéressant pour tolérer plus d'erreurs au lieu d'une seul pour l'instant.

%avec
%tree.toStringTree (uon (root_value (json_collection (json_map !map { (json_pair (pair_key (string (literal name)) (presentation_properties ( oups (presentation_property (description description : (string firstname))) ))) : (json_value (scalar (string_scalar (string paul))))) }))))


%sans, on constate que sans la suppression on sort de la règle...

%tree.toStringTree (uon (root_value (json_collection (json_map !map { (json_pair (pair_key (string (literal name)) (presentation_properties ( oups description)) : (json_value (scalar (string_scalar (string firstname))))) ) : paul } \n))))


%https://medium.com/pragmatic-programmers/altering-antlrs-error-handling-strategy-80ce7d046730
%* The default implementation resynchronizes the parser by consuming tokens
%* until we find one in the resynchronization set--loosely the set of tokens
%* that can follow the current rule

%https://www.antlr.org/api/Java/org/antlr/v4/runtime/DefaultErrorStrategy.html
%https://pub.dev/documentation/antlr4/latest/antlr4/DefaultErrorStrategy-class.html

%https://stackoverflow.com/questions/18550031/how-to-control-error-handling-and-synchronization-in-antlr-4-c-sharp/18553706#18553706
%https://github.com/antlr/antlr4/blob/master/runtime/Java/src/org/antlr/v4/runtime/DefaultErrorStrategy.java


% https://stackoverflow.com/questions/25467627/how-to-continue-parsing-after-an-error-in-antlr-4-4  
%Antlr 4's parser error strategy is to drop tokens from the input until it can detect a sane state and then
% it continues parsing. Looking at your example, while trying to recover maybe it never reaches a sane state before EOF.

%https://pub.dev/documentation/antlr4/latest/antlr4/DefaultErrorStrategy-class.html

\subsubsection{ATN}

Comme mentionné il s'agit de machine à état \dots

Ci-dessous Un ATN d'une séquence :

\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=12cm]{assets/figures/seq_ATN.png}
    \end{center}
    \caption[ATN d'une séquence]{\label{seq_ATN} ATN d'une séquence}
\end{figure}


%% UON se veut ête moins ambigue que ces alternatives pour l'utilisateur\dots
\section{UON}
UON est essentiellement un langage de sérialisation qui est un superset de JSON et un superset partiel de YAML.
Il cherche à regrouper les meilleures caractéristiques de ces formats de sérialisation en un seul format.
Il fournit également des fonctionnalités supplémentaires utiles pour augmenter l'interopérabilité entre différents types de dispositifs, valider les données ainsi que pour diminuer le payload.

Nous allons voir ci-dessous un aperçu général de ce langage.

\subsection{Format de sérialisation}

Lorsqu'on sérialise des données, elles se retouvent soit sous forme binaire ou soit textuelle.
Ce projet se focalisera naturellement uniquement sur le format textuelle car c'est le seul format, humainement interprétable, que l'on manipulera dans un éditeur.
Le rôle d'un format de sérialisation et de pouvoir représenter des données sous une autre forme pour être ensuite manipuler.

Un éditeur ne manipulant qu'un fichier statique, les étapes de sérialisation et désérialisation ne se feront pas à notre niveau.
Pour pouvoir profiter de ces fonctionnalités, cela nécessite d'avoir un environement de programmation (ex : Python).

% TODO : Sérialisation
% TODO : Desérialisation

% TODO : Parler des similutes différences
%\subsection{JSON}
%\subsection{YAML}
%\subsection{XML}

\subsection{Pourquoi ?}
Son rôle est d'être utilisé dans l'industrie 4.0. Plus particulièrement dans la communication m2m (machine to machine) ainsi que pour l'IoT (Internet of Things).
Ces communications nécessitent souvent de communiquer entre de petits appareils dont la puissance de calcul est très limitée.
La perspective de disposer d'un protocole de communication applicatif à la fois interopérable et adapté aux échanges à faible puissance est très appréciable.
Parce que % TODO

\subsection{Communication}
Lorsque deux appareils veulent échanger des informations, ils disposent de deux canaux de communication :
\begin{itemize}
    \item Un canal de communication en ligne utilisé pour la transmission des données de contenu (payload). Les données peuvent être représentées sous forme humaine et binaire.
    \item Un canal de communication contractuel utilisé pour l'accord sur la description des données (schema).
\end{itemize}

\vspace{\parskip}

À la figure \ref*{data-exchange}, on a à gauche deux dispositifs : un capteur de température à faible consommation
et une puissante passerelle domotique. Pour réduire la taille du payload, les deux dispositifs peuvent convenir d'un schéma qui décrit le format des données.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=15cm]{assets/figures/data-exchange.png}}
    \end{center}
    \caption[UON data exchange]{\label{data-exchange}UON data exchange}
\end{figure}

À droite on affine le schéma. Pour réduire davantage les données transmises.
Par exemple, en disant que la température exprimée avec un nombre est maintenant une valeur non signée de huit bits exprimée en degrés Celsius.

\subsection{Design}
UON est complètement conforme avec JSON et partiellement avec YAML.

Il peut être décomposé au travers de 4 niveaux de complexités :
\begin{itemize}
    \item \textbf{UON:0} est entièrement compatible avec JSON en ce qui concerne le RFC8259.
    \item \textbf{UON:1} est partiellement conforme à YAML.
    \item \textbf{UON:2} fournit des propriétés de type, la coercition et les chaînes de caractères multilignes.
    \item \textbf{UON:3} offre des types et des références riches.
\end{itemize}

\subsection{Types}
Chaque élément est composé d'un type. Et chacun de ces types peut avoir des propriétés.
3 types de propriétés existent :
\begin{itemize}
    \item \textbf{Presentation properties} : Elles influencent la présentation d'une valeur.
    \item \textbf{Validation propeties} : Elles sont utilisés pour valider, contraindre et décrire un fichier UON.
    \item \textbf{Application properties} : Elles ne peuvent être lues que depuis UON en utilisant le type !prop. Elles sont accessibles depuis l'application (Python, JavaScript, ...). Elles sont utilisées pour générer un fichier sérialisé (binaire, ou UON), mais elles ne sont jamais explicitement transmises.
\end{itemize}

\subsection{Validation}
Dans de telles communications, la validation des données transmises entre des machines dont la puissance de calcul est très limitée est un atout majeur.
Une des propriétés uniques de UON est l'usage de schéma directement intégré dans le langage.

% TODO : Lister les différents forme de shcéma de validation

\subsection{Liens}
Si vous souhaitez vous renseigner d'avantages, la spécification complète du projet se trouve sur cette \href{https://github.com/uon-language/specification/}{page}.
Et le dépôt du projet se trouve \href{https://github.com/uon-language/specification}{ici}.

% TODO : Datatypes ?

%% Résumé rapide de la grammaire initiale + dire qu'elle est relativement bonne et qu'on a pas modifier grand chose

%  Adaptation (passé de lark à antlr) (regex, espace différents, pas de librairie pour les indentations, ...)
%   YAML
%       Retirer dans un premiers temps le yaml
%   Indentation
%   Propriétés dans les types + de units

% On va pas tout spécifier mais relever certains point de la grammaire qui est pertinent
% Apercu de la grammaire plus en détail et exmple avec arbre + Examples
% Collection
% map
% seq
% clé-valeur
% clé IDENTIFIER
% nombbre

% Faire attention de faire le découpage pour la completion

\chapter{Scope de la grammaire}\label{grammar scope}
Pour pouvoir proposer du support pour un nouveau langage, il est nécessaire de savoir sur quoi celui-ci portera exactement.
Il est donc important de préciser sur quels éléments du langage nous nous focaliserons.

Une spécifiation d'UON existe, elle couvre énormément d'aspects. Cependant, tout n'est pas détaillé et donc des points sont sujet à intérprétation.
C'est un langage complexe et qui pourrait encore, être ammener à évoluer. Les propositions et les suggestions sont les bienvenues.

Heurseuement pour nous, une grammaire concrète avait déjà été écrit en Lark par l'ancien élève Stéphane Selim pour son travail de bachelor "Parser for a serialization language UON".
Son travail se trouve sur \href{https://github.com/uon-language/uon-parser}{ici}

Nous nous appuyeront sur cette grammaire que nous convertirons en format ANTLR et ferons les modifications jugés nécessaire.

Les fonctionnalités se baseront sur celle-ci. Ceci dans le but de rester cohérents entre elles.
Nous ferons attention à son extensibilité et expliquerons les choix d'implémentations.
Elle sera complétée et améliorée après la mise en place des fonctionnalités attendues, si le temps le permet.

Cette grammaire se trouve décrite dans le fichier UON.g4 du projet.

\section{Résumé}
Pour commencer, il est nécessaire de préciser ce que permet cette grammaire initialement.
Nous utiliserons la grammaire qui a été adapté dans le format d'ANTLR pour illustrer nos exemples.

% TODO EBNF ICI ? Type 2 de grammaire ?

\subsection{root value}

Un fichier UON peut contenir trois structures différentes.

\begin{lstlisting}[frame=single,caption={uon-root-value},label={uon-root-value}]
    uon: root_value;

    root_value: json_collection | schema;

    json_collection: json_map | json_seq;

    yaml_collection: yaml_map | yaml_seq;
\end{lstlisting}

\subsubsection{Collections}
La grammaire actuelle nous permet de définir des collections. Ces collections sont des mapping et séquences soit dans un format proche de json ou de yaml.

UON est par nature proche du format JSON. Mais UON étant "YAML compliant" à certain niveau, mon précesseur avait jugé utile d'incorporer ce format
dans le langage. Cette strucure a été repris mais expliquons à la section ? %TODO
les modifications nécessaire qui ont été apporté.

\textbf{Remarque :}Par simplicité nous dirons qu'il s'agit du format "json" ou "yaml" lorsqu'on les mentionnera.

Ces collections peuvent être imbriqué par format, on ne peut pas avoir dans un fichier une collection en format json et yaml en même temps.

% TODO : exemple de map

\begin{lstlisting}[frame=single,caption={json-map},label={json-map}]
    {
        "name" ( description : a12 ) :  !str( comment : ok , optional: False) ok, # error with outline with simply 12
        age: !float !float128 !float32 +12.2 C,
        hobbys ( description : "what i like the most",  optional : false ) : !seq [
            "cinema", "sport", "etc..."],
        license  :  !bool  true,
        car:   !bool false,
        sensor ( optional : false , description : ok ) : !map {
            type: "temperature",
            parameters1: !seq [ 1, 2,3,4,5]}
    }
\end{lstlisting}

\begin{lstlisting}[frame=single,caption={yaml-map},label={yaml-map}]
ok :
    -  ok
    - !float32 12
    -
         - ok
ok2 : -23
\end{lstlisting}


% TODO : exemple de seq

\begin{lstlisting}[frame=single,caption={json-seq},label={json-seq}]
\dots
\end{lstlisting}


\begin{lstlisting}[frame=single,caption={yaml-seq},label={yaml-seq}]
\dots
\end{lstlisting}


\subsubsection{Schéma}
% TODO : schéma de validation externe \ref{}
On peut défnir également un schéma de validation. Ce schéma doit se trouver dans un autre document que les deux structures ci-dessus.

Cette séparation permet d'éviter des ambiguités. On incite comme ça à l'utilisateur à séparer la logique entre différents fichiers.
Un schéma est un document qui décrit la spécification d'un nouveau type.

\begin{lstlisting}[frame=single,caption={yaml-seq},label={yaml-seq}]
    \dots
\end{lstlisting}

\subsection{Valeurs}

\subsubsection{Types}
Les terminaux peuvent être des string, des booleans ou des nombres.

\subsubsection{Propriétés}
Les clés des mapping des propriétés peut posséder des propriétés de présentation.
Les types dans un schéma de validation peut posséder des propriétés de valdation.

\section{Modification}

%Les valeur des clé-valeurs peuvent être null mnt
%les types peuvent avoir des propriétés dans le payload
%quelques unités ont été rajouté

% TODO
Uon est un langage comprenant plusieurs composantes est particulatités qui ne peuvent pas tous être traiter dans un simple éditeur de texte. (ex : cohercien)
%

Nous allons garder presque la même structure car cette grammaire regroupe déjà des éléments importants du langage UON.
Cependant quelques points concernant la strucutre général ont été modifiés.

Pour commencer il a fallu convertir la grammaire dans à la main car aucun convertisseur existe. Heursement ANTLR et Lark se base sur une notation EBNF.
Ce qui fait que la strucure reste relativement semblable.

Le seul point de divergence, à été sur l'utilisation d'expressions régulières car la syntaxe de celle-ci varie légèrement. Et sur la gestion de l'indentation expliqué % ici todo

Concernant la structure d'un fichier de grammaire ANTLR .g4 il y a quelques subtilités à connaitre :

Le fichier doit comporter le même nom que la règle de départ. %UON.g4 -> uon

Les règles de lexer définisse comment une chaine de texte doit être tokenifier. Elle doivent être écrit en majuscule !
Les règles de parser définisse comment une suite de token sera intérpréter. Elle doivent être écrit en miniscule !

% \textbf{Attention} : Utiliser des majuscules pour une règle, correspond à définir un "Lexer rule" et utiliser des minuscules à un  "Parser rule".

%Par défaut un fichier de grammaire peut contenir des règles de lexer et de parser. Mais l'on peut les séparer pour améliorer la visibilié.

%On peut ignorer des tokens.

%fragement

%WS: [ \n\r\t] -> channel(HIDDEN);

%LINE_COMMENT: '#' ~[\r\n]* -> skip;

\subsection{YAML}
%TODO : permet
La grammaire permettait deux notations différentes pour les collections : une proche de json et l'autre de yaml.

%Car UON est un superset de JSON et un set partiel de yaml.
%La spécification indique bien que UON doit être compatible avec yaml 1.2 sans toutefois mention explicitement tous les points
% TODO

% Il a été décidé au milieu du projet, de se focaliser d'abord sur le format proche de JSON, et de revenir sur cette aspect cette phase réalisé.
Il a été décidé au milieu du projet, de se focaliser d'abord sur le format proche de JSON, et de revenir sur cet aspect si le temps le permet.
Car la notation en YAML pose quelques difficultés. % posait
En JSON les retours à la lignes et indentations ne sont pas nécessaire à la compréhension du langage, contrairement à YAML.
Nous expliquons dans le chapitre suivant il faudrait faire pour cohabiter ces deux formats avec ANTLR. % nous avons fait

\subsubsection{L'indentation}

% TODO : tsconfig.json
% TODO : stratégie switch
% TODO : changement minus

Contrairement au parser lark, la gestion des indentations n'est pas géré automatiquement à l'aide d'une librairie externe.
Ce méchanisme doit malheurseuement être traité par nos soins.

La grande modularité d'ANTLR est un avantage mais peu se revéler très contraignant pour ce genres de tâches.

Mais avant de résoudre ce problème, il est nécessaire de rappeler que notre grammaire n'est pas uniforme concernant la gestion des retours à la ligne. 

Et c'est là où se trouve la vrai probématique.

En effet, UON peut-être écrit sous une forme proche de JSON ou de YAML. Le premier ne tient pas compte des indentations contrairement à la seconde.

Et la gestion des indentations nécessite de prendre en compte les retour à la ligne. % TODO newline regex

Il y a donc trois solutions qui ont été envisagées.

La première est de rajouter des token de retour ou il serait jugé bon de les avoir, et de les rendres optionnel.
Mais ce n'est pas très propre et on risque d'en placer énormément.

Le deuxième serait d'ignorer tout simplement les erreurs. C'est à dire que si on à un token de retour à la ligne qui ne devrait pas être là, alors l'ignorer tout simplement.
On pourrait penser ce mécanisme simple se révelerait efficace. Cependant je n'ai malheurseuement pas réussi à faire que le moteur de complétion c3
puisse ignorer ces erreurs.

La troisième solution trouvée et celle actuellement utilisée. Elle consite à prendre en compte uniquement les retours à la ligne si le lexer à détecter les tokens initiaux
nous permettant de savoir nous avons à faire à une construction proche du JSON ou du YAML et donc savoir si les retours à la ligne doivent être pris en compte ou non.

Par exemple, tester si le premier token est un token de type MINUS ("-"). Si c'est le cas, on peut déduire qu'il s'agira d'une séquence en format YAML.

Dès lors que les retours à la ligne existe, nous pouvons maintenant placer les tokens INDENT et DEDENT depuis le lexer
pour pouvoir gérer les imbrications correctement.

Pour ce faire, à chaque fois qu'un espace qu'un retour à la ligne est detécté par le lexer, nous prenons compte de la taille de l'espacemment qu'il contient ("\\r\\n    ")
Cet espacement nous permettra de savoir si l'on doit rajouter des INDENT ou DEDENT.

Le code permettant ce mécanisme peut être placé dans le fichier grammaire, en le placant dans le block \emph{lexer:@member{}}.
Lors de la génération le code est placé dans le lexer.

Cependant lors de la génération les imports seront manquants, il est nécessaire d'aller les rajouter manuellement.

%Le code se trouvant sur le dépot suivant à été fortement utile : https://github.com/umaranis/FastYaml

% TODO : manque la partie avec le switch si on skip ou non les espaces utiles pour créer les tokens indent ou dendent

Il est maintenant possible d'avoir par exemple la règle suivante :

\begin{lstlisting}[frame=single,caption={generator-code},label={generator-code}]
    yaml_seq : INDENT (SEQUENCE_TYPE)? seq_item+ DEDENT;
\end{lstlisting}

\subsubsection{Minus}
\begin{lstlisting}[frame=single,caption={uon-number},label={uon-number}]
NUMBER:
	(('+'|'-')? INT ('.' [0-9]*)? EXP?) // +1.e2, 1234, 1234.5
	| '.' [0-9]+ EXP? // -.2e3
	| (('+'|'-')? '0' [xX] HEX+) // 0x12345678
	| (('+'|'-')? '0' [oO] HEX+) ; // 0o12345678

NUMERIC_LITERAL: 'inf' | 'nan' | '-inf' | '-nan' | '+inf' | '+nan';
\end{lstlisting}

Le carcatère "-" étant utilisé pour définir un séquence en format yaml.
Il a fallu forcer le caractère directement dans l'expression régulière définissant un nombre.

\textbf{Remarque} : cette solution n'est pas optimale car si on modifie la grammaire il faudra modifier ce code.

\subsection{String}

\begin{lstlisting}[frame=single,caption={uon-string},label={uon-string}]
    string :
	literal
	| QUOTED_STRING
	| UNQUOTED_STRING;
\end{lstlisting}

Une valeur textuelle peut soit être entre guillement ou non.

C'est donc aussi le cas pour la clé d'une map.

Cela se rapproche par exemple, un peu plus de ce que fait JSON5 par exemple. % TODO lien
Aussi cela permet d'avoir des clés qui peuvent contenir exactement ce que l'on veut.
Car encapsuler entre des caractères spécifiques, il n'y pas d'ambiguités possible. Alors que sans on est contraint de limiter la forme de la clé.

Par contre un cas spécial a du être traiter. Si l'on veut pouvoir utiliser directement un mot sans guillemet mais qu'il existe dans la grammaire.
Il faut ajouter ce cas, c'est pourquoi nous avons la règle de parser "literal" pour pouvoir utiliser un de ces token comme string.

\begin{lstlisting}[frame=single]
    {
        temperature : 10 C
    }
\end{lstlisting}

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=12cm]{assets/figures/literal-key-uon.PNG}}
    \end{center}
    \caption[literal-key-uon]{\label{literal-key-uon}literal-key-uon}
\end{figure}

Sur le schéma \ref*{literal-key-uon} On voit que la clé et bien un token de type literal.
Ce token temperature exite comme token car c'est une valeur utilisé comme quantité mais si on ne précise pas explicitement que l'on souhaite avoir cette valeur
comme possible  clé alors on aura une erreur si on ne le précise pas \dots % TODO

\subsubsection{Unquoted}
%% TODO

\begin{lstlisting}[frame=single,caption={unquoted},label={unquoted}]
    UNQUOTED_STRING: IDENTIFIER+;

    IDENTIFIER
        : [\p{L}]    // matches a single code point in the category "letter"
        | [\p{M}]    // a character intended to be combined with another character (e.g. accents, umlauts, enclosing boxes, etc.)
        | [\p{N}]    // matches any kind of numeric character in any script.
        | [\p{Pc}]   //  punctuation character such as an underscore that connects words.
        | '!';
\end{lstlisting}


\subsubsection{Quoted}

%% TODO
\begin{lstlisting}[frame=single,caption={quoted},label={quoted}]
    QUOTED_STRING:
    '"' DOUBLE_QUOTE_CHAR* '"'
    | '"''"''"' DOUBLE_QUOTE_CHAR* '"''"''"'
    | '\'' SINGLE_QUOTE_CHAR* '\'';

    fragment DOUBLE_QUOTE_CHAR: ~["\\\r\n];

    fragment MULTILINE_QUOTE_CHAR: ~[];

    fragment SINGLE_QUOTE_CHAR: ~['\\\r\n];
\end{lstlisting}

N'importe quelle caractère peut se trouver

%% TODO : Multiline ? ne pas tolérer les espaces dans la clé mais dans le texte

\subsection{Propriétés, unités et nombres}

Des popriétés de présentation on été rajouter en reprenant les constructions utilisés pour définir des propriétés pour les clés ainsi que de validation.

% "_" dans le caption cause des erreurs !!!
\begin{lstlisting}[frame=single,caption={types properties},label={types_properties}]
    types_properties: OPEN_PAR (types_propertie (COMMA types_propertie)*)? CLOSE_PAR;
    types_propertie: comment | description | optional;
    comment: COMMENT COLON string;

    \dots

    string_scalar: (STR_TYPE (types_properties)?)? string;
\end{lstlisting}

number, bolean, null, string(literal, unquoted et quoted),
literal : tous les token utilisé déja ailleur mais qu'on voudrait avoir dans les clé par exmeple...

typs : string, nombre, boolean,

quantité et unités



\section{Debugging}

Il n'est pas simple de debugger une grammaire sans aucun outil ou seulement à l'aide d'un terminal, cela peut même se revéler extremement difficile.
Heursement pour nous, il existe une \href{https://plugins.jetbrains.com/plugin/7358-antlr-v4}{extension} sur intellij qui nous permet de visulaiser en temps réel, le parse tree du texte que l'on est en train de saisir.

% TODO : Figure d'intelj en action

% Les figures représentants les arbres se baseront sur ce plugin ...


\chapter{Implémentation}
Dans ce chapitre, nous allons explorer les aspects techniques concernant l'implémentation de fonctionnalités. Nous allons voir également plus en détail les outils utilisés ainsi que le déploiement de l'extension sur le marketplace.
Ce travail reflète uniquement mon approche personnelle sur la matière. Et ne devrait pas être considérée comme l'unique manière de procéder.

%TODO
%\section{Tolérance}
%Le parseur ne devrait uniquement controler la syntaxe. une bonne règle à se rappeler et que dans le doute il est préférable de laisser l'analyser syntaxique (parser) 
%Le document et controler la sémantique pour s'assurer que la règle a du sens

%"The parser should only check the syntax. So the rule of thumb is that when in doubt you 
%let the parser pass the content up to your program. Then, in your program, you check the semantics and make sure that the rule actually have a proper meaning."
% Citation !!! mégathread ?


\section{Code}

Par rapport à la figure %todo
les points suivants au été rajouté à la figure %todo

\textbf{Remarque} : Les "..." signifie qu'il y a des sous fichiers et/ou répertoire mais ont été ignoré pous ne pas surcharger
la figure.

\begin{figure}[!h]
    \centering
    \framebox[\textwidth]{%
        \begin{minipage}{0.9\textwidth}
            \dirtree{%
                .1 ..
                .2 github.
                .3 workflows.
                .4 pipeline.yml.
                .2 vscode.
                .3 launch.json.
                .3 tasks.json.
                .2 .gitignore.
                .2 README.md.
                .2 src.
                .3 completion.
                .4 completion.ts.
                .3 error.
                .4 ErrorListener.ts.
                .4 UonCompletionErrorStrategy.ts.
                .3 generated.
                .4 ....
                .3 grammar.
                .4 .antlr.
                .4 UON.g4.
                .3 outline.
                .4 UonASTVisitor.ts.
                .4 UONDocumentSymbolProvider.ts.
                .3 test.
                .4 ....
                .3 extension.ts.
                .2 tsconfig.json.
            }
        \end{minipage}
    }
    \caption{UON VS Code extension structure}
\end{figure}

Le code de l'implémentation est disponible sur le dépôt public \href{https://github.com/vitorva/vscode-uon}{suivant}.

\textbf{Remarque} : L'implémentation détaillée dans ce rapport pourrait ne plus représenter l'état du dépôt, car ce projet est sujet à évoluer.

\section{Extension}
Nous allons voir maintenant, l'essentiel de la procédure pour créer et publier son extension.

La procèdure détaillé se trouve \href{https://code.visualstudio.com/api/get-started/your-first-extension
}{ici}

\subsection{Génération de l'extension}

VS Code permet de générer un squelette (boilerplate) pour la réalisation d'une extension.
Pour ce faire il faut avoir Node.js et Git installés.

Ceci dans le but d'installer Yeoman et VS Code Extension Generator.

\href{https://yeoman.io/}{Yeoman} est un outil permettant de créer un générateur et de l'exécuter dans un environnement adapté.
\href{https://www.npmjs.com/package/generator-code}{VS Code Extension Generator} et le générateur implémenté par VS Code pour générer la structure de l'extension.

La commande pour l'installation est la suivante :

\begin{lstlisting}[frame=single,caption={generator-code},label={generator-code}]
npm install -g yo generator-code
\end{lstlisting}

Puis il suffit de lancer le générateur :

\begin{lstlisting}[frame=single]
yo code
\end{lstlisting}

Et de compléter ce qui est attendu dans le terminal pour créer notre extension.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=8cm]{assets/figures/yo-code.png}
    \end{center}
    \caption[Yo Code generator]{\label{yo-code}Yo Code generator}
\end{figure}

Cela nous créera l'arborescence mentionnée précédemment au point \hyperref[Extension File Structure]{Extension File Structure}.

\subsection{Lancement de l'extension}
On peut lancer notre application en mode debug avec la touche F5. Cela va ouvrir une nouvelle fenêtre VS Code qui va contenir notre extension.

Par défaut, l'application générée vient avec du code permettant d'afficher un message de bienvenu. On l'affiche au travers du command pannel (CTRL + SHIFT + P). On peut s'assurer comme cela que l'implémentation de base fonctionne correctement.
On constate donc que par défaut, l'application doit être lancé au travers d'une commande, mais pour notre cas il sera plus intéressant que VS Code nous propose les fonctionnalités à l'ouverture d'un fichier UON.
Pour cela, il faut modifier \emph{l'activation events} dans le fichier package.json par :
\begin{lstlisting}[frame=single]
    "activateEvents" : [
	"onLanguage:uon"
    ]
\end{lstlisting}

\subsection{Déploiement sur le Marketplace}

% https://code.visualstudio.com/api/working-with-extensions/publishing-extension
% https://code.visualstudio.com/api/working-with-extensions/continuous-integration
% https://marketplace.visualstudio.com/manage/publishers/test2publish?noPrompt=true
% https://dev.azure.com/Stev03/_usersSettings/tokens

\textbf{Prérequis} : Posséder un compte Azure.

Il faut d'abord récupérer un Personal Access Token. Pour ce faire, il faut créer une \href{https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/create-organization?view=azure-devops}{organisation}.
Puis, créer un access token avec ces paramètres :
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/access-token.png}}
    \end{center}
    \caption[Access Token]{\label{access-token}Access Token}
\end{figure}

Ce token va nous permettre publier notre extension sur un Publisher. Celui-ci nous permettra de gérer notre extension en ligne.

Pour créer un publisher, il faut se connecter avec le même compte qui a créé l'organisation et le token ci-dessus via la \href{https://marketplace.visualstudio.com/manage/publishers/}{management page}.

Quand cela est fait, il faut ensuite ajouter la propriété "publisher" et saisir l'id de celui-ci dans le package.json de notre extension.

Puis finalement, il suffit dans un terminal de saisir les commandes suivantes :
\begin{lstlisting}[frame=single]
    vsce login [le nom du publisher]
    Vsce publish
\end{lstlisting}

\textbf{Remarque} : Attention au terminal utilisé. Dans mon cas, le terminal de node (git.bash) sur windows ne fonctionne pas lorsque des interactions avec le terminal est nécessaire.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/manage-publisher.png}}
    \end{center}
    \caption[Interface web pour manager l'extension]{\label{manage-publisher} Interface web pour manager l'extension}
\end{figure}

Il est ensuite facile de manager l'extension au travers de la page web.
L'extension apparaitra automatiquement dans le marketplace si elle est publiée.

\subsection{Intégration continue (CI)}

Commme le squelette de base de l'extension à été créé en utilisant le générateur Yeoman de VS Code, la structure pour écrire nos tests d'intégrations
existe déjà.
% citation : https://code.visualstudio.com/api/working-with-extensions/testing-extension

Il suffit de lancer la commande "npm run test" ou "yarn test" pour lancer les tests se trouvant dans le repertoire % TODO.

C'est l'api \href{https://mochajs.org/api/}{mocha} qui est utilisé pour ces tests.

Actuellement on test de cette facon les fonctionnalités de la complétion et de l'outline view.

Concernant les tests pour la coloration syntaxique, le module \href{https://github.com/PanAeon/vscode-tmgrammar-test}{vscode-tmgrammar-test} a été choisi.
Car depuis mocha on a pas moyen de le faire.

Nous avons donc quelques tests unitaire simples mais qui permet toutefois de s'assurer qu'un changement ne viens pas modifier le comportement d'une manière inattendu.

Tous les tests se trouvent dans le dossier test à la racine du projet. % TODO lien avec l'arborescence

% TODO
\subsubsection{Autocomplétion}
On s'assure que les candidates de complétions attendues à un emplacement soit les bons selon une entré de code.

\subsubsection{Syntax highlight}
créer un fichier .uon
faire un -snap
modifier le fichier snap et rajouter l'entete qui nous permet de définir que c'est un fichier tests
on peut mnt le tester avec -test


\subsubsection{Outline view}
Un visiteur pour compter le nombre de noeud attendu pour s'assurer qu'on soit bien passer par les noeuds
Créer les construction à la main et comparer leur sorti textuelles

%REGEX
% utilisation d'outils en ligne : ex : https://regex101.com/
% référence officielle : https://www.regular-expressions.info/


\subsection{Deployement automatisé (CD)}

L'extension est actuellement automatiquement déployé lorsqu'une release sur la branche main est réalisé.
L'étape de mise en place est détaillé par VS Code à cet \href{https://code.visualstudio.com/api/working-with-extensions/continuous-integration#github-actions}{emplacement}.

Il s'agit d'une étape supplémentaire dans la pipeline après que les tests soient effectués.

Elle consite à exécuter la commande vsce publish depuis notre package.json pour publier l'extension.

\href{https://www.npmjs.com/package/vsce}{vsce} est le gestionnaire d'extension de VS Code.

Il est nécessaire d'ajouter le token (PAT) comme secret dans notre dépot github. %todo ref

\textbf{Remarque } : On ne peut déployer qu'une extension avec une version supérieur que la précédente. % TODO : automatisé ça ?


% TODO : maintenance
%Les explications permettant de reprendre le projet se trouve sur le Readme à la racine du repository
% Ne devrait pas faire parti de l'organisation du prof ?


\section{Génération du parser ANTLR}
\subsubsection{Grammaire}

Quand le fichier de grammaire est écrit, il suffit de lancer le code suivant :

\begin{lstlisting}[frame=single]
    antlr4ts -visitor path/to/MyGrammar.g4
\end{lstlisting}

Il nous génère ces fichiers :
\begin{itemize}
    \item UON.interp
    \item UON.tokens
    \item UONLexer.interp
    \item UONLexer.tokens
    \item UONLexer.ts
    \item UONParser.ts
\end{itemize}

\section{Syntax highlighting}

% Pour signaler à l'utilisateur que la saisie est tout de même incorrecte. On peut s'aider en parallèle de la coloration syntaxique.

La coloration syntaxique permet que chaque élément du texte soit affiché dans l'éditeur avec une coloration et un style en fonction de son type.
Il y a deux composantes principales. % Citation vscode ?
\begin{itemize}
    \item La tokenisation qui consiste à séparer le texte en une liste de token.
    \item La thémisation (Theming) qui permet d'attribuer à un token une couleur et un style.
\end{itemize}

%TODO : à changer
\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/basic-uon.png}
    \end{center}
    \caption[code UON]{\label{basic-uon} Exemple basique en UON}
\end{figure}


VS code utilise Textmate grammars \cite{textmate-grammars} comme le moteur de tokénisation de la syntaxe.
Cette grammaire a été inventée par l'éditeur TextMate est a été adopté par de nombreux éditeurs et IDE.
Elle contient une liste structurée d'expressions régulières qui permet d'associer un scope
à un ou plusieurs tokens qui déclencherait l'activation d'une de ces expression.

Nous définissons notre grammaire TextMate dans le fichier "uon.tmLanguage.json" qui se trouve dans le dossier "syntaxes" à la racine du projet.
Ce fichier doit être référencé à travers du point de contribution "grammar" dans le package.json.

% TODO : https://macromates.com/manual/en/language_grammars

%Le but de ce fichier en plus de permettre la coloration syntaxique, et qu'il permet également de rendre l'éditeur de texte "intelligent" quant au contexte dans lequel se trouve le caret.
%Cela permet d'avoir des comportements différents selon la situation (ex : Pas de bracket closing dans les commentaires).

\subsection{Scope}
Un scope défini le contexte d'un groupe de token et peut-être considéré comme son type.

Il s'agit d'un identifiant, c'est grâce au scope d'un token, qu'un fichier de thème peut attribuer une couleur avec une mécanisme de "clé-valeur".

TextMate liste une liste de scope commun que de nombreux thèmes ciblent déjà et recommande de les utiliser.
Ceci dans le but de permettre à notre langage d'être supporté le plus possible.
%% https://macromates.com/manual/en/language_grammars

Pour respecter au maximum les conventions attendues et s'assurer qu'une coloration automatique soit appliquée. La stratégie suivante a été adoptée : partir depuis un fichier de thème déjà créé (Thème dark par défaut sur VS Code). 
Et de réutiliser les scopes définis dans ce fichier.

Ce fichier se trouve sur le dépôt \href{https://github.com/microsoft/vscode/blob/main/extensions/theme-defaults/themes/dark_vs.json}{suivant}.
Mais peut être également généré depuis le contrôle panel (CTRL + SHIFT + P).

Ce procédé nous permet de profiter des thèmes de VS Code implémenté par défaut dans l'éditeur, par exemple les deux suivants :

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/vscode-dark-theme.PNG}
    \end{center}
    \caption[vscode dark theme]{\label{vscode-dark-theme} vscode-dark-theme}
\end{figure}


\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/vscode-kimbi-dark-theme.PNG}
    \end{center}
    \caption[vscode kimbi dark theme]{\label{vscode-kimbi-dark-theme} vscode kimbi dark theme}
\end{figure}

\subsubsection{Example}
Prenons un exemple simple, le scope pour les nombres : "constant.numeric.uon"

On voit qu'il s'agit d'une suite d'identifiant.

La convention est d'avoir des mots clés séparé par un point pour spécifier chaque niveau de précision supplémentaire

Il est aussi possible d'avoir plusieurs scope pour un élément : "string.uon support.type.property-name.uon"

Les scopes sont séparé par un espace.
C'est celui le plus à droite qui l'emporte. Cela peut-être utile si l'on souhaite maximiser les chances qu'un thème nous fournisse une coloration.

% https://macromates.com/manual/en/scope_selectors


Les scopes sont souvent imbriqués et c'est le plus spécifique qui est utilisé pour le choix du thème.
Il est possible d'analyser notre code à l'aide du "Scope Inspector", qui est un outil disponible dans l'éditeur depuis le control panel,
pour visualiser la hiérarchie des scopes pour un token.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/scope-inspector.png}
    \end{center}
    \caption[Scope inspector]{\label{basic-uon} Le scope inspector sur un exemple de code en UON}
\end{figure}

La section "textmate scopes" montre la liste des scopes utilisés pour le token qu'on séléctionne. Le scope le plus spécialisé se trouve au sommet.

La coloration syntaxique n'est pas considérée comme un "programmatic language feature" et donc n'est pas géré par la spécification LSP et par l'API de VS Code non plus.
Une des raisons évoquées et que ce mécanisme doit être le plus rapide possible. Et que le faire directement depuis le client garantit une latence faible.
%citation d'un repo

\subsection{Fonctionnement}

Voici ci-dessous un exemple relativement simple, mais permettant de comprendre le méchanisme général utilisé dans notre fichier "uon.tmLanguage.json" :
\begin{lstlisting}[frame=single, caption={Textmate grammar},label={Textmate grammar}]
 1  {  scopeName = 'source.untitled';
 2     patterns = (
 3        {  name = 'keyword.control.untitled';
 4           match = '\b(if|while|for|return)\b';
 5        },
 6        {  name = 'string.quoted.double.untitled';
 7           begin = '"';
 8           end = '"';
 9           patterns = (
10              {  name = 'constant.character.escape.untitled';
11                 match = '\\.';
12              }
13           );
14        },
14     );
15  }
\end{lstlisting}
\textbf{"scopeName"} (ligne 1) : Scope racine du fichier

\textbf{"patterns"} (ligne 2) : C'est un tableau contenant les règles actuelles utilisées pour parser le document. Elles sont appliquées dans l'ordre.
Ces règles peuvent être directement écrites à cet emplacement, mais il est aussi possible de les définir dans le "repository". Puis de les inclure.

\textbf{"repository"} : Un dictionnaire (clé-valeur) de règles.

\textbf{"name"} : Nom d'un scope

On constate deux constructions possibles.
La première règle applique le scope à un élément qui déclenche le match.
La deuxième définit un délimiteur de début et de fin .

Les règles se trouvant dans le "patterns" suivant (à la ligne 12) seront appliqué au éléments se trouvant à l'intérieur de ces délimiteurs.
On voit qu'il est donc possible d'imbriquer les règles.

\subsection{uon.tmLanguage}

%TODO


\subsubsection{Propriétés}

La coloration syntaxique doit refléter au mieux cette grammaire. Il y a donc une coloration spécifique pour les propriétés appartenant à un type.
Cela rend la coloration syntaxique fortement lié à la grammaire mais on se rends compte d'un coup d'oeuil si la propriété et correcte.

% TODO : propriétés pas définis

\subsection{Capture}

Il y a globalement deux manières pour appliquer un scope.

La première et de définir un caractère de départ et un autre de fin. Puis définir les patterns que l'on appliquera au élément se trouvant à l'intérieur.

% exemple : !str(comment : "mon premier commentaire")

Mais dans certains cas, il n'y a pas de délimiteur et on voudrait appliquer une coloration différente aux éléments qui composent le groupe de caractère qu'une expression régulière
a déclenché.

Il s'agit d'un nombre mais l'option "Capture" nous permet d'appliquer un scope au groupe de selection.
Il n'est pas facile pour des expressions assez complexe de trouver ce nombre mais on peut s'aider du site \href{https://regex101.com/}{suivant}


\subsection{Semantic Highlight}

Un point important et qu'il ne faut pas confondre la coloration syntaxique avec ce que l'on nomme la coloration sémantique. Car il s'agit d'une couche que l'éditeur rajoute sur la précédente pour l'améliorer si nécessaire.
La tokénisation sémantique permet de fournir des informations supplémentaires sur les tokens en se basant sur une compréhension profonde du langage.
De manière à résoudre les symboles dans le contexte d'un projet.

Il s'agit donc d'obtenir des informations contextualiséés afin fournir une coloration plus précise pour un token.

Par exemple avec du code Typescript :

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=15cm]{assets/figures/semantic-coloration-without.png}
    \end{center}
    \caption[Sans coloration sémantique]{\label{semantic-coloration-without} Sans coloration sémantique}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=15cm]{assets/figures/semantic-coloration-with.png}
    \end{center}
    \caption[Sans coloration sémantique ]{\label{semantic-coloration-with} Avec coloration sémantique }
\end{figure}

On voit à la ligne 10 que la colorisation de la fonction a changé et correspond maintenant à un paramètre d'une fonction.
Cela est donc intéressant pour des langages plus complexes dans leur nature qu'un langage de sérialisation. Une coloration syntaxe suffit amplement dans notre cas.
Car se baser sur la structure du document nous donne déjà toutes les informations nécessaires.

\section{Auto-completion}

% exemple de completion final

Cette fonctionnalité fait parti de ce qu'on appelle plus communément "IntelliSense" \cite{intelliSense} qui un terme général désignant diverses fonctions d'édition de code.

Le terme "auto-complétion" existe également sous d'autres appélations (ex : autocomplétion, complétion automatique, complétion, etc.).
L'intérêt est de proposer des suggestions de saisie au fur est à mesure que l'on écrit.

Il s'agit d'une fonctionnalité informatique permettant à l'utilisateur de limiter la quantité d'informations qu'il saisit avec son clavier
en se voyant proposer un complément qui pourrait convenir à la chaîne de caractères qu'il a commencé à taper.
% TODO : citation wikipedia

Le terme "suggestions" étant relativement vague. C'est ici ou réside la subtilité. Cela va dépendre principalement du langage traité.

Car en JSON ou en YAML, l'auto complétion n'existe pas vraiment dû à leur nature même de ces langages de sérialisation.
Et contrairement à des langages de programmation. Il n'y a pas de mots clés, on ne peut pas définir de nom de fonction, ni de classe.
On ne peut également pas définir de variables.

On aurait donc rien de base à proposer pour ces deux langages, sans utliser une souce d'infomation externe comme les "JSON-schéma".
À la limite, seul les clé ou valeur précédement saisie peuvent éventuellement être reproposé mais cela est fait automatiquemnt sous VS Code
avec le type de complétion nommé "word based completion".
% https://stackoverflow.com/questions/54827043/how-to-disable-abc-suggestion-on-vs-code

Donc pour commencer, lorsqu'on implémente une telle fonctionnalité, il faut pouvoir répondre à une série de question :
Quelles sont les suggestions pertinentes à proposer, pourquoi et quand le faire ?

Il est donc nécessaire d'analyser le langage UON et de voir ce qu'il est pertinent de proposer comme suggestion de complétion.

La syntaxe UON peut être exprimée par la figure \ref{syntax}.

\begin{figure}[!ht]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/syntax.png}}
    \end{center}
    \caption[syntax]{\label{syntax} UON syntax}
\end{figure}

On n'y remarque des points extrêmement intéressant.

Toute valeur est un type, et un type peut avoir des propriétés associés.

Les suggestions de baseront donc sur les types et leur propriétés.

\subsection{antlr4-c3}

Comme mentionné au début de ce document, une approche typique pour implémenter de la complétion est d'utiliser un parser du langage.

Mais en complément nous allons aussi utilisé \href{https://github.com/mike-lischke/antlr4-c3}{"antlr4-c3"} (également simplement nommé c3).

Avant il n'y a avait que des solutions personnelles. Cette librairie a pour objectif de fournir une implémentation commune.
Elle est disponible comme node module et est écrit en Typescript.

Elle a été conçu par \href{https://github.com/mike-lischke}{Mike Lischke}.

Cet outil est écrit en Typescript mais existe aussi en version \href{https://github.com/mike-lischke/antlr4-c3/tree/master/ports/java}{JAVA} et \href{https://github.com/mike-lischke/antlr4-c3/tree/master/ports/c%23}{C\#}.

Il s'agit d'un moteur de complétion pour les parsers basés sur ANTL4. Le moteur c3 est capable de fournir des candidats de complétions.
Nous utiliserons principalement ces candidats pour les suggestions proposé dans l'éditeur de code.

c'est un outil relativement puissant, qui nous simplifie grandement la tâche

% citation https://tomassetti.me/code-completion-with-antlr4-c3/
% Le moteur de complétion n'utlise pas le parse tree pour proposer des candidats, mais est il est quand même nécessaire de lancer l'exéction du parser (parser.uon()) pour remplir le stream de token (tokenStream).

% citation MEGA TUTO
Le moteur de complétion n'utlise pas le parse tree pour proposer des candidats. Mais l'exécution du parser est nécessaire pour remplir le token stream \ref{antlr-setup}.
Car le moteur s'appuyera sur ce flux de tokens et également du mécanisme de machine à état (ATN) d'antlr disponible dans le parser.

% citation
\textbf{Remarque} : Le résultat du parsing n'a pas besoin d'être correcte. Il est possible de travailler avec du code incomplet ou malformé, ce qui est relativement fréquent dans un éditeur.

Pourquoi un telle moteur ?
Car connaitre tout les types de tokens qu'un utilisateur serait amené à saisir ne suffit pas. Pour que la complétion soit réellement intéressante, nous voudrions connaitre quelle règle de la grammaire pourrait être valide à une position donnée.
Et cet outil apporte une solution pratique est élégante pour résoudre ce cas.

Pour notre scénario, on voudra proposer des suggestion à la position du curseur, ce qui fait le code après celui-ci ne nous intéresse pas (dans le cadre actuelle du langage).
On utilisera la position du curseur (caret). Pour uniquement traiter le code du début de l'éditeur jusqu'à la position du curseur.

Une utilité de prendre en compte ce qui se trouve apès le curseur serait par exemple le cas de code SQL :

\begin{lstlisting}[frame=single]
SELECT | FROM USER // ("|" étant le curseur)
\end{lstlisting}

Dans ce scénario des suggestions liées à la table USER devrait être proposé et nécessiterait un traitement particulier.

Pour recevoir des candidats il suffit d'avoir un environnement comme présenté par la figure \ref{antlr-setup}, d'installer le module npm \emph{antrl4ts} et
de donner au moteur, une instance du parser et un indice.

\begin{lstlisting}[frame=single] % TODO
    let core = new c3.CodeCompletionCore(parser);
    let candidates = core.collectCandidates(index);
\end{lstlisting}

L'explication pour trouver cette indice se trouve dans la séction \hyperref[candidates]{\textbf{Candidats}}.
% \textbf{Remarque} : L'indice est calculé en se basant sur la taille totale du stream de token. (voir fichier completion.ts)

Cependant une contrainte pour que le moteur propose des candidats est qu'il est nécessaire que tout le code se trouvant avant le curseur soit correct.
Comme mentionné plus tard %TODO

Ce qui se trouve à la suite peut être erroné, car on ne n'en tient pas compte, comme mentionner ci-dessus.

Nous recevrons alors un objet de type \emph{CandidatesCollection}.

\begin{lstlisting}[frame=single]
    class CandidatesCollection {
        tokens: Map<number, TokenList>;
        rules: Map<number, RuleList>;
    }
\end{lstlisting}

Cette collection contient deux map.

\textbf{tokens} : fait le lien entre le prochain token possible et une liste contenant des tokens qui doit le suivre immédiatement, si elle existe.

Nous allons uniquement utiliser les clés de cette map pour proposer nos suggestions.

Car la liste n'est jamais utilisé actuellement, il n'y jamais deux tokens qui devrait se suivre obligatoirement dans la grammaire qui sont proposé par le moteur.

Par exemple pour obtenir une séquence de token comme dans l'exemple suivant :

\begin{lstlisting}
    !map {switch : |}
                   - !bool
                   - !bool true
\end{lstlisting}

Cela nécessiterait d'adapter la grammaire en conséquence pour que la valeur "true" soit le seul élément possible qui suit "!bool".


% TODO
% - parler de comment convertir ce symbole en token
% - c'est quoi la valeur concrètement : La valeur d'un token ou son nom ?


\textbf{rules} : fait le lien entre la règle du parser actuelle et la liste de règles utilisé pour arriver à ce stade.

Cette map nous permet de questionner la liste des symboles pour retourner des informations supplémentaires.

% TODO https://github.com/lark-parser/lark/issues/684

%Comment aurions nous du procéder sans un tel outil ?
%Il évidemment possible de le faire sans ce moteur.
%Un excellent article mentionne une telle implémentation aussi avec ANTLR ici.

%Cela rend bien entendu le travail plus conséquent.

\subsubsection{Symbol tables}

C'est un méchanisme puissant de c3 mais que nous n'allons pas exploiter,
car je n'ai pas trouvé d'usage dans le cadre du cahier des charges de ce projet et par manque de temps.
Cependant il reste intéressant de détailler briévement son utilité pour d'éventuelles améliorations.

On pourrait penser qu'avoir à disposition un parser nous permettrait de récupérer toutes les informations nécessaires au travers d'un AST. % TODO AST abbréviation
Cependant ce n'est pas le cas pour la plupart des scénarios.
Nous pouvons savoir quels sont les tokens valide à un emplacement (ce qui est suffisant pour nous). Mais pour certains langages, des informations manquent.
Et c'est ce que la table de symbole tend à résoudre.

Une table de symbole est une structure qui contient des informations complémentaires sur des symboles, cela peut s'agir de noms, leur scopes et d'autre caractèrisiques.
Cependant, un langage de sérialisation est relativement moins complexe à gérer qu'un langage de programmation.
Nous n'avons par exemple pas à mémoriser des variables et leur visibilité au sein du code.

Mais il est mentionné dans la spécification qu'un schéma pourrait être définit au même niveau que le payload.
Avec ce scénario en tête, une table de symbole pourrait se revélé utile.

Car on pourrait se servir de ce schéma de validation qui spécifierait un nouveau type pour proposer de la complétion appropriée
lorsque l'on saisira ce type dans le payload. Typiquement des propriétés.

\subsection{Tokens ignorés}

Il est posssible d'indiquer au moteur les tokens qu'il peut ignorer, pour ne pas les récupérer dans la liste des candidats fournis par le moteur.
Cela est nécessaire pour nous car tous les tokens ne sont pas intéressant.

Les tokens peuvent être classé en deux catégories.

Ceux dans la valeur sont fixes
Ceux dans la valeur dépends de la saisie d'un utilisateur au travers d'expression régulière.

la seconde n'est pas intéressante car le moteur ne fera que retourner le nom du token ce qui est inutile.
la première catégorie, il faut aussi ignorer les tokens parentèse etc.. car pas vraiment pertinent.

\subsection{Gestion des erreurs }\label{error handle}

On aurait pu penser naivement que le mécanisme avancé de récupération d'erreur \emph{Parser Error Recovery Strategy} permettrait également de rendre celui de la complétion aussi plus tolérant.
Mais ce mécanisme n'est utilisé que pour générer un parse tree, cela est donc inutile pour ce cas. % car se base sur TODO

Donc comme déja mentionner, pour que le moteur puisse retourner des candidats, la syntaxe se trouvant avant la position du curseur doit être valide.
Car sinon on pourrait avoir dans le stream de token des éléments qui ne seront pas compris par le moteur et l'on obtiendra pas de candidats.

Une explication fourni par le créateur du moteur dans la rubrique "issues" du dépot du \href{ https://github.com/mike-lischke/antlr4-c3/issues/29}{projet}.
% https://github.com/mike-lischke/antlr4-c3/issues/29

est que dès le moment où le moteur est confronté à une erreur de syntax (token inattendu ou invalide), il n'a plus moyen de trouver un chemin valide.
Et que parcourir toutes les possiblités jusqu'à trouver le token qui suit celui qui pose problème est trop long et peut mener à des erreurs d'ambiguités si le token suivant se trouve dans plusieurs règles de la grammaire.

Mais l'on pourrait s'en contenter car finalement, il est logique qu'une erreur puisse entrainer un tel comportement.
Si dans un langage de programmation un utilisateur saisit du code erroné, il fort possible que la complétion cesse de fonctionner.

\subsubsection{Correction custom}
Cependant comme nous travaillons avec une syntaxe beaucoup moins complexe l'on pourrait modifier le texte depuis l'application pour corriger voir supprimer le problème.
Par exemple, dans le cas d'un mapping de clé valeur, une ligne pourrait être omis sans nuir à la compéhension du document.

C'est ce qui peut être fait de la manière suivante :
\begin{enumerate}
    \item Étape 1 : créer une nouvelle classe de listener qui détecte les erreurs lors du parsing.
    \item Étape 2 : stocker la position des tokens qui posent problème dans la classe du listener.
    \item Étape 3 : modifier le texte récupérer dans l'éditeur localement en conséquence.
    \item Étape 4 : relancer la phase de la création du parser en lui donnant le texte modifié.
\end{enumerate}

Pour l'instant : % TODO

Avec ce mécanisme personnalisé on arrive à rendre la completion un peu plus tolérant au erreur mais le problème et que l'on couple la logique du moteur avec l'application et donc n'est pas une très bonne pratique.
car si on modifie la grammaire il faudra repércuté les changements dans l'application.

\subsection{Snippets}
Il est possible d'ajouter des snippets sous VS Code. C'est-à-dire des templates qui nous permettent de générer du code prédéfini. Puis de les ajouter aux suggestions.
C'est utile si on a envie d'avoir des suggestions un peu plus personnalisés.
Typiquement on pourrait vouloir avoir comme suggestion dirctement un ensemble de propriétés comme illusté par la figure \ref*{snippet-suggestion}.
Ce qui n'est pas possible avec le moteur.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/snippet-suggestion.png}
    \end{center}
    \caption[Suggestion d'un snippet]{\label{snippet-suggestion} Suggestion d'un snippet}
\end{figure}

%TODO : résultat

Il est possible de passer à la case suivante à l'aide de la touche tab. %case..
Cependant il est nécessaire de proposer ces suggestions au bon moment. La solution envisagé et de vérifier que le token du type possédant
ces propriété ce trouve dans les candidats du moteur et qu'il ne s'agit pas de schéma % TODO

C'est un mécanisme intéressant mais fortement lié à l'application


\subsection{Triggers}
L'éditeur va trigger automatiquement l'autocomplétions en appelant le provider lorsque l'on saisit plusieurs caractères. Il aussi possible de l'activer manuellement en exécutant la commande "ctrl + space".
On peut aussi ajouter au provider des symboles qui vont trigger son activation. Les symboles espace " " et de retour à la ligne (\\rn) ont été défini comme triggers, car cela semble assez naturel dans ce langage.
Cependant il ne s'agit que d'une appréciation personnelle.

%TODO :
No suggestion :
quand un espace précède un nouveau mot alors c'est simple pour le placer
quand il est colé c'est + compliqué. Il faut calculer la position du début et de fin du nouveau mot car VS Code peut trigger l'autocomplete n'importe quand et du coup
avec du texte colé il peut pas savoir à partir de ou le texte en train d'etre ecrit est un mot du lnagage...mais utilité de faire ça ???

%TODO : quick-suggest-setting (par défaut ?) : https://github.com/microsoft/vscode/issues/101333}

\subsection{Candidats}\label{candidates}

%Il est nécessair d'implémenter une fonction pour convertir la positon actuelle du curseur en la position du prochain token attendu.
%Pour être transmis à la fonction "collectCandidates(tokenIndex)". %todo

Il est nécessaire d'implémenter une fonction pour trouver l'indice du prochain candidat.

Actuellement ce qu'on veut c'est le prochain token possible après donc il s'agira

La documenation du projet indique bien que ce n'est pas un problème trivial à résoudre.
Un point important est de savoir si l'on tient compte ou non des espaces vides dans la grammaire.

Sans les espaces, il est difficle de gérer certains cas particuliers % TODO montrer des exemples

Actuellement pour connaitre l'indice du ou des prochains candidats, on récupère le flux des tokens (tokenStream) trouvé lors du parsing.
Puis, on récupère dans un tableau chaque token individuellement (y compris les espaces).

Cependant lorsque l'utilisateur écrit et que cela trigger l'autocomplétion, le texte parsé est souvent incorect, car le dernier élément saisi n'est pas encore complet.

Grâce à la taille du tableau et en se basant sur les erreurs qui ont été générées, on peut déduire l'indice du ou des prochains tokens que l'on peut afficher.

\subsubsection{Index du candidat}
L'indexation est gérée de 2 manières différentes. En fonction de la prise en compte de l'espace ou non.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/candidat-index.png}}
    \end{center}
    \caption[Indexation des candidats]{\label{candidat-index} Indexation des candidats}
\end{figure}

Cela va dépendre si on décide d'ignorer ou non les espaces dans la grammaire g4.
Les développeurs de c3 recommandent de ne pas ignorer les espaces. Car cela permet de gérer plus de cas particuliers.

\subsection{Mécanisme automatique}
VS Code va automatiquement filtrer les suggestions pendant la saisie à l'aide d'un algorithme de fuzzing search.
Pour rappel le fuzzy search (ou recherche approximative) permet de trouver une chaine de caractère en fournissant une chaine qui n'a besoin d'avoir une correspondance exacte
avec le texte que l'on cherche.

Par défaut la complétion proposé par VS Code porte sur les symboles déjà existants dans le code.
Ce type de complétion est nommé "word based completion" dans VS Code.

\section{Outline view}

La outline view est une représentation de la structure du fichier actuellement ouvert par l'éditeur.
On appel généralement cettre stucture le "document outlining" ou "code outlining".

% https://code.visualstudio.com/api/references/vscode-api
% https://code.visualstudio.com/api/references/vscode-api#DocumentSymbol
% https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/
En analysant les spécifications LSP ou le DocumentSymbolProvider de l'api VS Code, on constate que l'outline peut être représenté sous deux formes :

\begin{itemize}
    \item SymbolInformation qui est une flat list de tous les symboles trouvés dans un document. Dans ce cas, ni le range de localisation du symbole ni le nom du conteneur du symbole ne doivent être utilisés pour déduire une hiérarchie.
    \item DocumentSymbol qui est une hiérarchie de symboles trouvés dans un document texte donné
\end{itemize}
%TODO : citation ?

la structure peut ête donc représenté de manière hiérarchique ou non (seulement être listés).

%TODO Lien

Un élément de l'outline (noeud) représente un élément unique du texte ou une structure.

On a accès à cette représentation dans le sidebar gauche par défaut.

%TODO : picture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/outline-view-sidebar.PNG}}
    \end{center}
    \caption[outline-view-sidebar]{\label{outline-view-sidebar} outline-view-sidebar}
\end{figure}

ou depuis la barre de recherche en saisissant la commande "CTRL + SHIFT + o" :

%TODO : picture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/outline-view-searchbar.PNG}}
    \end{center}
    \caption[outline-view-searchbar]{\label{outline-view-searchbar} outline-view-searchbar}
\end{figure}

Il est possible obtenir la structure d'un fichier en analysant manuellement le texte (par exemple ligne par ligne).
Mais cette approche n'est pas optimale et il serait diffile de représenter la strucutre hiérarchique du document à l'aide de regex par exemple,
comme mentionné au point : % TODO

Nous allons donc utiliser notre parser.

Le résulat du parsing est un "parse tree" (aussi nommé CST) qui est une représentation concrète de notre input. Le parse tree contient toutes les infomrations et beaucoup d'entre elles ne nous intéressent pas.
Typiquement les informations grammaticales et structurelles.
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/parse-tree.PNG}}
    \end{center}
    \caption[UON parse tree]{\label{parse-tree} UON parse tree}
\end{figure}

C'est pourquoi il faut pouvoir le parcourir pour en extraire un AST.
Un AST est une représentation abstraite de notre input. Il ne contient généralement que les informations les plus pertinents.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/ast.PNG}}
    \end{center}
    \caption[UON AST]{\label{ast} UON AST}
\end{figure}
%TODO : Images non contractuelles

Donc pour le générer on va vouloir récupérer uniquement ces informations pendant le parcours.

Pour parcourir un arbre, deux approches sont possibles : Le pattern Listener ou Visitor.
La première consiste à écouter les types de noeuds traversés qui nous intéressent. Le problème et que c'est approche ne nous permet pas de manipuler un objet en cours de route, ni gérer le flux d'exécution.
Car il n'est pas possible de communiquer entre les noeuds. Cela peut être tout de même utile si l'on veut afficher la strucure de l'arbre dans un terminal par exemple.

Contrairement à la seconde approche, qui nous permet de gérer ces deux cas. Et c'est donc tout naturellement que cette solution a été prévilégiée.

Cependant on ne va pas d'abord créer un AST puis construire la outline view à l'aide de celui-ci.
Nous allors directement construire l'outline view à la volé, pendant le parcours du parse tree. L'outline view sera une représentation direct de notre AST.

L'arbre sera donc un objet VS Code de type \href{https://code.visualstudio.com/api/references/vscode-api#DocumentSymbol}{DocumentSymbol}.

Il y a quelques points intéressant à commenter concernant cet objet.

La variable "children" qui nous permet d'avoir des encapsulations d'objets "DocumentSymol".

Un noeud doit être représenter par un "Symbolkind" % https://vshaxe.github.io/vscode-extern/vscode/SymbolKind.html
Nous sommes donc limités sur la représentation et devons faire les ajustements nécessaire.

Il y a deux paramètres de ranges à fournir à la création de l'objet : une qui contient leur définition et une qui pointe vers leur plage la plus intéressante, par exemple la plage d'un identifiant.

Le premier se veut plus global que le second.
Dans notre cas les deux peuvents représenter la même chose et représenter la range du token affiché dans l'éditeur.

% https://stackoverflow.com/questions/55846146/make-vs-code-parse-and-display-the-structure-of-a-new-language-to-the-outline-re/63060797#63060797
%The range is the definition range (the whole definition of your symbol, e.g. for a typescript class, it starts at
%the keyword class and ends with the closing }) and the selectionRange is often only the symbol's token range
%(but according to the specification, it can also include the doc comment block and the visibility modifiers, it is the implementor's choice).

Lors de la génération des fichiers ANTLR, il est indiquer de générer également un fichier visiteur "UONVisitor" en précisant le tag "- visitor".

Cela va nous créer un fichier qui contiendra une interface définissant une fonction pour chaque type de noeud de note parseur et retournera un résultat de type générique.
Chacune de ses fonctions prends comme paramètre son contexte.

%Cette interface étends la classe \href{https://www.antlr.org/api/Java/org/antlr/v4/runtime/tree/ParseTreeVisitor.html}{ParseTreeVisitor} de ANTLR qui gère le méchanisme de parcours et d'aggrégation des résulats.

La classe UonASTVisitor implémente cette interface et étend la classe \href{https://www.antlr.org/api/Java/org/antlr/v4/runtime/tree/AbstractParseTreeVisitor.html}{AbstractParseTreeVisitor}
pour obtenir le comportememt par défaut du pattern visitor.

On doit donner le "parse tree" créé par le parser à la fonction d'entrée \emph{visit} de notre visitor. % const ast = uonASTVisitor.visit(tree);

L'intéret d'utiliser le pattern Visitor est de pouvoir retourner un élément à chaque étape du parcours.
De plus note classe \emph{UONVisitor} étant générique, c'est ce qui nous permet de manipuler un objet VS code.

%TODO : raison ?
%La classe surchargée est de type "any" car pendant le parcours un élément peu être de deux natures différentes (Une valeur/terminal ou une strucutre Vscode).


\subsection{visitChildren}
%Des fonctions de la classe AbstractParseTreeVisitor ont été surchargé pour adapter le comportement à notre situation.

La fonction \emph{visitChildren} est la suivante :

\begin{lstlisting}
1.    visitChildren(node) {
2.        let result : any = this.defaultResult();
3.        let n = node.childCount;
4.        for (let i = 0; i < n; i++) {
5.            if (!this.shouldVisitNextChild(node, result)) {
6.                break;
7.            }
8.            let c = node.getChild(i);
9.            let childResult = c.accept(this);
10.            result = this.aggregateResult(result, childResult);
12.        }
13.        return result;
14.    }
\end{lstlisting}

Le résultat par défaut est une liste vide. Lors du parcours de la remonté dans un noeud parent, nous allons rassembler les enfants dans une liste.

Pourquoi ne pas manipuler directement un object DocumentSymbol au lieu d'une liste ?
Tout simplement car sinon on aurait du le faire dans la fonction d'aggrégation et l'on préfera ici déléguer la logique dans le noeud parent concerné
qui pourra utiliser les valeurs récupérer pour construire l'information voulue.

C'est un choix d'implémentation pour faciliter la construction d'un object DocumentSymbol.

\subsection{Parcours}
Le parcours est effectué selon un algorithme de parcours en profondeur (DFS).

Pour rappel un parcours DFS et effectué dans l'ordre affiché à la figure \ref*{Algorithme de parcours en profondeur (DFS)}

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/DFS.PNG}}
    \end{center}
    \caption[Algorithme de parcours en profondeur (DFS)]{\label{Algorithme de parcours en profondeur (DFS)} Algorithme de parcours en profondeur (DFS)}
\end{figure}

Ce qui est agréable en utilisant cet algorithme et qu'il nous permet de commencer à construire l'information à partir des noeuds terminaux
de l'arbre et de remonter naturellement au parent le plus proche.

Un parent va pouvoir obtenir la liste des enfants dans un ordre que l'on s'attendrait à recevoir.
Cela rend la manipulation dans un parent plus simple concernant des modifications que l'on voudrait effectuer pour représenter ces informations
dans un objet de type \emph{DocumentSymbol}.

\subsection{La position}

Généralement un noeud nous permet de nous rediréger sur l'élément du fichier lorsqu'on le séléctionne depuis la sidebar ou depuis la search bar. %todo reference
Il faut donc pouvoir être capable de récupérer la position d'un élément du fichier.

Heursement on peut récupérer cette information lors d'un parcours d'un noeud terminal du parse tree.
Il suffit d'observer le token associé au symbole de ce noeud et plus particulièrement les
propriétés \emph{line} et \emph{charPositionInLine} (qui est la position que l'on qualifierait de "colonne")
Il faut toutefois juste corriger la position de la ligne en lui soustraiant une valeur de 1 car l'indice d'une ligne commence à 0 sur VS Code et 1 pour ANLTR % TODO

\subsection{Résultat}
L'aspect visuel de l'outline view à été conçu pour ressembler à celle de JSON et de YAML.
Suivant cette exemple de code UON basique :

\begin{lstlisting}
    {
        name : "paul",
        age: 18,
        hobbys: {
            music: "rock"
        },
        parents : ["steve", "jeanne"]
    }
\end{lstlisting}

Nous obtiendrons l'outline suivant :

% capture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/uon-payload-outline-without-properties.PNG}}
    \end{center}
    \caption[Outline UON sans proprités]{\label{uon-payload-outline-without-properties} Outline UON sans proprités}
\end{figure}

Mais UON prenons en compte également les propriétés, il a été jugé pertinents de les rajouter dans la outline.
Pour ne pas changer la strucutre et ne pas surcharger l'arbre,
nous rajoutons simplement les propriétés des clés et des types comment enfant de l'attribut parent du noeud de l'outline.

Par exemple, si nous rajoutons des propriétés à la clé est un type :
\begin{lstlisting}
    {
        name : "paul",
        age(optional : false): !int( comment : "a comment") 18,
        hobbys: {
            music: "rock"
        },
        parents : ["steve", "jeanne"]
    }
\end{lstlisting}

Nous obtiendrons l'outline suivante :

%capture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/uon-payload-outline-with-properties.PNG}}
    \end{center}
    \caption[Outline UON avec proprité]{\label{uon-payload-outline-with-properties} Outline UON avec proprité}
\end{figure}

Les fichiers représentant un schéma de validation est légèrement différent :

\begin{lstlisting}
    !!mySensor: !schema ( description : "provide great infos", name : ok) {
        name : !str,
        power( optional : true) : !int( min : 5, max : 50)
    }
\end{lstlisting}

Dans un fichier de validation on ne peut avoir comme valeur uniquement des types ayant ou non des propriétés. Ce sont ces éléments qui nous intéresse le plus.
Donc on a plus de proprités "value props" mais on liste ces propriétés directement comme enfant de l'objet. Cela pour gagner en visibilté.

Nous aurons donc l'outline suivante :

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/uon-schema-outline.PNG}}
    \end{center}
    \caption[Outline d'un schéma]{\label{uon-schema-outline} Outline d'un schéma}
\end{figure}

\section{Info on Hover}

C'est une fonctionnalité assez triviale qui consite à afficher une explication lorsque l'on passe le curseur sur un élément.
Cela consite à aller chercher dans un dictionnaire si la valeur séléctionné est bien une clé du dictionnaire et de récupérer la valeur si c'est bien le cas.

Ce dictionnaire est un fichier.json qui est importé dans le fichier \emph{extension.js}.

Il est nécessaire de rajouter la ligne \emph{"resolveJsonModule": true} dans le fichier tsconfig.json pour autoriser ce type d'import.

Même si son implémentation est relativement simple, il n'en reste pas moins un élément fortement utile et pouvant être étendu par la suite.
Le code est déjà réutilisé pour fournir la documentation des types lors des suggestions de complétions  % \ref{}

\section{Lint}
%On affiche les erreurs trouvé avec antlr -> avantages positions
%Les erreurs sont liés au placement des tokens (manque un éléments du langages, mauvais token ou leur "ortographe").
%Message trop technique mais ok pour voir d'ou viens la faute.


Un Linter est un outil d'analyse de code qui permet de détecter les erreurs et les problèmes de syntaxe.

Il est donc nécessaire de pouvoir exploiter correctement les erreurs.

Il est possible de créer un listener qui aura pour rôle d'observer les erreurs lors du parsing avec ANLTR.
Pour cela il faut créer une classe qui implémente l'interface "ANTLRErrorListener" et attribuer ce listener à notre parser
avec la commande :
%#parser.addErrorListener(errorListener);

Cette interface définit la fonction \emph{syntaxError}.

Cette fonction sera trigger comme sans nom l'indique lors d'une erreur de syntaxe.
Une erreur de syntaxe peut survenir quand un élément et mal ortographié ou manquant.

Il est bien de rappeler que certaines erreurs de syntaxes sont plus grave ques d'autres.
Même si ANTLR propose des solutions pour se récupérer d'un processus de parsing défectueux, il reste limité et ne peux pas faire de miracle.

Elle contient des paramètres intéressant à exploiter pour informer l'utilisateur.

Sa définition est la suivante :
\begin{lstlisting}
    syntaxError?: <T extends TSymbol>(recognizer: Recognizer<T, any>, offendingSymbol: T | undefined, line: number, charPositionInLine: number, msg: string, e: RecognitionException | undefined) => void;
\end{lstlisting}

%Upon syntax error, notify any interested parties.
%This is not how to recover from errors or compute error messages.
%ANTLRErrorStrategy specifies how to recover from syntax errors and how to compute error messages.
%This listener's job is simply to emit a computed message, though it has enough information to create its own message in many cases.

Comme pour la outline %\ref*{} ,
Il est possible de récupér la position du token. Il suffit de récupérer les valeurs des propriétés \emph{line} et \emph{charPositionInLine}.
Il s'agira ici d'un token qui a posé problème et l'on voudra afficher à son emplacement l'indication visuelle (soulignement ondulé rouge) ainsi que le message d'erreur associé, lorsque l'utilisateur passe
sa souris dessus.

Le message d'erreur peut-être récupéré à partir de la propriété \emph{msg}. Même s'il est davantage destiné au debuggage.
Les messages restent relativement simple. Ils indiquent à l'utilisateur quel token pose problème et quels sont les caractères qui pourrait y être subtituer.

Pour afficher ces informations sur l'éditeur, il faut utiliser un objet VS Code de type \emph{DiagnosticCollection}.

Elle peut afficher une liste d'erreur.

On va garder en mémoire cette liste dans la classe que l'on mettra à jour à chaque fois que le listener détecte une erreur.
Et on actualisera à chaque fois l'objet \emph{DiagnosticCollection} avec cette liste.

Cependant le parser ne pas se rendre compte si ce n'est pas plutôt le token précédent qui pourrait ammener à une erreur.
Cela ne prends non plus pas en compte ce qu'il vient après et donc remplacer un token par un de ceux proposé peut se revélé contre productif.

\subsection{Suggestion}
Certains messages d'erreurs nous informes parfois des tokens qui devrait se situer à la place de celui qui a causl l'erreur.
Une idée serait donc de pouvoir remplacer ce token erronées par ceux suggérés.
Le problème et que cette information ne se trouve sous forme textuelle.
Il faudrait donc pouvoir analyser les fichiers de stratégie et les modifiers pour en récuper la liste des tokens....

% TODO surcharger des méthodes pour pouvoir être notifié des tokens manquant

\subsection{Quick fick}

Actuellement il y a un seul quick fix qui a été implémenter et cela à titre de découverte.

Ce quickfix permet de supprimer le token qui pose problème en se basant sur les ranges fourni lors de la création d'un objet Diagnostic depuis le listener.

A l'heure actuelle , il n'y a pas encore de suggestions à proprement parlé.

L'idée serait de pouvoir remplacer le token par un ou plusieurs qui aurait du sens.
Et l'idéal serait de prendre en compe le texte venant après le curseur.

\section{Fonctionnalités triviales}
Il s'agit de fonctionnalités couramment disponibles dans un support de langage. Leur implémentation est relativement simple et rapide et n'a donc pas été mentionnée dans le cahier des charges.
Les suivantes ont été implémentées :

\textbf{Comment}
\begin{itemize}
    \item Il est possible de commenter du code sous forme de ligne.
    \item Il est possible de commenter et décommenter du code (Toggling)
\end{itemize}

\textbf{Symbol pairs}
\begin{itemize}
    \item Il est possible de faire la correspondance pour certaines paires de symboles à l'aide d'une indication visuelle. (Matching)
    \item Certains symboles du langage doivent être automatiquement complétés si l'utilisateur saisit le premier élément de celle-ci. (Autoclosing)
\end{itemize}

\textbf{Code folding}
\begin{itemize}
    \item Cacher un bloc de code sur une ligne en fonction de son niveau d'indentation.
\end{itemize}

% TODO
% Couplage de fonctionnalité - logique
% Info on hover - autocompletion
% lint - code outline

\chapter{Planning}

% TODO : Ajouter discussion de l'avancée du travail. Êtes-vous à jour ? Avez-vous pris du retard ?
% TODO : Des implémentations pas compliqué mais peu de documentation sur antlrts dans le cadre d'une extension + faut trouver comment faire en testant parfois
% TODO :
%    Difficulté :
%    Regrouper toutes les sources.
%    Grand projet personnel....
%    Analyse des resultat

\subsection{Diagramme de gantt}
Voici le diagramme de Gantt pour ce projet.
Ce diagramme sera potentiellement mis à jour durant ce semestre du fait que certaines tâches ne sont pas connues à l'avance et pour réorganiser des taches si besoin.

\includepdf[page=1]{assets/planning/gantt.pdf}

\chapter{Conclusion}
% TODO

% what should be included in a conclusion ?
% Restate your thesis.
% Synthesize or summarize your major points.
% Make the context of your argument clear.


Travailler sur ce projet s'est revélé être très intéressant.

%L'objectif de ce travail de bachelor est de pouvoir avoir à disposition dans l'éditeur, les éléments de base pour
%Sans devoir consulter la documentation

Il a fallu découvrir et analyser un nouveau langage.
Définir quelles sont les fonctionnalités à implémenter dans un éditeur pouvant s'avérer utiles comme support pour ce langage.

Découvrir les technologies permettant leur implémentation et ensuite pouvoir les utiliser en sein de notre application. % fonctionnalité cohereent entre elles


%TODO : déplacer
%Il a été mentionner à plusieurs reprise qu'un parser est un élément pouvant nous faire aboutir à notre but.
%Heursement pour nous, des outils existent pour en générer un depuis une grammaire, nous faisant gagner un temps de développement énorme.
% De plus avec cette approche cela nous permet de travailler sur la grammaire en même temps que les fonctionnalités à implémenter.

Nous avons vu qu'il est possible de s'aider d'outils déja existant et que cela peut, si bien utiliser, accelérer grandement la mise en place d'un support de langage.

ANTLR s'est avérer être une composante dominante du projet. De plus sa gestion des erreurs permet d'avoir une meilleur tolérance concernant les erreurs
qui sont fréquent dans un éditeur.

L'outil de complétion c3, qui peut-être utilisé avec un parseur ANTLR a aussi été une très bonne découverte\dots

% TODO : Déplacer
%Nous avons vu est pu classifier les fonctionnalités dans deux catégories : Declarative et Programmatic features
%La première est fortement couplé à l'editeur. Pour la seconde nous avons choisi l'approche consistant à utiliser l'api de vscode.
%Mais cette api étant standardisé avec la norme LSP.
%Déplacer ces fonctionnalités en sein d'un langage serveur est possible

%% Les points d'améliorations : LSP
%% Avoir une notion de scope / schéma de validation interne si possible



%Implémenter du support nous oblige à analyser un langage en profondeur\dots

%Le support d'un langage est un sujet vaste et nous n'avons vu qu'une infime parti de ce qui est possible de faire.

%C'est aussi un sujet qui est très libre


Pour pouvoir fournir du support pour un lanagage, on est obliger de connaitre la grammaire sur laquelle ces fonctionnalités porteront.
Heursement pour nous nous avons pu reprendre une implémentation existante que nous avons adapté pour couvrir nos besoins.


Les éléments qui on recquis le plus de travail ont été la complétion.
Car même si le moteur est extrêment puissant et qu'il convient très bien pour notre utilisation. Il reste toutefois important de comprendre son fonctionnement et que c'est un outil pouvant s'avérer complexe.

Adapter la grammaire a été un challenge ANTLR et différent de lark, la complétion nécessite aussi d'adapter la grammaire.

La outline ?
La subtiliter concernant les capture regex dans la grammaires

Nous avons fais le choix de travailler uniquement dans un environnement mais fournissant toutes les clés pour une extensibilités futurs

VS Code est un environnement riche et complexe mais qui est extrêmement bien documenter et agréable à utiliser.
Il est toutefois parfois même difficile de savoir ce que peut faire cet éditeur par défaut ou non.




De plus chaque langage ayant ses spécifités, les besoins peuvent variées les uns des autres.

De même, les même fonctionnalités entre deux langage peuvent avoir des comportements relativement différent.

L'avantage d'avoir travaillé sur un langage de sérialisation comme UON est que cela est relativement moins complexe à gérer qu'un langage de programmation.
C'est une bonne porte d'entrée ...
Cela nous oublige à découvrir et prendre conscience ce qu'il est possible de faire plus généralement.

Mais UON est aussi plus complexe que ces concurents sur pleins de points.
Ce qu'il fait qu'il reste encore des améliorations possibles

Lors de la réalisation de ce Travail de bachelor, nous avons mis on place des méchanismes


Un point contraignant lorsque l'on développe un support de langage, et que la grammaire est intrinsèquement liés à certaines fonctionnalités.
Un changement peu avoir des repercussions sur l'ensemble de celles-ci\dots


%% \vfil
%% \hspace{8cm}\makeatletter\@author\makeatother\par
%% \hspace{8cm}\begin{minipage}{5cm}
%%if
% Place pour signature numérique
%%\printsignature
%%fi
%% \end{minipage}
%% \clearpage

%% \let\cleardoublepage\clearpage
%% \backmatter

%% TODO : Ne fonctionne pas !!!
\label{glossaire}
\printnoidxglossary
\addcontentsline{toc}{chapter}{Glossaire}

\textbf{TODO : Ajouter dans la Bibliographie https://www.regular-expressions.info/anchors.html}
\printbibliography
\addcontentsline{toc}{chapter}{Bibliographie}

%% \label{index}
%% \printindex

\end{document}
