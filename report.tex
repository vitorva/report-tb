\documentclass[
    iict, % Saisir le nom de l'institut rattaché
    il, % Saisir le nom de l'orientation
    %confidential, % Décommentez si le travail est confidentiel
]{heig-tb}

\usepackage{float} % pour forcer l'empalcement d'une image
% https://tex.stackexchange.com/questions/8625/force-figure-placement-in-text

\usepackage{titlesec}

\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}

\usepackage[nooldvoltagedirection,european,americaninductors]{circuitikz}
\usepackage{hyperref}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{pdfpages}

\usepackage{dirtree}
\usepackage{caption}
\usepackage{subcaption}


%https://www.overleaf.com/project/62b06d8b8257fff1974999df
%\signature{mbernasconi.svg} TODO : Gérer la taille
\signature{va_signature.png}

\makenomenclature
\makenoidxglossaries
\makeindex

\addbibresource{bibliography.bib}

\input{nomenclature}
\input{acronyms}
\input{glossary}
\input{meta}

%\surroundwithmdframed{minted}

%% Début du document
\begin{document}
\selectlanguage{french}
\maketitle
\frontmatter
\clearemptydoublepage

%% Requis par les dispositions générales des travaux de Bachelor
\preamble
\let\cleardoublepage\clearpage
\authentification
\let\cleardoublepage\clearpage

%% Résumé / Version abbrégée
\begin{abstract}
    \input{abstract}
\end{abstract}

%% Sommaire et tables
\listoffigures
\addcontentsline{toc}{chapter}{\listfigurename}
\listoflistings
\addcontentsline{toc}{chapter}{Liste des codes sources}

\tableofcontents

\printnomenclature
\clearemptydoublepage
\pagenumbering{arabic}

\pagestyle{fancy}
\fancyhf{}
\renewcommand\headrulewidth{1pt}

\fancyhead[R]{Support du langage UON}
\fancyhead[L]{\itshape\nouppercase{\leftmark}}

\renewcommand{\chaptermark}[1]{\markboth{\MakeUppercase{#1}}{}}

\renewcommand\footrulewidth{1pt}

\fancypagestyle{plain}{%
    \fancyfoot[R]{Page \thepage/\pageref{LastPage}}
}
\fancyfoot[R]{Page \thepage/\pageref{LastPage}}

\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

\titlespacing*{\chapter}{0pt}{-40pt}{20pt}

%% Contenu
\mainmatter
\chapter{Introduction}

%"Contexte" qui explique comment s'insère ce travail de Bachelor. C'est quoi UON est-ce que quelqu'un a déjà travaillé dessus, ce qu'il a fait, ce qu'il reste à faire.

Ce projet de Bachelor consiste à fournir du support pour le nouveau langage de sérialisation UON, sous VS Code.
UON est un format de sérialisation qui vise à rassembler les meilleures caractéristiques des principaux formats de sérialisation disponibles sur internet.

Ce projet consiste donc à fournir à l'utilisateur, les outils pour lui permettre la rédaction la plus optimale d'un fichier UON dans l'éditeur VS Code.

C'est un nouveau langage complexe et une spécification détaillée existe, mais elle n'est pas terminée. Il reste maintenant à construire les éléments permettant son utilisation.
Un parser en Python a déjà été implémenté par Stéphane Selim durant son travail de bachelor "Parser for a serialization language UON" en 2020.
Ce travail s'était concentré sur trois aspects : la sérialisation et déserialisation binaire et textuelle d'objet en Python, la coercition entre les valeurs et la validation de fichiers.

Cepedendant ce travail de bachelor n'en est pas la suite directe.
Il consiste à fournir une autre composante qui est de permettre l'utilisation du langage UON dans un éditeur pour qu'il soit utilisé par un plus grand nombre.
Et à étudier les différentes approches et les mettre en oeuvre.
Les alternatives et améliorations possibles seront spécifiées quand cela est pertinent.
Le projet se veut aussi être expérimental et ouvert à la discussion.
Aucun cadre précis sur sa réalisation n'a été établi et les approches choisies sont libres tant qu'elles respectent le cahier des charges.

Ce travail se focalise uniquement sur VS code dans un premier temps. Le choix de cet éditeur sera expliqué plus tard.

Dans les chapitres suivants, nous allons :

\begin{itemize}
    \item Se renseigner concernant ce qui est fourni par VS Code pour élaborer et déployer une extension.
    \item Se renseigner sur les éléments à prendre en compte pour pouvoir fournir du support pour un langage.
    \item Choisir les aspects du langage UON à traiter dans le cadre de ce projet.
    \item Documenter et réaliser l'implémentation d'une extension, son intégration continue ainsi que ses fonctionnalités.
    \item Documenter et mettre en place les tests effectués pour les différents composants du projet.
    \item Documenter également des pistes de réflexion, les alternatives et améliorations possibles.
\end{itemize}

\let\cleardoublepage\clearpage

\chapter{Cahier des charges}

L'objectif de ce travail de Bachelor est l'élaboration d'une extension VsCode pour la prise en charge du langage de sérialisation UON.

Cela consistera dans un premier temps, à implémenter un certain nombre de fonctionnalités considérées intéressantes à avoir initialement pour un nouveau langage dans un éditeur de code.

Puis si le temps le permet, de rajouter des fonctionnalités supplémentaires.

\subsection*{VSCode Extension}
\begin{itemize}
    \item Réaliser une extension dans le code source se trouve sur Github.
    \item Publier l'extension sur \href{https://marketplace.visualstudio.com/}{le marketplace} de Visual Studio.
    \item Documenter la mise en place de l'extension et l'implémentation des fonctionnalités.
    \item Le système doit disposer d'une intégration continue.
\end{itemize}

\subsection*{Grammaire}
\begin{itemize}
    \item Indiquer le "scope" de la grammaire UON utilisée.
\end{itemize}
\textbf{Remarque} : toutes les fonctionnalités se baseront sur cette grammaire.

\subsection*{Fonctionnalités à implémenter}
\subsubsection*{Syntax highligting}
\begin{itemize}
    \item Chaque élément du texte doit être affiché dans l'éditeur avec une coloration et un style en fonction de son type.
\end{itemize}

\subsubsection*{Auto-complétion}
\begin{itemize}
    \item Des propositions de complétions doivent apparaitre pendant la saisie d'une chaine de texte si des complétions possibles existent pour une telle chaine.
\end{itemize}

\subsubsection*{Document Outlining}
\begin{itemize}
    \item Fournir le "Symbol tree" du document.
\end{itemize}

\subsubsection*{Hover Information}
\begin{itemize}
    \item Lorsque l'on passe le curseur sur un élément du texte, des informations le concernant doivent apparaitre s'ils existent pour celui-ci.
\end{itemize}

\subsection*{Si le temps le permet}

\subsubsection*{Lint}
\begin{itemize}
    \item Détecter les erreurs de syntaxe pendant la saisie et les afficher.
    \item Proposer des solutions de corrections.
\end{itemize}

\subsubsection*{Formatter (outil permettant de formater le code)}
\begin{itemize}
    \item Il est possible de formater le code au format minimal.
    \item Il est possible de formater le code au format canonique.
\end{itemize}

\subsubsection*{Converter (outil permettant de convetir du code)}
\begin{itemize}
    \item Du code JSON est capable d'être converti en UON et inversement.
    \item Du code YAML est capable d'être converti en UON et inversement.
\end{itemize}

\chapter{Pré-étude}

\section{Parsing}\label{parsing}

Le résultat de l'analyse syntaxique (parsing) nous permet d'obtenir une représentation de la structure d'un texte. Ce qui est très utile dans le cadre d'un support de langage.

Il est donc important de rappeler le mécanisme général. Car avant la phase de l'analyse syntaxique, il y a habituellement une analyse lexicale.
À cette étape, un input (du code ou texte) est décomposé en une liste de tokens à l'aide d'un lexer.

Ce sont ces tokens qui seront ensuite utilisés lors de l'analyse syntaxique.
L'analyse syntaxique consiste à regarder quelles sont les règles qui sont applicables pour une suite de tokens définie dans une grammaire.

Le résultat de cette analyse est un "Parse Tree" qui représente toutes les dérivations de notre input en fonction de la grammaire.
Elle est souvent représentée sous forme d'un arbre hiérarchique.

Généralement c'est cette structure finale qui sera interprétée ou compilée.

Ce processus peut être représenté par la figure \ref{Analayse du code source}.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/interpreter-flow.PNG}}
    \end{center}
    \caption[Analayse du code source]{\label{Analayse du code source} Analayse du code source}
\end{figure}

Dans notre cas, nous utiliserons seulement l'arbre et uniquement lorsque cela sera nécessaire.

\section{VS Code}
Il a été choisi, car il fait partie des éditeurs les plus populaires. C'est également l'éditeur que j'utilise le plus personnellement et donc je suis déjà familier de son environnement.
D'autres alternatives existent comme VIM et Atom pour ne citer qu'eux. Et chacun d'eux fournit également de quoi implémenter une extension.
C'est donc un choix personnel.

Un éditeur de code est un logiciel destiné à la création et l'édition de fichiers textes. Plus généralement des fichiers statiques, comme un langage de sérialisation. Contrairement à un IDE (Environnement de développement) qui permet en plus la compilation et le debugging de programme plus complexe.

Visual Studio Code (ou VS Code) est un éditeur de code multi-plateforme, open source et gratuit.

Il est pensé pour être extensible (de L'UI à l'expérience utilisateur), presque tout peut être customisé et amélioré à travers de L'Extension API \cite{extension-api}.
On peut mentionner le theming, les fonctionnalités de langages, la publication de l'extension et les tests.

VS Code fournit des documentations détaillées sur plein de sujets : étapes pour la création d'une extension, son déploiement continue ainsi que le support de langage en lui-même.
Il est aussi possible facilement de trouver des guides, ainsi que \href{https://github.com/microsoft/vscode-extension-samples}{des exemples de codes} concernant les extensions.
Il s'agit donc évidemment de la source principale des explications données dans ce document concernant ce qui touche à cet éditeur.
La communauté est aussi très active (stack overlow, gitter, issue github, etc.).

Le code source de VS Code de Microsoft est open source, sous licence MIT permissive.
Il a été écrit en TypeScript et JavaScript. Une extension peut être également écrite dans ces deux langages.
Nous choisirons le TypeScript car VS Code recommande ce langage. Mais certaines composantes externes (langage server) peuvent être écrites dans n'importe quel langage. Tant qu'elles fournissent l'API adéquate pour communiquer.

\subsection{Command palette}\label{command-palette}

La \href{https://code.visualstudio.com/docs/getstarted/userinterface#_command-palette}{command palette}
est le point d'accès à toutes les fonctionnalités de VS Code.

Pour l'ouvrir il faut exécuter la combinaison suivante : "CTRL + SHIFT + p".

\subsection{Extension Anatomy}
Il y a trois concepts cruciaux à comprendre pour réaliser une extension \cite{extension-anatomy} .

\begin{itemize}
    \item \href{https://code.visualstudio.com/api/references/activation-events}{\textbf{Activation Events}}: des événements à partir desquels l'extension devient active.
    \item \href{https://code.visualstudio.com/api/references/contribution-points}{\textbf{Contribution Points}}: des déclarations statiques qui sont faites dans l'Extension Manifest \emph{package.json} pour étendre l'extension. Il s'agit d'un ensemble de déclarations JSON faites au travers du champ \emph{contributes}.
    \item \href{https://code.visualstudio.com/api/references/vscode-api}{\textbf{VS Code API}}: un ensemble d'API JavaScript que nous pouvons invoquer dans le code.
\end{itemize}

En général, l'extension est une combinaison de plusieurs \emph{Contribution Point} et l'utilisation de l'API pour étendre les fonctionnalités de VS Code.

\subsubsection{Extension File Structure}\label{Extension File Structure}

La structure de base d'une extension est illustrée par la figure \ref{Squelette de base d'une extension VS Code}.

% https://tex.stackexchange.com/questions/98703/framebox-and-subfigure-with-dirtree-package
\begin{figure}[H] % \begin{figure}[!h]
    \centering
    \framebox[\textwidth]{%
        \begin{minipage}{0.9\textwidth}
            \dirtree{%
                .1 ..
                .2 vscode.
                .3 launch.json     // Config for launching and debugging the extension.
                .3 tasks.json      // Config for build task that compiles TypeScript.
                .2 .gitignore.
                .2 README.md.
                .2 src.
                .3 extension.ts   // Extension source code.
                .2 tsconfig.json  // TypeScript configuration.
            }
        \end{minipage}
    }
    \caption[Squelette de base d'une extension VS Code]{\label{Squelette de base d'une extension VS Code} Squelette de base d'une extension VS Code}
\end{figure}

\textbf{Extension Manifest} :
chaque extension doit contenir un fichier \emph{package.json}. En plus des champs propres à Node.js, on peut spécifier des scripts, dépendances de développement et des champs spécifiques à VS Code.

\textbf{Extension Entry File} :
il s'agit du fichier principal de l'extension (Extension.js).
De base, il contient deux fonctions : \emph{activate} et \emph{deactivate}.
La fonction activate est exécutée à l'activation de notre extension par un \emph{Activation Event}. On initialisera ici notre extension.
La fonction \emph{deactivate} est exécutée lorsque l'application devient inactive et sert principalement à nettoyer le code avant la désactivation de l'extension

\textbf{Remarque} : le niveau de customisation est assez élevé. La seule limitation indiquée et qu'il n'est pas possible d'accéder au DOM de l'éditeur.

Nous avons donc vu les briques de bases pour la conception d'une extension. Mais cette base est suffisante pour commencer à travailler.

Si vous souhaitez vous renseigner d'avantages sur l'élaboration d'une extension vous pouvez consulter la page \href{https://code.visualstudio.com/api}{suivante}.
Concernant les fonctionnalités générales de l'éditeur VS Code vous pouvez consulter \href{https://code.visualstudio.com/docs}{la documentation officielle}.

\section{Support de langage}
VS Code fournit la possibilité d'ajouter du support pour un nouveau langage de programmation au travers d'implémentation de fonctionnalités. Ces fonctionnalités peuvent être classées en deux catégories détaillées
dans les sections \hyperref[Declarative language features]{Declarative language features} et \hyperref[Programmatic language features]{Programmatic language features}.

\subsection{Declarative language features}\label{Declarative language features}
Elles ajoutent un support d'édition de texte de base pour un langage de programmation.
Par exemple, les éléments suivants :

\begin{itemize}
    \item syntax highlighting
    \item snippet completion
    \item bracket matching
    \item bracket autoclosing
    \item bracket autosurrounding
    \item comment toggling
    \item auto indentation
    \item folding (by markers)
\end{itemize}

Il s'agit de fonctionnalités implémentées à l'aide de fichier de configuration.

% TODO : faire ref à l'explication ci-dessus ?
Puis, elles doivent être enregistrées comme \emph{Contribute Point}.

\subsection{Programmatic language features}\label{Programmatic language features}
Il s'agit de fonctionnalité plus riche et plus complexe à mettre en place.
Par exemple, les éléments suivants :

\begin{itemize}
    \item hover information
    \item jump to Definition
    \item bracket matching
    \item error checking
    \item bracket autosurrounding
    \item auto completion
\end{itemize}

Il y a deux approches pour les implémenter que nous allons voir ci-dessous.

\subsubsection{VS Code API (Direct implementation)}
La première solution est d'utiliser \href{https://code.visualstudio.com/api/references/vscode-api#languages}{l'API de VS Code} pour les fonctionnalités de langage.
Cette API permet d'implémenter directement les fonctionnalités du langage sans devoir mettre en place l'infrastructure les supportant.
% TODO : s'assurer que l'api vscode une implementation de LSP ?
% Comment faire la transition entre mon implémentationa avec celle d'un langage server

Beaucoup de fonctionnalités se font en s'inscrivant à des providers depuis notre application.
L'éditeur de code fera ensuite les requêtes à ces providers lorsque cela sera nécessaire.

\subsection{Language server}

La seconde solution est de fournir nous-mêmes ces méthodes en respectant la spécification LSP \cite{lsp-specification} au travers d'un langage serveur.
Les avantages souvent mentionnés ce cette approche sont que le langage server peut être écrit avec le langage que l'on souhaite et
que cela permet aussi à d'autres éditeurs de texte compatibles avec le langage server d'utiliser ces fonctionnalités sans devoir les implémenter de nouveau.

Pour être utilisé sur VS Code, un langage server à deux parties :
\begin{itemize}
    \item \textbf{Un client} : c'est une extension écrite en Javascript ou Typescript qui à accès à tous les endpoints de VS Code.
    \item \textbf{Langage server} : un outil d'analyse linguistique fonctionnant dans un processus séparé.
\end{itemize}

\vspace{\parskip}

Le client et le serveur communiquent à l'aide du protocole LSP (pour "language server") dès que des informations devraient être fournies à l'éditeur.

L'implémentation d'un tel serveur peut être libre en respect avec la spécification, mais des implémentations existent déjà. Telle que \emph{LSP4} écrit en Java ou \emph{VS Code-languageserver-node}
écrit en Typescript qui n'est pas exclusivement réservé à VS Code comme son nom pourrait l'indiquer.

Un langage server peut être utilisé sur d'autre éditeur compatible, mais le client devra être implémenté de nouveau pour chaque éditeur.
Cette approche est aussi plus complexe à implémenter.

\subsection{API vs LSP}\label{api vs lsp}

Le schéma suivant \ref{API de VS Code vs méthodes LSP} montre la correspondance des méthodes entre la première et seconde approche.

Cette liste est non exhaustive, mais permet de montrer l'équivalence des deux solutions.

\begin{figure}[!ht]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/api-vscode.png}}
    \end{center}
    \caption[API de VS Code vs méthodes LSP]{\label{API de VS Code vs méthodes LSP} API de VS Code vs méthodes LSP}
\end{figure}

\subsection{Fault tolerant Parser}
Pour fournir des fonctionnalités de langue, un point souvent mentionné est l'utilisation d'un parser.
Car l'arbre généré par celui-ci contient des informations concernant la structure du document utiles pour certaines fonctionnalités.
Il s'agira souvent en pratique d'un \Gls{ast}.

Des outils permettant la génération d'un parser existent à partir d'une grammaire et seront privilégiés.

Cependant dans le cadre de son utilisation dans un éditeur ou IDE, il devrait être "fault tolerant".
Car la plupart du temps, le code dans l'éditeur est incomplet et syntaxiquement incorrect.
Mais l'on s'attend tout de même que certaines fonctionnalités continuent à fonctionner (ex : la outline view).

Le parseur doit donc pouvoir générer au mieux un parse tree qui à du sens depuis du code erroné.

On pourrait également se demander s'il est possible de se passer d'une telle strucutre et de n'utiliser à la place uniquement des expressions régulières.
Cependant un problème majeur survient rapidement si on choisit cette approche, la récursivité devient extrêmement difficile à traiter. \cite{antlr-mega-tutorial}

%"you cannot find a (regular) expression inside another one, unless you code it by hand for each level. Something that quickly became unmaintainable."
%Example : % TODO

\section{Choix technologiques}
Nous allons maintenant mentionner les technologies et approches choisies et expliquer les raisons.

\subsection{Direct implementation}
L'implémentation d'un langage serveur n'étant pas une priorité et pour se concentrer sur la réalisation des fonctionnalités, la solution de contacter directement l'API de VS Code a été privilégiée.
Si le temps le permet, toute la logique du code concernant les fonctionnalités riches\ref{Programmatic language features} pourrait être déplacée dans un langage server.

Beaucoup de possibilités existent comme mentionné au point \hyperref[api vs lsp]{API vs LSP}, les providers utilisés sont les suivants :
\begin{itemize}
    \item \textbf{registerCompletionItemProvider} : pour afficher des suggestions de complétions.
    \item \textbf{registerHoverProvider} : pour gérer le hover.
    \item \textbf{registerDocumentSymbolProvider} : pour afficher les éléments dans la Outline View.
\end{itemize}

\subsection{ANTLR}

\Gls{antlr} est un générateur de parser qui est actuellement dans sa quatrième version.
Il a été créé par Terence Parr à l'Université de San Francisco et est très utilisé, autant dans le monde académique que professionnel \cite{antlr}.

Le générateur prend en entrée une grammaire, qui est un fichier décrivant formellement un langage. La grammaire d'ANTLR est décrite en utilisant la notation \Gls{ebnf}.

ANTLR est composé de deux parties principales. Il y a l'outil en java qui permet de générer le lexer et le parser, donc le code permettant de reconnaitre un langage comme mentionné dans la section \ref{parsing}.
Ainsi que l'environnement d'exécution (runtime) qui permet leur exécution.

Il y a donc, seulement un seul outil qui permet de générer le code dans les langages cibles. Et il est écrit en Java.
Pour pouvoir utiliser cet outil dans un environnement autre que Java, il est nécessaire de l'utiliser dans un runtime (environnement d'exécution) propre au langage souhaité.

Le runtime ainsi que les fichiers générés doivent être dans le même langage. La liste se trouve \href{https://github.com/antlr/antlr4/blob/master/doc/targets.md}{ici}.
Malheursement, le typscript n'en fait pas partie.

\subsubsection{antlr4ts}
C'est \emph{antlr4ts} qui sera finalement choisi. C'est un outil permettant d'utiliser ANTLR dans environnement d'exécution en typescript et donc parfaitement adapté pour VS Code.
Il a été conçu par Sam Harwell indépendamment de l'organisation ANTLR.

% https://github.com/tunnelvisionlabs/antlr4/blob/master/doc/targets.md

C'est un outil qui semble populaire est très utilisé.

La figure \ref{Utilisation du parser d'ANTLR} illustre un exemple simple d'utilisation.

\begin{listing}[!ht]
    \begin{minted}{typescript}
        let inputStream = new ANTLRInputStream("var c = a + b()");
        let lexer = new ExprLexer(inputStream);
        let tokenStream = new CommonTokenStream(lexer);

        let parser = new ExprParser(tokenStream);
        let errorListener = new ErrorListener();
        parser.addErrorListener(errorListener);
        let tree = parser.expression();
    \end{minted}
    \caption{Utilisation du parser d'ANTLR}
    \label{Utilisation du parser d'ANTLR}
\end{listing}

\subsubsection{ATN}

La version 4 de ANTLR a comme avantage sur son ancienne version (ANTLR3) d'avoir la structure de la grammaire directement disponible dans le parser via
un mécanisme de machine à état \Gls{atn}.

La figure \ref{ATN d'une séquence} montre un atn d'une règle de parser.

\begin{figure}[!ht]
    \begin{center}
        \includegraphics[width=12cm]{assets/figures/seq_ATN.png}
    \end{center}
    \caption[ATN d'une séquence]{\label{ATN d'une séquence} ATN d'une séquence}
\end{figure}

Cela permet de comprendre que le parser passe par plusieurs états lors d'une analyse syntaxique.
Cette image a été généré en utilisant le plugin \href{https://marketplace.visualstudio.com/items?itemName=mike-lischke.vscode-antlr4}{ANTLR4 grammar syntax support}.

\subsubsection{Gestion des erreurs}

ANTLR est capable de signaler des erreurs et se rétablir d'erreurs plus graves qui pourraient faire échouer le processus de parsing.
ANTLR définit l'interface \emph{ANTLRErrorStrategy} pour résoudre ce problème.
Elle sert à définir une stratégie concernant les erreurs rencontrées.

On peut distinguer trois types d'erreurs \cite{ANTLRErrorStrategy} :
\begin{itemize}
    \item L'analyseur syntaxique n'a pas pu déterminer le chemin à prendre dans l'ATN (aucune des alternatives disponibles ne pouvait correspondre).
    \item L'input actuel ne correspond pas ce que nous attendons.
    \item Un prédicat évalué comme faux.
\end{itemize}

Par défaut c'est l'implémentation \emph{DefaultErrorStrategy} qui est appliquée et qui consiste à supprimer un token qui poserait problème ou en rajouter un s'il manquerait, avant de
se resynchronisation à l'ATN et de continuer le parsing. Cette resynchronisation permet de continuer dans la règle courante que traverse le parser. \cite{antlrs-error-handling-strategy}

S'il ANTLR n'a pas pu se rétablir et qu'il a encore des erreurs, il va consommer tout les tokens qui suit celui qui a posé problème.
Puis, ce mécanisme s'arrêtera quand il réussira à trouver un token qui lui permettra de quitter la règle de parser courante sans la valider, mais permettant de continuer le processus de parsing.
Au pire des cas il s'agira d'un token de retour à la ligne \Gls{eof}.

C'est grâce à ce mécanisme qui fait que si nous utilisons le \emph{DefaultErrorStrategy} nous obtiendrons un parse tree dans tous les cas.
Le résultat du parsing n'a pas besoin d'être correct. Il est possible de travailler avec du code incomplet ou malformé, ce qui est relativement fréquent dans un éditeur.

Malheursement, je n'ai pas pu explorer plus en profondeur ce mécanisme complexe de resynchronisation.
Cela pourrait se révéler intéressant pour tolérer plus d'erreurs au lieu d'une seul pour l'instant.

\section{UON}
%aborder uniquement les points globaux + essentiles et utiles pour nous

UON est essentiellement un langage de sérialisation qui est un superset de JSON et un superset partiel de YAML.
Il cherche à regrouper les meilleures caractéristiques de ces formats de sérialisation en un seul format.
Il fournit également des fonctionnalités supplémentaires utiles pour augmenter l'interopérabilité entre différents types de dispositifs, valider les données ainsi que pour diminuer le payload.

Nous allons voir ci-dessous un aperçu général de ce langage.

\subsection{Format de sérialisation}

Lorsqu'on sérialise des données, elles se retrouvent soit sous forme binaire ou soit textuelle.
Ce projet se focalisera naturellement uniquement sur le format textuel, car c'est le seul format, humainement interprétable, que l'on manipulera dans un éditeur.
Le rôle d'un format de sérialisation et de pouvoir représenter des données sous une autre forme pour être ensuite manipulé.

Un éditeur ne manipulant qu'un fichier statique, les étapes de sérialisation et désérialisation ne se feront pas à notre niveau.
Pour pouvoir profiter de ces fonctionnalités, cela nécessite d'avoir un environnement de programmation (ex : Python).


\subsection{Pourquoi ?}
Son rôle est d'être utilisé dans l'industrie 4.0. Plus particulièrement dans la communication \Gls{m2m} ainsi que pour l'\Gls{IoT}.
Ces communications nécessitent souvent de communiquer entre de petits appareils dont la puissance de calcul est très limitée.

\subsection{Communication}
Lorsque deux appareils veulent échanger des informations, ils disposent de deux canaux de communication :
\begin{itemize}
    \item Un canal de communication en ligne utilisé pour la transmission des données de contenu (payload). Les données peuvent être représentées sous forme humaine et binaire.
    \item Un canal de communication contractuel utilisé pour l'accord sur la description des données (schema).
\end{itemize}

\vspace{\parskip}

À la figure \ref*{data-exchange}, on a à gauche deux dispositifs : un capteur de température à faible consommation
et une puissante passerelle domotique. Pour réduire la taille du payload, les deux dispositifs peuvent convenir d'un schéma qui décrit le format des données.

\begin{figure}[H]
    \begin{center}
        \frame{\includegraphics[width=15cm]{assets/figures/data-exchange.png}}
    \end{center}
    \caption[Échange de données en UON]{\label{data-exchange}Échange de données en UON}
\end{figure}

À droite on affine le schéma. Pour réduire davantage les données transmises.
Par exemple, en disant que la température exprimée avec un nombre est maintenant une valeur non signée de huit bits exprimée en degrés Celsius.

\subsection{Design}
UON est complètement conforme avec JSON et partiellement avec YAML.

Il peut être décomposé au travers de 4 niveaux de complexités :
\begin{itemize}
    \item \textbf{UON:0} est entièrement compatible avec JSON en ce qui concerne le RFC8259.
    \item \textbf{UON:1} est partiellement conforme à YAML.
    \item \textbf{UON:2} fournit des propriétés de type, la coercition et les chaînes de caractères multilignes.
    \item \textbf{UON:3} offre des types et des références riches.
\end{itemize}

\subsection{Types}
% TODO


\subsection{Propriétés}\label{properties}
Chaque élément est composé d'un type. Et chacun de ces types peut avoir des propriétés.
3 types de propriétés existent :
\begin{itemize}
    \item \textbf{Presentation properties} : elles influencent la présentation d'une valeur.
    \item \textbf{Validation propeties} : elles sont utilisées pour valider, contraindre et décrire un fichier UON.
    \item \textbf{Application properties} : elles ne peuvent être lues que depuis UON en utilisant le type !prop. Elles sont accessibles depuis l'application (Python, JavaScript, etc.). Elles sont utilisées pour générer un fichier sérialisé (binaire, ou UON), mais elles ne sont jamais explicitement transmises.
\end{itemize}

\subsection{Validation}
Dans de telles communications, la validation des données transmises entre des machines dont la puissance de calcul est très limitée est un atout majeur.
Une des propriétés uniques de UON est l'usage de schéma directement intégré dans le langage.

C'est une structure qui décrit comment un fichier ou type doit être représenté.

UON propose 4 différents types de stratégies.

\begin{itemize}
    \item Embedded schema
    \item Included schema
    \item Linked schema
    \item Separated schema
\end{itemize}

%  Un schéma est un document qui décrit la spécification d'un nouveau type.

\subsection{Liens}
Si vous souhaitez vous renseigner d'avantages, la spécification complète du projet se trouve sur cette \href{https://github.com/uon-language/specification/}{page}.
Et le dépôt du projet se trouve \href{https://github.com/uon-language/specification}{ici}.

\chapter{Scope de la grammaire}\label{grammar scope}
Pour pouvoir proposer du support pour un nouveau langage, il est nécessaire de savoir sur quoi celui-ci portera exactement.
Il est donc important de préciser sur quels éléments du langage nous nous focaliserons.

Une spécification d'UON existe, elle couvre énormément d'aspects. Cependant, tout n'est pas détaillé et donc des points sont sujet à interprétation.
C'est un langage complexe et qui pourrait encore, être ammener à évoluer. Les propositions et les suggestions sont les bienvenues.

Heurseuement, une grammaire concrète avait déjà été écrite en Lark par l'ancien élève Stéphane Selim.
Cette grammaire se trouve sur \href{https://github.com/uon-language/uon-parser/tree/master/grammar}{ici}

Cette grammaire a été convertie en format ANTLR. Des modifications ont été apportées et seront expliquées par la suite.

Les fonctionnalités se baseront sur celle-ci. Dans le but de rester cohérents entre elles.

La grammaire finale est écrite dans le fichier UON.g4.

\section{Résumé}
Pour commencer, il est nécessaire de préciser ce que permet cette grammaire initialement.

Ci-dessous se trouveront des exemples de ce qu'il est possible de faire.
Les points concernant une adaption particulière ou des modifications seront expliqué à la section \ref{modification}.

\textbf{Remmarque} : La grammaire qui a été adaptée dans le format d'ANTLR sera utilisé pour illustrer nos exemples.

\subsection{Éléments racines}

Un fichier UON peut contenir comme élément racine trois structures différentes : les collections en json et yaml, ainsi qu'un schéma.

\begin{listing}[!ht]
    \begin{minted}{antlr}
    uon: root_value;

    root_value: json_collection | yaml_collection | schema;

    json_collection: json_map | json_seq;

    yaml_collection: yaml_map | yaml_seq;
    \end{minted}
    \caption{Valeur racine de la grammaire UON}
    \label{uon-root-value}
\end{listing}

\subsection{Collections}
La grammaire actuelle nous permet de définir des collections. Ces collections sont des mapping et séquences soit dans un format proche de json ou de yaml.

UON est par nature proche du format JSON. Mais UON étant "YAML compliant" à certain degré, mon prédécesseur avait jugé utile d'incorporer ce format
dans le langage.

Ces collections peuvent être imbriqué seulement par format, on ne peut pas avoir dans un fichier une collection en format json et yaml en même temps.

% TODO : exemple de map
\begin{lstlisting}[frame=single,caption={Exemple d'un mapping en format JSON},captionpos=b,label={json-map}]
    {
        "name" ( description : a12 ) :  !str(optional: False) paul,
        age: !float !float128 !float32 +12.2 C,
        hobbys ( description : "what i like the most" ) : !seq [
            "cinema", "sport", "etc..."],
        license  :  !bool  true,
        car:   !bool false,
        sensor ( optional : false , description : ok ) : !map {
            type: "temperature",
            parameters1: !seq [ 1, 2,3,4,5]}
    }
\end{lstlisting}

\begin{lstlisting}[frame=single,caption={Exemple d'un mapping en format YAML},captionpos=b,label={yaml-map}]
ok :
    -  ok
    - !float32 12
    -
         - ok
ok2 : -23
\end{lstlisting}

% TODO : exemple de seq
\begin{lstlisting}[frame=single,caption={Exemple d'une séquence en format JSON},captionpos=b,label={json-seq}]
\dots
\end{lstlisting}


\begin{lstlisting}[frame=single,caption={Exemple d'une séquence en format YAML},captionpos=b,label={yaml-seq}]
\dots
\end{lstlisting}

\subsection{Schéma}
On peut définir également un schéma de validation.

Ce schéma doit se trouver dans un autre document que les deux structures ci-dessus.
Cette séparation permet d'éviter des ambiguïtés. On incite comme ça à l'utilisateur à séparer la logique entre différents fichiers.

\begin{lstlisting}[frame=single,caption={Exemple d'un schéma de validation},captionpos=b,label={yaml-seq}]
    \dots
\end{lstlisting}

\subsection{Types}
Les types supportés sont les suivants :
% TODO

\subsection{Propriétés}

Les propriétés en UON sont décrit dans la section \ref{properties}.

% TODO
Les clés des mapping des propriétés peuvent posséder des propriétés de présentation.
Les types dans un schéma de validation peuvent posséder des propriétés de validation.

\section{Modification}\label{modification}

UON est un langage comprenant plusieurs composantes est particularités qui ne peuvent pas toutes être traité dans un simple éditeur de texte.

Nous allons garder presque la même structure, car cette grammaire regroupe déjà des éléments importants du langage UON.

Cependant quelques points concernant la structure générale ont été modifiés.

Pour commencer il a fallu convertir la grammaire dans à la main car aucun convertisseur n'existe. Heursement ANTLR et Lark se basent sur une notation EBNF.
Ce qui fait que la structure de la grammaire reste relativement semblable.

Le seul point de divergence a été sur l'utilisation d'expressions régulières car la syntaxe de celle-ci varie légèrement. Et sur la gestion de l'indentation.

Concernant la structure d'un fichier de grammaire ANTLR .g4 il y a quelques subtilités à connaitre :

\begin{itemize}
    \item Le fichier doit comporter le même nom que la règle de départ.
    \item Les règles de lexer définissent comment une chaine de texte doit être tokenifier. Elles doivent être écrites en majuscule.
    \item Les règles de parser définissent comment une suite de tokens sera intérprétée. Elles doivent être écrites en miniscule.
\end{itemize}

\subsection{YAML}

La grammaire permet deux notations différentes pour les collections : une proche de json et l'autre de yaml.

En JSON les retours à la ligne et indentations ne sont pas nécessaires à la compréhension du langage, contrairement à YAML.
Le chapitre suivant explique ce qui a été fait pour pouvoir gérer ce problème.

\subsubsection{L'indentation}

Contrairement au parser lark, la gestion des indentations n'est pas gérée automatiquement à l'aide d'une librairie externe.
Ce mécanisme doit malheureusement être traité par nos soins.

Mais avant de résoudre ce problème, il est nécessaire de rappeler que notre grammaire n'est pas uniforme concernant la gestion des retours à la ligne.
Et c'est là où se trouve la vraie problématique. En effet, UON peut être écrit sous une forme proche de JSON ou de YAML. Le premier ne tient pas compte des indentations contrairement à la seconde.
Et la gestion des indentations nécessite de prendre en compte les retours à la ligne.

Il y a donc trois solutions qui ont été envisagées :

La première est de rajouter des tokens de retour ou il serait jugé bon de les avoir dans la grammaire, et de les rendres optionnels.
Mais ce n'est pas très propre et on risque d'en placer énormément.

Le deuxième serait d'ignorer tout simplement les erreurs.  C'est-à-dire que si on à un token de retour à la ligne qui ne devrait pas être là, alors l'ignorer tout simplement.
On pourrait penser ce mécanisme simple se révélerait efficace. Cependant cela pose problème pour le moteur de complétion qui a besoin d'avoir un input correct, comme expliqué dans la section \ref{error handle}

La troisième solution trouvée et celle actuellement utilisée. Elle consiste à prendre par défaut les retours à la ligne et de les ignorers dès qu'on détecte les tokens
qui ne sont jamais utilisé dans un format YAML (Les accolades et virgules).

Dès lors que les retours à la ligne existent, nous pouvons maintenant placer les tokens INDENT et DEDENT depuis le lexer
pour pouvoir gérer les imbrications correctement.

Pour ce faire, à chaque fois qu'un espace qu'un retour à la ligne est détecté par le lexer, nous prenons compte de la taille de l'espacement qu'il contient ("\\r\\n    ")
Cet espacement nous permettra de savoir si l'on doit rajouter des INDENT ou DEDENT.

Il est possible de placer le code permettant ce mécanisme dans le fichier grammaire, en l'écrivant dans le block \emph{lexer:@member{}}.
Lors de la génération, le code est placé dans le lexer. Cependant lors de la génération les imports seront manquants, il est nécessaire d'aller les rajouter manuellement.

Le code permettant est une adaptation d'une solution déjà existance pour du YAML dans une grammaire ANTLR, elle se trouve \href{https://github.com/umaranis/FastYaml}{ici}

Il est maintenant possible d'avoir par exemple la règle suivante :

\begin{listing}[!ht]
    \begin{minted}{antlr}
        yaml_seq : INDENT (SEQUENCE_TYPE)? seq_item+ DEDENT;
    \end{minted}
    \caption{Règle de grammaire gérant les indentations}
    \label{ex-indentation-rule}
\end{listing}

Les tokens INDENT et DEDENT seront créés par le lexer si nécessaire.

\subsubsection{Minus}

\begin{listing}[!ht]
    \begin{minted}{antlr}
    NUMBER:
        (('+'|'-')? INT ('.' [0-9]*)? EXP?) // +1.e2, 1234, 1234.5
        | '.' [0-9]+ EXP? // -.2e3
        | (('+'|'-')? '0' [xX] HEX+) // 0x12345678
        | (('+'|'-')? '0' [oO] HEX+) ; // 0o12345678

    NUMERIC_LITERAL: 'inf' | 'nan' | '-inf' | '-nan' | '+inf' | '+nan';
    \end{minted}
    \caption{Règle de grammaire concernant les nombres}
    \label{number-rule}
\end{listing}

Le caractère "-" étant utilisé pour définir une séquence en format yaml.
Il a fallu forcer le caractère directement dans l'expression régulière définissant un nombre.

\textbf{Remarque} : cette solution n'est pas optimale, car si on modifie la grammaire il faudra modifier ce code.

\subsection{String}

\begin{listing}[!ht]
    \begin{minted}{antlr}
        string :
        literal
        | QUOTED_STRING
        | UNQUOTED_STRING;
    \end{minted}
    \caption{Règle de grammaire concernant les strings}
    \label{string-rule}
\end{listing}

Une valeur textuelle peut soit être entre guillemets ou non.

C'est donc aussi le cas pour la clé d'une map.

Cela se rapproche par exemple, un peu plus de ce que fait \href{https://json5.org/}{JSON5} par exemple.
Aussi cela permet d'avoir des clés qui peuvent contenir exactement ce que l'on veut.
Car encapsuler entre des caractères spécifiques, il n'y a pas d'ambiguïtés possibles. Alors que sans on est contraint de limiter la forme de la clé.

Par contre un cas spécial a dû être traité. Si l'on veut pouvoir utiliser directement un mot sans guillemet, mais qu'il existe dans la grammaire.
Il faut ajouter ce cas, c'est pourquoi nous avons la règle de parser "literal" pour pouvoir utiliser un de ces tokens comme string.

\begin{lstlisting}[frame=single]
    {
        temperature : 10 C
    }
\end{lstlisting}

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/literal-key-uon.PNG}}
    \end{center}
    \caption[Utilisation d'un token de type literal comme clé]{\label{literal-key-uon}Utilisation d'un token de type literal comme clé}
\end{figure}

Sur le schéma \ref{literal-key-uon} On voit que la clé et bien un token de type \emph{literal}.
Ce token \emph{temperature} existe comme token car c'est une valeur utilisée comme quantité, mais si on ne précise pas explicitement que l'on souhaite avoir cette valeur
comme possible clé alors on aura une erreur si on ne le précise pas \dots % TODO

\subsubsection{Unquoted}
%% TODO

\begin{lstlisting}[frame=single,caption={Règle concernant les strings sans guillement},label={unquoted}]
    UNQUOTED_STRING: IDENTIFIER+;

    IDENTIFIER
        : [\p{L}]    // matches a single code point in the category "letter"
        | [\p{M}]    // a character intended to be combined with another character (e.g. accents, umlauts, enclosing boxes, etc.)
        | [\p{N}]    // matches any kind of numeric character in any script.
        | [\p{Pc}]   //  punctuation character such as an underscore that connects words.
        | '!';
\end{lstlisting}

%REGEX
% utilisation d'outils en ligne : ex : https://regex101.com/
% référence officielle : https://www.regular-expressions.info/
% https://www.regular-expressions.info/anchors.html

\subsubsection{Quoted}

%% TODO
\begin{lstlisting}[frame=single,caption={Règle concernant les strings avec guillement},label={quoted}]
    QUOTED_STRING:
    '"' DOUBLE_QUOTE_CHAR* '"'
    | '"''"''"' DOUBLE_QUOTE_CHAR* '"''"''"'
    | '\'' SINGLE_QUOTE_CHAR* '\'';

    fragment DOUBLE_QUOTE_CHAR: ~["\\\r\n];

    fragment MULTILINE_QUOTE_CHAR: ~[];

    fragment SINGLE_QUOTE_CHAR: ~['\\\r\n];
\end{lstlisting}

N'importe quelle caractère peut se trouver

%% TODO : Multiline ? ne pas tolérer les espaces dans la clé mais dans le texte

\subsection{Propriétés, unités et nombres}

Des propriétés de présentation ont été rajoutées en reprenant les constructions utilisées pour définir des propriétés pour les clés ainsi que de validation.

% "_" dans le caption cause des erreurs !!!
\begin{lstlisting}[frame=single,caption={Règle concernant les propriétés de présentation des types},label={types_properties}]
    types_properties: OPEN_PAR (types_propertie (COMMA types_propertie)*)? CLOSE_PAR;
    types_propertie: comment | description | optional;
    comment: COMMENT COLON string;

    \dots

    string_scalar: (STR_TYPE (types_properties)?)? string;
\end{lstlisting}

number, bolean, null, string(literal, unquoted et quoted),
literal : tous les token utilisé déjà ailleurs mais qu'on voudrait avoir dans les clé par exmeple...

typs : string, nombre, boolean,

quantité et unités


\section{Debugging}

Il n'est pas simple de debugger une grammaire sans aucun outil ou seulement à l'aide d'un terminal, cela peut même se révéler extrêmement difficile.
Heursement pour nous, il existe une \href{https://plugins.jetbrains.com/plugin/7358-antlr-v4}{extension} sur intellij qui nous permet de visualiser en temps réel, le parse tree du texte que l'on est en train de saisir.

% TODO : Figure d'intelj en action

% Les figures représentants les arbres se baseront sur ce plugin ...


\chapter{Implémentation}
Dans ce chapitre, nous allons explorer les aspects techniques concernant l'implémentation de fonctionnalités. Nous allons voir également plus en détail les outils utilisés ainsi que le déploiement de l'extension sur le marketplace.
Ce travail reflète uniquement mon approche personnelle sur la matière. Et ne devrait pas être considérée comme l'unique manière de procéder.

%TODO
%\section{Tolérance}
%Le parseur ne devrait uniquement controler la syntaxe. une bonne règle à se rappeler et que dans le doute il est préférable de laisser l'analyser syntaxique (parser) 
%Le document et controler la sémantique pour s'assurer que la règle a du sens

%"The parser should only check the syntax. So the rule of thumb is that when in doubt you 
%let the parser pass the content up to your program. Then, in your program, you check the semantics and make sure that the rule actually have a proper meaning."
% Citation !!! mégathread ?


\section{Code}

Par rapport à la figure %todo
les points suivants au été rajouté à la figure %todo

\textbf{Remarque} : Les "..." signifie qu'il y a des sous fichiers et/ou répertoire mais ont été ignoré pous ne pas surcharger
la figure.

\begin{figure}[!h]
    \centering
    \framebox[\textwidth]{%
        \begin{minipage}{0.9\textwidth}
            \dirtree{%
                .1 ..
                .2 github.
                .3 workflows.
                .4 pipeline.yml.
                .2 vscode.
                .3 launch.json.
                .3 tasks.json.
                .2 .gitignore.
                .2 README.md.
                .2 src.
                .3 completion.
                .4 completion.ts.
                .3 error.
                .4 ErrorListener.ts.
                .4 UonCompletionErrorStrategy.ts.
                .3 generated.
                .4 ....
                .3 grammar.
                .4 .antlr.
                .4 UON.g4.
                .3 outline.
                .4 UonASTVisitor.ts.
                .4 UONDocumentSymbolProvider.ts.
                .3 test.
                .4 ....
                .3 extension.ts.
                .2 tsconfig.json.
            }
        \end{minipage}
    }
    \caption{Structure du projet}
    \label{project-structure}
\end{figure}

Le code de l'implémentation est disponible sur le dépôt public \href{https://github.com/vitorva/vscode-uon}{suivant}.

\textbf{Remarque} : L'implémentation détaillée dans ce rapport pourrait ne plus représenter l'état du dépôt, car ce projet est sujet à évoluer.

\section{Extension}
Nous allons voir maintenant, l'essentiel de la procédure pour créer et publier son extension.

La procédure détaillée se trouve \href{https://code.visualstudio.com/api/get-started/your-first-extension
}{ici}

\subsection{Génération de l'extension}

VS Code permet de générer un squelette (boilerplate) pour la réalisation d'une extension.
Pour ce faire il faut avoir Node.js et Git installés.

Ceci dans le but d'installer Yeoman et VS Code Extension Generator.

\href{https://yeoman.io/}{Yeoman} est un outil permettant de créer un générateur et de l'exécuter dans un environnement adapté.
\href{https://www.npmjs.com/package/generator-code}{VS Code Extension Generator} et le générateur implémenté par VS Code pour générer la structure de l'extension.

La commande pour l'installation est la suivante :

% ,caption={generator-code},label={generator-code}
\begin{lstlisting}[frame=single]
npm install -g yo generator-code
\end{lstlisting}

Puis il suffit de lancer le générateur :

\begin{lstlisting}[frame=single]
yo code
\end{lstlisting}

Et de compléter ce qui est attendu dans le terminal pour créer notre extension.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=8cm]{assets/figures/yo-code.png}
    \end{center}
    \caption[Générateur Yo Code]{\label{yo-code}Générateur Yo Code}
\end{figure}

Cela nous créera l'arborescence mentionnée précédemment au point \hyperref[Extension File Structure]{Extension File Structure}.

\subsection{Lancement de l'extension}
On peut lancer notre application en mode debug avec la touche F5. Cela va ouvrir une nouvelle fenêtre VS Code qui va contenir notre extension.

Par défaut, l'application générée vient avec du code permettant d'afficher un message de bienvenu. On l'affiche au travers du command palette. On peut s'assurer comme cela que l'implémentation de base fonctionne correctement.
On constate donc que par défaut, l'application doit être lancée au travers d'une commande, mais pour notre cas il sera plus intéressant que VS Code nous propose les fonctionnalités à l'ouverture d'un fichier UON.
Pour cela, il faut modifier \emph{l'activation events} dans le fichier package.json par :
\begin{lstlisting}[frame=single]
    "activateEvents" : [
	"onLanguage:uon"
    ]
\end{lstlisting}

\subsection{Déploiement sur le Marketplace}

% https://code.visualstudio.com/api/working-with-extensions/publishing-extension
% https://code.visualstudio.com/api/working-with-extensions/continuous-integration
% https://marketplace.visualstudio.com/manage/publishers/test2publish?noPrompt=true
% https://dev.azure.com/Stev03/_usersSettings/tokens

% delete publisher : https://stackoverflow.com/questions/54179509/how-do-you-delete-a-visual-studio-marketplace-publisher

% https://github.com/Microsoft/vscode-vsce/issues/249
% https://docs.npmjs.com/downloading-and-installing-packages-globally
% https://github.com/clangd/clangd/issues/159
% https://github.com/microsoft/vscode-vsce/issues/238

\textbf{Prérequis} : Posséder un compte Azure.

Il faut d'abord récupérer un Personal Access Token. Pour ce faire, il faut créer une \href{https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/create-organization?view=azure-devops}{organisation}.
Puis, créer un access token avec ces paramètres :
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/access-token.png}}
    \end{center}
    \caption[Access Token]{\label{access-token}Access Token}
\end{figure}

Ce token va nous permettre publier notre extension sur un Publisher. Celui-ci nous permettra de gérer notre extension en ligne.

Pour créer un publisher, il faut se connecter avec le même compte qui a créé l'organisation et le token ci-dessus via la \href{https://marketplace.visualstudio.com/manage/publishers/}{management page}.

Quand cela est fait, il faut ensuite ajouter la propriété "publisher" et saisir l'id de celui-ci dans le package.json de notre extension.

Puis finalement, il suffit dans un terminal de saisir les commandes suivantes :
\begin{lstlisting}[frame=single]
    vsce login [le nom du publisher]
    Vsce publish
\end{lstlisting}

\textbf{Remarque} : Attention au terminal utilisé. Dans mon cas, le terminal de node (git.bash) sur windows ne fonctionne pas lorsque des interactions avec le terminal sont nécessaires.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=12cm]{assets/figures/manage-publisher.png}}
    \end{center}
    \caption[Interface web pour manager l'extension]{\label{manage-publisher} Interface web pour manager l'extension}
\end{figure}

Il est ensuite facile de manager l'extension au travers de la page web.
L'extension apparaitra automatiquement dans le marketplace si elle est publiée.

\subsection{Intégration continue (CI)}

Le squelette de base de l'extension ayant été créé en utilisant le générateur Yeoman de VS Code, la structure pour écrire nos tests d'intégrations
existe déjà \cite{testing-extension}.

Il suffit de lancer la commande "npm run test" ou "yarn test" pour lancer tous les tests se trouvant dans le répertoire \emph{test} (voir figure \ref{project-structure}).

C'est l'API \href{https://mochajs.org/api/}{mocha} qui est utilisé pour exécuter ces tests.

Actuellement on teste de cette façon les fonctionnalités de la complétion et de l'outline view.

Concernant les tests pour la coloration syntaxique, le module \href{https://www.npmjs.com/package/vscode-tmgrammar-test?activeTab}{vscode-tmgrammar-test} a été choisi.
Car depuis mocha, on a pas moyen de le faire.

Quelques tests unitaires simples ont été implémenté, mais qui permet toutefois de s'assurer qu'un changement ne vient pas modifier le comportement d'une manière inattendue.

\subsection{Deployement automatisé (CD)}

% TODO :  incréementer le numéro de verison !

L'extension est actuellement automatiquement déployée lorsqu'une release sur la branche main est réalisée.
L'étape de mise en place est détaillée par VS Code à cet \href{https://code.visualstudio.com/api/working-with-extensions/continuous-integration#github-actions}{emplacement}.

Il s'agit d'une étape supplémentaire dans la pipeline après que les tests soient effectués.

Elle consiste à exécuter la commande vsce publish depuis notre package.json pour publier l'extension.

\href{https://www.npmjs.com/package/vsce}{vsce} est le gestionnaire d'extension de VS Code.

Il est nécessaire d'ajouter le token (PAT) comme secret dans notre dépôt github. %todo ref

\textbf{Remarque } : On ne peut déployer qu'une extension avec une version supérieure que la précédente. % TODO : automatisé ça ?

\section{Génération du parser ANTLR}
\subsubsection{Grammaire}

Quand le fichier de grammaire est écrit, il suffit de lancer le code suivant :

\begin{lstlisting}[frame=single]
    antlr4ts -visitor path/to/MyGrammar.g4
\end{lstlisting}

Il nous génère ces fichiers :
\begin{itemize}
    \item UON.interp
    \item UON.tokens
    \item UONLexer.interp
    \item UONLexer.tokens
    \item UONLexer.ts
    \item UONParser.ts
\end{itemize}

\section{Syntax highlighting}

% Pour signaler à l'utilisateur que la saisie est tout de même incorrecte. On peut s'aider en parallèle de la coloration syntaxique.
% TODO : https://macromates.com/manual/en/language_grammars

% Dark + (default dark)

La coloration syntaxique permet que chaque élément du texte soit affiché dans l'éditeur avec une coloration et un style en fonction de son type.
Il y a deux composantes principales.
\begin{itemize}
    \item La tokenisation qui consiste à séparer le texte en une liste de token.
    \item La thémisation (Theming) qui permet d'attribuer à un token une couleur et un style.
\end{itemize}

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/example-uon-coloration.png}
    \end{center}
    \caption[Exemple de coloration d'un code UON]{\label{example-uon-coloration} Exemple de coloration d'un code UON}
\end{figure}

VS code utilise Textmate grammars comme le moteur de tokénisation de la syntaxe.
Cette grammaire a été inventée par l'éditeur TextMate est a été adopté par de nombreux éditeurs et IDE.
Elle contient une liste structurée d'expressions régulières qui permet d'associer un scope
à un ou plusieurs tokens qui déclencherait l'activation d'une de ces expressions. \cite{syntax-highlight-guide}

La grammaire TextMate du langage UON se trouve dans le fichier "uon.tmLanguage.json" dans le dossier "syntaxes" à la racine du projet.
Ce fichier doit être référencé à travers du point de contribution "grammar" dans le package.json.

\subsection{Scope}
Un scope défini le contexte d'un groupe de token et peut-être considéré comme son type.

Il s'agit d'un identifiant, c'est grâce au scope d'un token, qu'un fichier de thème peut attribuer une couleur avec un mécanisme de "clé-valeur".

TextMate défnit des convention de nommage que de nombreux thèmes ciblent déjà et recommande de les utiliser.
Ceci dans le but de permettre à notre langage d'être supporté le plus possible. \cite{textmate-grammars}

Pour respecter au maximum les conventions attendues et s'assurer qu'une coloration automatique soit appliquée. La stratégie suivante a été adoptée : partir depuis un fichier de thème déjà créé (Thème dark par défaut sur VS Code). 
Et de réutiliser les scopes définis dans ce fichier.

Ce fichier se trouve sur le dépôt \href{https://github.com/microsoft/vscode/blob/main/extensions/theme-defaults/themes/dark_vs.json}{suivant}.
Mais peut être également généré depuis le command palette.

Ce procédé nous permet de profiter des thèmes de VS Code implémenté par défaut dans l'éditeur. Par exemple, illustrée par les figures \ref{vscode-dark-theme} et \ref{vscode-kimbi-dark-theme}.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/vscode-dark-theme.PNG}
    \end{center}
    \caption[Thème vscode dark]{\label{vscode-dark-theme} vscode-dark-theme}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/vscode-kimbi-dark-theme.PNG}
    \end{center}
    \caption[Thème vscode kimbi dark]{\label{vscode-kimbi-dark-theme} vscode kimbi dark theme}
\end{figure}

\subsubsection{Example}
Prenons un exemple simple du fichier "uon.tmLanguage.json", le scope pour les nombres : "constant.numeric.uon"

La convention est d'avoir des mots clés séparés par un point pour spécifier chaque niveau de précision supplémentaire
Il est aussi possible d'avoir plusieurs scope pour un élément : "string.uon support.type.property-name.uon". Ces scopes sont séparés par un espace.
C'est celui le plus à droite est prioritaire. Cela peut-être utile si l'on souhaite maximiser les chances qu'un thème nous fournisse une coloration.
\cite{scopes-selectors}

Les scopes sont donc souvent imbriqués et c'est le plus spécifique qui est utilisé pour le choix du thème.
Il est possible d'analyser notre code à l'aide du "Scope Inspector", qui est un outil disponible dans l'éditeur depuis la command palette de VS Code.
pour visualiser la hiérarchie des scopes pour un token.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/scope-inspector.png}
    \end{center}
    \caption[Scope inspector]{\label{basic-uon} Le scope inspector sur un exemple de code en UON}
\end{figure}

La section "textmate scopes" montre la liste des scopes utilisés pour le token qu'on séléctionne. Le scope le plus spécialisé se trouve au sommet.

La coloration syntaxique n'est pas considérée comme un "programmatic language feature" et donc n'est pas gérée par la spécification LSP et par l'API de VS Code non plus.
Une des raisons évoquées et que ce mécanisme doit être le plus rapide possible. Et que le faire directement depuis le client garantit une latence faible
\cite{syntax-highlighting-editor}.

\subsection{Fonctionnement}

La figure \ref{Textmate grammar} illustre un exemple très simplifié utilisé du fichier "uon.tmLanguage.json".

\begin{listing}[!ht]
    \begin{minted}{json}
       1. {
       2.   "name": "UON",
       3.   "scopeName": "source.uon",
       4.   "patterns": [
       5.       {
       6.           "include": "#root_value"
       7.       },
       8.       {
       9.           "include": "#comments"
      10.       }
      11.    ],
      12.    "repository": {
      13.       "unquotedObjectKey": {
      14.           "match": "[\\p{L}]([\\p{L}]|[\\p{N}]|[\\p{Pc}])*",
      15.            name": "entity.name.tag.unquotedObjectKey.uon"
      16.        },
      17.        "comments": {
      18.           "begin": "#",
      19.           "beginCaptures": {
      20.               "0": {
      21.                       "name": "punctuation.definition.comment.begin.uon"
      22.                }
      23.            },
      24.            "end": "\\n",
      25.            "endCaptures": {
      26.               "0": {
      27.                       "name": "punctuation.definition.comment.end.uon"
      28.               }
      29.             },
      30.            "name": "comment.line.number-sign.uon"
      31.            // "patterns": [ ... ]
      32.        }
      33.    }
      34. }
    \end{minted}
    \caption{Exemple simple d'une grammaire Texmate}
    \label{Textmate grammar}
\end{listing}

Dans cette exemple, la coloration n'est appliqué que sur les commentraires et les clés sans guillemets du langage UON.
Tout n'est pas couvert dans cette exemple, mais cela permet de bien comprendre le mécanisme général utilisé.

Les points importants sont les suivants :

\textbf{"scopeName"} (ligne 3) : Scope racine du fichier. C'est le scope de base appliqué sur chaque élément.

\textbf{"patterns"} (ligne 4) :
C'est un tableau contenant les règles actuelles utilisées pour parser le document en sein du scope courrant.
Elles sont appliquées dans l'ordre.
Ces règles peuvent être directement écrites à cet emplacement, mais il est aussi possible de les définir dans un objet devant être nommé "repository". Puis de les inclure, comme fait actuellement.

\textbf{"repository"} (ligne 12) : Un dictionnaire (clé-valeur) de règles.

\textbf{"name"} : le nom d'un scope qui sera attribuer.

On constate deux constructions possibles.
La première règle applique le scope à un élément qui déclenche directement l'expression régulière.

La deuxième définit un délimiteur de début et de fin.
On peut "capturer" des parties et leur attribuer des scopes encore plus spécifique.
Il est aussi possible de rajouter un objet "patterns" pour appliquer des règles plus spécifique à l'intérieur de ces délimiteurs.

\subsubsection{Propriétés}

La coloration syntaxique doit refléter au mieux cette grammaire. Il y a donc une coloration spécifique pour les propriétés appartenant à un type.
Cela rend la coloration syntaxique fortement liée à la grammaire, mais on se rend compte d'un coup d'oeil si la propriété et correcte.

\subsection{Capture}

% exemple : !str(comment : "mon premier commentaire")

Il y a globalement deux manières pour appliquer un scope.

La première et de définir un caractère de départ et un autre de fin. Puis définir les patterns que l'on appliquera aux éléments se trouvant à l'intérieur.

Mais dans certains cas, il n'y a pas de délimiteur et on voudrait appliquer une coloration différente aux éléments qui composent le groupe de caractère qu'une expression régulière
a déclenché.

Il s'agit d'un nombre mais l'option "Capture" nous permet d'appliquer un scope au groupe de sélection.
Il n'est pas facile pour des expressions assez complexes de trouver ce nombre mais on peut s'aider du site \href{https://regex101.com/}{suivant}

\subsection{Semantic Highlight}

Un point important et qu'il ne faut pas confondre la coloration syntaxique avec ce que l'on nomme la coloration sémantique. Car il s'agit d'une couche que l'éditeur rajoute sur la précédente pour l'améliorer si nécessaire.
La tokénisation sémantique permet de fournir des informations supplémentaires sur les tokens en se basant sur une compréhension profonde du langage.
De manière à résoudre les symboles dans le contexte d'un projet.

Il s'agit donc d'obtenir des informations contextualisées afin fournir une coloration plus précise pour un token.

Par exemple avec du code Typescript :

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=15cm]{assets/figures/semantic-coloration-without.png}
    \end{center}
    \caption[Exemple sans coloration sémantique]{\label{semantic-coloration-without} Sans coloration sémantique}
\end{figure}

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=15cm]{assets/figures/semantic-coloration-with.png}
    \end{center}
    \caption[Exemple avec coloration sémantique ]{\label{semantic-coloration-with} Avec coloration sémantique }
\end{figure}

On voit à la ligne 10 que la colorisation de la fonction a changé et correspond maintenant à un paramètre d'une fonction.
Cela est donc intéressant pour des langages plus complexes dans leur nature qu'un langage de sérialisation. Une coloration syntaxe suffit amplement dans notre cas.
Car se baser sur la structure du document nous donne déjà toutes les informations nécessaires.

\section{Auto-completion}

Cette fonctionnalité fait parti de ce qu'on appelle plus communément "IntelliSense" qui un terme général désignant diverses fonctions d'édition de code \cite{intelliSense}.
Le terme "auto-complétion" existe également sous d'autres appellations (ex : autocomplétion, complétion automatique, complétion, etc.).

L'intérêt est de proposer des suggestions de saisie au fur est à mesure qu'un utilisateur écrit une chaine de caractères.
Ces suggestions sont des compléments qui pourraient convenir à la chaine de caractère saisit. Cela va donc dépendre principalement du langage traité.
Car en JSON ou en YAML, l'auto complétion n'existe pas vraiment dû à la syntaxe même de ces langages de sérialisation.
Il n'y a pas de mots clés comme on pourrait en trouver dans un langage de programmation.
Il n'aurait donc rien de base à proposer pour ces deux langages, sans utiliser une source d'infomation externe comme les "JSON-schéma".
À la limite, seules les clés ou valeurs précédemment saisies peuvent éventuellement être reproposées mais cela est fait par défaut sous VS Code pour n'importe quel type de langage
avec le type de complétion nommé "word based completion" \cite{word-based-completions}.

C'est pourquoi lorsqu'il faut implémenter une telle fonctionnalités, il faut savoir quoi proposer et quand le faire.
Pour cela, Il est nécessaire d'analyser le langage UON et de voir ce qu'il est pertinent de proposer comme suggestion de complétion.
La syntaxe UON peut être exprimée par la figure \ref{syntax}.

\begin{figure}[!ht]
    \begin{center}
        \frame{\includegraphics[width=12cm]{assets/figures/syntax.png}}
    \end{center}
    \caption[Règle de syntaxe en UON]{\label{syntax} UON syntax}
\end{figure}

On constate que : toute valeur est un type, et un type peut avoir des propriétés associées.

Les suggestions se baseront donc sur les types et leurs propriétés.

\subsection{antlr4-c3}

Une approche typique pour implémenter de la complétion est d'utiliser un parser du langage.
Mais en complément, nous allons aussi utilisé \href{https://github.com/mike-lischke/antlr4-c3}{"antlr4-c3"} (également simplement nommé c3).
Il s'agit d'un moteur de complétion pour les parsers basés sur ANTL4. Le moteur c3 est capable de fournir des candidats de complétions.
Les suggestions proposées dans l'éditeur sera majoritairement des candidats fournis par le moteur. C'est donc un outil relativement puissant, qui simplifie grandement la tâche du développeur.

Il a été conçue par \href{https://github.com/mike-lischke}{Mike Lischke}. Cet outil est écrit en Typescript et est disponible comme node module. Il existe aussi en version \href{https://github.com/mike-lischke/antlr4-c3/tree/master/ports/java}{JAVA} et \href{https://github.com/mike-lischke/antlr4-c3/tree/master/ports/c%23}{C\#}.

Le moteur de complétion n'utilise pas le parse tree pour proposer des candidats. Mais l'exécution du parser est nécessaire pour remplir le token stream comme illustré précédement par la figure \ref{Utilisation du parser d'ANTLR}.
Car le moteur s'appuyera sur ce flux de tokens et également du mécanisme de machine à état d'ANTLR4 disponible dans le parser. \cite{antlr-mega-tutorial}

Pourquoi un tel moteur ?
Car connaitre tous les types de tokens qu'un utilisateur serait amené à saisir ne suffit pas. Pour que la complétion soit réellement intéressante, nous voudrions connaitre quelle règle de la grammaire pourrait être valide à une position donnée.
Et cet outil apporte une solution pratique est élégante pour résoudre ce cas.

\subsection{Candidats}\label{candidates}

Pour recevoir des candidats, il suffit d'avoir un environnement comme présenté par la figure \ref{Utilisation du parser d'ANTLR}, d'installer le module npm \emph{antrl4ts} et
de donner au moteur, une instance du parser et un indice.

\begin{listing}[!ht]
    \begin{minted}{typescript}
    let core = new c3.CodeCompletionCore(parser);
    let candidates = core.collectCandidates(index);
    \end{minted}
    \caption{Initialisation du moteur de complétion c3}
    \label{c3-setup}
\end{listing}

Un candidat est un token qui pourrait se situer à une position donnée dans l'input.
Cette position correspond à un indice donné au moteur.

\subsubsection{Indexation}
L'indexation est gérée de deux manières différentes comme illustré par la figure \ref{candidat-index}, tiré de la documentation de c3.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/candidat-index.png}}
    \end{center}
    \caption[Indexation des candidats]{\label{candidat-index} Indexation des candidats}
\end{figure}

Cela va dépendre si on décide d'ignorer ou non les espaces depuis la grammaire g4.

Ce qui est fait actuellement c'est de récupérer ces tokens dans un canal mais sans le considérer dans le langage.
C'est ce qui est recommandé dans la documentation du projet c3. Car cela permet de gérer plus de cas particuliers \cite{c3-example-soql}.
% https://neuroning.com/post/implementing-code-completion-for-vscode-with-antlr/

Même si l'on n'en profite pas actuellement, cela permet l'extensibilité du code par la suite. % TODO : à vérifier

% La documentation du projet indique bien que ce n'est pas un problème trivial à résoudre.

\subsubsection{Trouver l'indice}

L'indice devrait correspondre à l'emplacement du curseur dans l'input.

À priori, il serait nécessaire de convertir la position du curseur dans un indice.
Mais cela est n'est pas réelement nécessaire, si on ne prend pas en compte tout le code.

La méthode actuellement appliquée et que l'on va extraire le code saisie jusqu'à la position du curseur.
C'est ce code qui sera l'input que l'on va donner au parser lorsque la complétion sera trigger par VS Code.

Il sera ensuite possible de déduire l'indice du prochain candidats en se basant sur la taille du tableau de tokens que le token stream contient.

%Ce traitement a été choisi car le code se trouvant après le curseur n'a pas d'impact sur la génération des candidats.
%Et que l'on pas dans la syntaxe de la grammaire actuellement, des cas à gérer concernant la position du curseur.

\subsection{CandidatesCollection}

Le résultat du moteur à l'appel de la fonction \emph{collectCandidates} est un objet de type \emph{CandidatesCollection}.

% TODO https://github.com/lark-parser/lark/issues/684 -> api ?
\begin{listing}[!ht]
    \begin{minted}{typescript}
    class CandidatesCollection {
        tokens: Map<number, TokenList>;
        rules: Map<number, RuleList>;
    }
    \end{minted}
    \caption{CandidatesCollection}
    \label{CandidatesCollection}
\end{listing}

Cette collection contient deux map : \emph{tokens} et \emph{rules}.

La map \emph{tokens} fait le lien entre le prochain token possible et une liste contenant des tokens qui doit le suivre immédiatement, si elle existe.

Nous allons principalement utiliser les clés de cette map pour proposer nos suggestions.
Cette clé est juste l'id d'un token. Cette id sera converti pour représenter la valeur du token qu'il représente et c'est ce qui sera affiché dans l'éditeur.
% TODO

La liste est aussi utilisé à une moindre mesure pour obtenir par exemple des suggestions suivantes : "descripton : "
On obtient deux tokens d'un coup "description" et ":"

Cependant cela nécessite que les tokens se suivent obligatoirement et sans alternatives possibles dans la grammaire.

La map \emph{rules} fait le lien entre la règle du parser actuelle et la liste de règles utilisé pour arriver à ce stade.
Cette map nous permet de questionner la liste des symboles pour retourner des informations supplémentaires.

\subsubsection{Symbol tables}\label{Symbol tables}

C'est un mécanisme de c3 mais qui ne sera pas exploité, car je n'ai pas trouvé d'usage dans le cadre du cahier des charges de ce projet et par manque de temps.
Cependant il reste intéressant de détailler brièvement son utilité pour d'éventuelles améliorations.

On pourrait penser qu'avoir à disposition un parser nous permettrait de récupérer toutes les informations nécessaires au travers d'un AST.
Cependant ce n'est pas le cas pour la plupart des scénarios.
Nous pouvons savoir quels sont les tokens valide à un emplacement (ce qui est suffisant pour nous). Mais pour certains langages, des informations manquent.
Et c'est ce que la table de symboles essaye de résoudre.

Une table de symboles est une structure qui contient des informations complémentaires sur des symboles, cela peut s'agir de noms, leur scopes et d'autres caractéristiques.
Cependant, un langage de sérialisation est relativement moins complexe à gérer qu'un langage de programmation.
Nous n'avons par exemple pas à mémoriser des variables et leur visibilité au sein du code.

Mais il est mentionné dans la spécification qu'un schéma pourrait être défini au même niveau que le payload.
Avec ce scénario en tête, une table de symboles pourrait se révéler utile. Car on pourrait se servir de ce schéma de validation qui spécifierait un nouveau type pour proposer de la complétion appropriée
lorsque l'on saisira ce type dans le payload. Typiquement des propriétés.

\subsection{Tokens ignorés}

Il est possible d'indiquer au moteur les tokens qu'il peut ignorer, pour ne pas les récupérer dans la liste des candidats fournis par le moteur.
Cela est nécessaire pour nous, car tous les tokens ne sont pas intéressant.

Les tokens peuvent être classé en deux catégories.

\begin{enumerate}
    \item Ceux dans leur valeur sont fixes.
    \item Ceux dans leur valeur dépendent de la saisie d'un utilisateur au travers d'expression régulière.
\end{enumerate}

Les tokens qui sont utiliser comme suggestions sont ceux issue de a première catégorie. Mais quelques tokens sont ignorés : comme les parenthèse, accolades car ce n'est pas vraiment pertinent. % et les deux-points
La seconde n'est pas intéressante, car le moteur ne fera que retourner le nom du token ce qui est inutile. Par exemple "NUMBER", lorsqu'il est possible d'avoir un nombre.

\subsection{Gestion des erreurs }\label{error handle}

On aurait pu penser naïvement que le mécanisme avancé de récupération d'erreur \emph{Parser Error Recovery Strategy} permettrait également de rendre celui de la complétion aussi plus tolérant.
Mais ce mécanisme n'est utilisé que pour générer un parse tree, cela est donc inutile pour ce cas. % car se base sur TODO

%Donc comme déjà mentionner, pour que le moteur puisse retourner des candidats, la syntaxe se trouvant avant la position du curseur doit être valide.
Cependant une contrainte pour que le moteur propose des candidats est qu'il est nécessaire que tout le code se trouvant avant le curseur soit correct.
Car sinon on pourrait avoir dans le stream de token des éléments qui ne seront pas compris par le moteur et l'on n'obtiendra pas de candidats.

Une explication fournit par le créateur du moteur dans la rubrique "issues" du dépot du \href{ https://github.com/mike-lischke/antlr4-c3/issues/29}{projet}.
% https://github.com/mike-lischke/antlr4-c3/issues/29

est que dès le moment où le moteur est confronté à une erreur de syntaxe (token inattendu ou invalide), il n'a plus moyen de trouver un chemin valide.
Et que parcourir toutes les possibilités jusqu'à trouver le token qui suit celui qui pose problème est trop long et peut mener à des erreurs d'ambiguïtés si le token suivant se trouve dans plusieurs règles de la grammaire.

Mais l'on pourrait s'en contenter, car finalement, il est logique qu'une erreur puisse entrainer un tel comportement.
Si dans un langage de programmation un utilisateur saisit du code erroné, il fort possible que la complétion cesse de fonctionner.

\subsubsection{Correction custom}
Cependant comme nous travaillons avec une syntaxe beaucoup moins complexe l'on pourrait modifier le texte depuis l'application pour corriger voir supprimer le problème.
Par exemple, dans le cas d'un mapping de clé valeur, une ligne pourrait être omis sans nuire à la compréhension du document.

C'est ce qui peut être fait de la manière suivante :
\begin{enumerate}
    \item Étape 1 : créer une nouvelle classe de listener qui détecte les erreurs lors du parsing.
    \item Étape 2 : stocker la position des tokens qui posent problème dans la classe du listener.
    \item Étape 3 : modifier le texte récupérer dans l'éditeur localement en conséquence.
    \item Étape 4 : relancer la phase de la création du parser en lui donnant le texte modifié.
\end{enumerate}

Mais cette approche nécessite de traiter énormément de cas particulier et mais le problème et que l'on couple la logique du moteur avec l'application et donc n'est pas une très bonne pratique.
Car si on modifie la grammaire, il faudra répercuter les changements dans l'application.
C'est pourquoi actuellement on ne supprime que la première ligne qui pose problème et qu'il n'a pas été jugé nécessaire d'aller plus loin.

\subsection{Snippets}
Il est possible d'ajouter des snippets sous VS Code. C'est-à-dire des templates qui nous permettent de générer du code prédéfini. Puis de les ajouter aux suggestions.
C'est utile si on a envie d'avoir des suggestions un peu plus personnalisées.
Typiquement on pourrait vouloir avoir comme suggestion directement un ensemble de propriétés comme illustré par la figure \ref*{snippet-suggestion}.
Ce qui n'est pas possible avec le moteur.

\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/snippet-suggestion.png}
    \end{center}
    \caption[Suggestion d'un snippet]{\label{snippet-suggestion} Suggestion d'un snippet}
\end{figure}

% TODO : exemple concret
\begin{figure}[!h]
    \begin{center}
        \includegraphics[width=10cm]{assets/figures/snippet-suggestion.png}
    \end{center}
    \caption[Suggestion d'un snippet]{\label{snippet-suggestion} Suggestion d'un snippet}
\end{figure}

Ce modèle de snippet permet d'afficher une chaine de caractères avec des case vides à remplir.
Il est possible de passer à la case suivante à l'aide de la touche \emph{tab}.

Cependant il est nécessaire de proposer ces suggestions au bon moment. La solution envisagée et de vérifier que le token du type possédant
ces propriétés ce trouve dans les candidats du moteur et qu'il ne s'agit pas de schéma

C'est un mécanisme utile est simple, mais fortement lié à l'application et il ne faudrait trop en utiliser.

\subsection{Triggers}
L'éditeur va trigger automatiquement l'autocomplétions en appelant le provider lorsque l'on saisit plusieurs caractères. Il aussi possible de l'activer manuellement en exécutant la commande "ctrl + space".
On peut aussi ajouter au provider des symboles qui vont trigger son activation. Les symboles espace " " et de retour à la ligne (\\rn) ont été définis comme triggers, car cela semble assez naturel dans ce langage.
Cependant il ne s'agit que d'une appréciation personnelle.

%\subsection{Pas de suggestion}
%TODO :
%No suggestion :
%quand un espace précède un nouveau mot alors c'est simple pour le placer
%quand il est colé c'est + compliqué. Il faut calculer la position du début et de fin du nouveau mot car VS Code peut trigger l'autocomplete n'importe quand et du coup
%avec du texte colé il peut pas savoir à partir de ou le texte en train d'etre ecrit est un mot du lnagage...mais utilité de faire ça ???

%TODO : quick-suggest-setting (par défaut ?) : https://github.com/microsoft/vscode/issues/101333}

\subsection{Mécanisme automatique}
VS Code va automatiquement filtrer les suggestions pendant la saisie à l'aide d'un algorithme de fuzzing search.
Pour rappel le fuzzy search (ou recherche approximative) permet de trouver une chaine de caractère en fournissant une chaine qui n'a besoin d'avoir une correspondance exacte
avec le texte que l'on cherche.

Par défaut la complétion proposée par VS Code porte sur les symboles déjà existants dans le code.
Ce type de complétion est nommé "word based completion" dans VS Code.

\section{Outline view}

La outline view est une représentation de la structure d'un fichier actuellement ouvert par l'éditeur. On appelle généralement cette structure le "document outlining" ou "code outlining".

En analysant la spécification \href{https://microsoft.github.io/language-server-protocol/specifications/lsp/3.17/specification/}{LSP} ou le \href{https://code.visualstudio.com/api/references/vscode-api#DocumentSymbol}{DocumentSymbolProvider} de l'api VS Code, on constate que l'outline peut être représenté sous deux formes :

\begin{itemize}
    \item SymbolInformation qui est une flat list de tous les symboles trouvés dans un document.
    \item DocumentSymbol qui est une hiérarchie de symboles trouvés dans un document.
\end{itemize}

La structure peut être donc représenté de manière hiérarchique ou seulement être listés.

On a accès à cette représentation dans la sidebar gauche par défaut.

%TODO : picture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/outline-view-sidebar.PNG}}
    \end{center}
    \caption[Outline view disponible depuis la sidebar]{\label{outline-view-sidebar} outline-view-sidebar}
\end{figure}

ou depuis la command palette en saisissant la combinaison "CTRL + SHIFT + o" :

%TODO : picture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=14cm]{assets/figures/outline-view-searchbar.PNG}}
    \end{center}
    \caption[Outline view disponible depuis la searchbar]{\label{outline-view-searchbar} outline-view-searchbar}
\end{figure}

Il est possible obtenir la structure d'un fichier en analysant manuellement le texte (par exemple ligne par ligne).
Mais cette approche n'est pas optimale et il serait difficile de représenter la structure hiérarchique du document à l'aide de regex \cite{antlr-mega-tutorial}.

Nous allons donc utiliser notre parser.

Le résulat du résultat est un "parse tree" (aussi nommé CST) qui est une représentation concrète de notre input.
Le parse tree contient toutes les informations et beaucoup d'entre elles ne nous intéressent pas.
Typiquement les informations grammaticales et structurelles.
C'est pourquoi il faut pouvoir le parcourir pour en extraire un AST.
Un AST est une représentation abstraite de notre input. Il ne contient généralement que les informations les plus pertinentes \cite{cst}.

Par exemple, le parse tree d'une opération arithmétique simple pourrait être représenté par la figure \ref{parse-tree}.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=8cm]{assets/figures/parse-tree.PNG}}
    \end{center}
    \caption[Exemple d'un parse tree]{\label{parse-tree} UON parse tree}
\end{figure}

Son AST pourrait être, par exemple, ce que l'on voit à la figure \ref{ast}.

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/ast.PNG}}
    \end{center}
    \caption[Exemple d'un AST]{\label{ast} UON AST}
\end{figure}

Donc pour générer une telle structure, il va falloir parcourir le parse tree pour récupérer uniquement les informations utiles pendant le parcours.

Pour parcourir un arbre, deux approches sont possibles : Le pattern \emph{Listener} ou \emph{Visitor}.
La première consiste à écouter les types de noeuds traversés qui nous intéressent. Le problème et que cette approche ne nous permet pas de manipuler un objet en cours de route ni gérer le flux d'exécution.
Car il n'est pas possible de communiquer entre les noeuds. Cela peut être tout de même utile si l'on veut afficher la structure de l'arbre dans un terminal par exemple.
Contrairement à la seconde approche, qui nous permet de gérer ces deux cas. Et c'est donc tout naturellement que cette solution a été privilégiée.

\section{DocumentSymbol}

Cependant on ne va pas d'abord créer un AST puis construire la outline view à l'aide de celui-ci.
Nous allors directement construire l'outline view à la volé, pendant le parcours du parse tree. L'outline view sera une représentation directe de notre AST.

L'arbre sera donc un objet VS Code de type \href{https://code.visualstudio.com/api/references/vscode-api#DocumentSymbol}{DocumentSymbol}.

Il y a quelques points intéressants à commenter concernant cet objet.
La variable \emph{children} permet d'avoir des encapsulations d'objets \emph{DocumentSymbol}. C'est ce qui permettra de créer une hiérarchie.
Un noeud doit être représenté par un \emph{Symbolkind}. Nous sommes donc limités sur la représentation et devons faire les ajustements nécessaires.

Il y a deux paramètres de ranges à fournir à la création de l'objet. Leur différence n'est pas forcément très clair.

Le premier se veut plus global que le second.
La première range indique l'entiéreté de la définition d'un objet. Par exemple, pour une classe en Typescript en séléctionnerait le mot \emph{class} si on clique sur le nom de la classe.
Le deuxième range (selectionRange) se veut plus spécifique en ciblant uniquement un seul token.
Dans notre cas les deux peuvent représenter la même chose et représenter la range du token affiché dans l'éditeur.

\section{Visitor}

Lors de la génération des fichiers ANTLR, il est indiqué de générer également un fichier visiteur \emph{UONVisitor} en précisant le tag \emph{- visitor}.

Cela va nous créer un fichier qui contiendra une interface définissant une fonction pour chaque type de noeud de note parseur et retournera un résultat de type générique.
Chacune de ses fonctions prend comme paramètre son contexte.

La classe UonASTVisitor implémente cette interface qui étend la classe \href{https://www.antlr.org/api/Java/org/antlr/v4/runtime/tree/AbstractParseTreeVisitor.html}{AbstractParseTreeVisitor}
pour obtenir le comportement par défaut du pattern visitor. Elle gère le méchanisme de parcours et d'aggrégation des résulats.

Pour obtenir l'ASR, on doit donner le parse tree créé par le parser à la fonction d'entrée \emph{visit} de notre visitor.

\begin{listing}[!ht]
    \begin{minted}{typescript}
        const ast = uonASTVisitor.visit(tree);
    \end{minted}
    %\caption{Utilisation du parser d'ANTLR}
    %\label{Utilisation du parser d'ANTLR}
\end{listing}

De plus note classe \emph{UONVisitor} étant générique, c'est ce qui nous permet de manipuler un objet VS code.
Actuellement la classe surchargée est de type \emph{any} car pendant le parcours un élément peu être de deux natures différentes (un noeud terminal ou un DocumentSymbol).
Cette séparation est faite pour simplifier des étapes de constructions.

\subsection{visitChildren}
La fonction \emph{visitChildren} permet la taversé de l'arbre.

Elle est illustré par la figure \ref{visitChildren}.

\begin{listing}[!ht]
    \begin{minted}{typescript}
    visitChildren(node: RuleNode) {
        let result: any = this.defaultResult();
        let n = node.childCount;
        for (let i = 0; i < n; i++) {
            if (!this.shouldVisitNextChild(node, result)) {
                break;
            }
            let c = node.getChild(i);
            let childResult = c.accept(this);
            result = this.aggregateResult(result, childResult);
        }
        return result;
    }
    \end{minted}
    \caption{Fonction visitChildren }
    \label{visitChildren}
\end{listing}

Le résultat par défaut est une liste vide. Lors du parcours de la remontée dans un noeud parent, nous allons rassembler les enfants dans une liste.

Pourquoi manipuler une liste au lieu d'un objet \emph{DocumentSymbol} directement ?
Tout simplement, car sinon on aurait du le faire dans la fonction d'agrégation et l'on préféra ici déléguer la logique dans le noeud parent concerné
qui pourra utiliser les valeurs récupérer pour construire l'information voulue par exemple dans une séquence ou un mapping.

C'est un choix d'implémentation pour faciliter la construction d'un objet \emph{DocumentSymbol}.

\subsection{Parcours}

Le parcours est effectué selon un algorithme \Gls{dfs}.

Pour rappel un parcours DFS est effectué dans l'ordre affiché à la figure \ref*{Algorithme de parcours en profondeur (DFS)}

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=10cm]{assets/figures/DFS.PNG}}
    \end{center}
    \caption[Algorithme de parcours en profondeur (DFS)]{\label{Algorithme de parcours en profondeur (DFS)} Algorithme de parcours en profondeur (DFS)}
\end{figure}

Ce qui est agréable en utilisant cet algorithme et qu'il nous permet de commencer à construire l'information à partir des noeuds terminaux
de l'arbre et de remonter naturellement au parent le plus proche.

Un parent va pouvoir obtenir la liste des enfants dans un ordre que l'on s'attendrait à recevoir.
Cela rend la manipulation dans un parent plus simple concernant des modifications que l'on voudrait effectuer pour représenter ces informations
dans un objet de type \emph{DocumentSymbol}.

\subsection{La position}

Généralement un noeud nous permet de nous rediriger sur l'élément du fichier lorsqu'on le sélectionne depuis la sidebar ou depuis la search bar. %todo reference
Il faut donc pouvoir être capable de récupérer la position d'un élément du fichier.

Heursement on peut récupérer cette information lors d'un parcours d'un noeud terminal du parse tree.
Il suffit d'observer le token associé au symbole de ce noeud et plus particulièrement les
propriétés \emph{line} et \emph{charPositionInLine} (qui est la position que l'on qualifierait généralement de colonne)
Il faut toutefois juste corriger la position de la ligne en lui soustraient une valeur de 1 car l'indice d'une ligne commence à 0 sur VS Code et 1 pour ANLTR.

\subsection{Résultat}
L'aspect visuel de l'outline view à été conçu pour ressembler à celle de JSON et de YAML.
Suivant cet exemple de code UON basique :

\begin{lstlisting}
    {
        name : "paul",
        age: 18,
        hobbys: {
            music: "rock"
        },
        parents : ["steve", "jeanne"]
    }
\end{lstlisting}

Nous obtiendrons l'outline suivant :

% capture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=6cm]{assets/figures/uon-payload-outline-without-properties.PNG}}
    \end{center}
    \caption[Outline UON sans propriétés]{\label{uon-payload-outline-without-properties} Outline UON sans propriétés}
\end{figure}

Mais UON prenons en compte également les propriétés, il a été jugé pertinents de les rajouter dans la outline.
Pour ne pas changer la structure et ne pas surcharger l'arbre,
nous rajoutons simplement les propriétés des clés et des types comme enfant de l'attribut parent du noeud de l'outline.

Par exemple, si nous rajoutons des propriétés à la clé est un type :
\begin{lstlisting}
    {
        name : "paul",
        age(optional : false): !int( comment : "a comment") 18,
        hobbys: {
            music: "rock"
        },
        parents : ["steve", "jeanne"]
    }
\end{lstlisting}

Nous obtiendrons l'outline suivante :

%capture
\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=6cm]{assets/figures/uon-payload-outline-with-properties.PNG}}
    \end{center}
    \caption[Outline UON avec propriétés]{\label{uon-payload-outline-with-properties} Outline UON avec propriétés}
\end{figure}

La représentation d'un schéma de validation est légèrement différente :

\begin{lstlisting}
    !!mySensor: !schema ( description : "provide great infos", name : ok) {
        name : !str,
        power( optional : true) : !int( min : 5, max : 50)
    }
\end{lstlisting}

Dans un fichier de validation, on ne peut avoir comme valeur uniquement des types ayant ou non des propriétés. Ce sont ces éléments qui nous intéressent le plus.
Donc on a plus de proprités "value props", mais on liste ces propriétés directement comme enfant de l'objet. Cela pour gagner en visibilité.

Nous aurons donc l'outline suivante :

\begin{figure}[!h]
    \begin{center}
        \frame{\includegraphics[width=6cm]{assets/figures/uon-schema-outline.PNG}}
    \end{center}
    \caption[Outline d'un schéma]{\label{uon-schema-outline} Outline d'un schéma}
\end{figure}

\section{Info on Hover}

C'est une fonctionnalité assez triviale qui consiste à afficher une explication lorsque l'on passe le curseur sur un élément.
Cela consiste à aller chercher dans un dictionnaire si la valeur sélectionnée est bien une clé du dictionnaire et de récupérer la valeur si c'est bien le cas.

Ce dictionnaire est un fichier.json qui est importé comme module. Il a été nécessaire de rajouter la ligne \emph{"resolveJsonModule": true} dans le fichier tsconfig.json pour autoriser ce type d'import.

Même si son implémentation est relativement simple, il n'en reste pas moins un élément fortement utile et pouvant être étendu par la suite.
Le code est déjà réutilisé pour fournir la documentation des types lors des suggestions de complétions.

Les informations affiché au survol de la souris d'un élément sont directement tiré de la spécification.

\section{Lint}
%On affiche les erreurs trouvé avec antlr -> avantages positions
%Les erreurs sont liés au placement des tokens (manque un éléments du langages, mauvais token ou leur "ortographe").
%Message trop technique. mais ok pour voir d'ou viens la faute.


Un Linter est un outil d'analyse de code qui permet de détecter les erreurs et les problèmes de syntaxe.

Il est donc nécessaire de pouvoir exploiter correctement les erreurs.

Il est possible de créer un listener qui aura pour rôle d'observer les erreurs lors du parsing avec ANLTR.
Pour cela il faut créer une classe qui implémente l'interface "ANTLRErrorListener" et attribuer ce listener à notre parser
avec la commande :
%#parser.addErrorListener(errorListener);

Cette interface définit la fonction \emph{syntaxError}.

Cette fonction sera trigger comme sans nom l'indique lors d'une erreur de syntaxe.
Une erreur de syntaxe peut survenir quand un élément et mal orthographié ou manquant.

Il est bien de rappeler que certaines erreurs de syntaxes sont plus graves que d'autres.
Même si ANTLR propose des solutions pour se récupérer d'un processus de parsing défectueux, il reste limité et ne peut pas faire de miracle.

Elle contient des paramètres intéressants à exploiter pour informer l'utilisateur.

Sa définition est la suivante :
\begin{lstlisting}
    syntaxError?: <T extends TSymbol>(recognizer: Recognizer<T, any>, offendingSymbol: T | undefined, line: number, charPositionInLine: number, msg: string, e: RecognitionException | undefined) => void;
\end{lstlisting}

%Upon syntax error, notify any interested parties.
%This is not how to recover from errors or compute error messages.
%ANTLRErrorStrategy specifies how to recover from syntax errors and how to compute error messages.
%This listener's job is simply to emit a computed message, though it has enough information to create its own message in many cases.

Comme pour la outline %\ref*{} ,
Il est possible de récupérer la position du token. Il suffit de récupérer les valeurs des propriétés \emph{line} et \emph{charPositionInLine}.
Il s'agira ici d'un token qui a posé problème et l'on voudra afficher à son emplacement l'indication visuelle (soulignement ondulé rouge) ainsi que le message d'erreur associé, lorsque l'utilisateur passe
sa souris dessus.

Le message d'erreur peut-être récupéré à partir de la propriété \emph{msg}. Même s'il est davantage destiné au debuggage.
Les messages restent relativement simples. Ils indiquent à l'utilisateur quel token pose problème et quels sont les caractères qui pourrait y être substituer.

Pour afficher ces informations sur l'éditeur, il faut utiliser un objet VS Code de type \emph{DiagnosticCollection}.

Elle peut afficher une liste d'erreur.

On va garder en mémoire cette liste dans la classe que l'on mettra à jour à chaque fois que le listener détecte une erreur.
Et on actualisera à chaque fois l'objet \emph{DiagnosticCollection} avec cette liste.

Cependant, le parser ne pas se rendre compte si ce n'est pas plutôt le token précédent qui pourrait amener à une erreur.
Cela ne prend non plus pas en compte ce qu'il vient après et donc remplacer un token par un de ceux proposés peut se se révélé contre-productif.
% TODO

\subsection{Suggestion}
Certains messages d'erreurs nous informes parfois des tokens qui devrait se situer à la place de celui qui a causé l'erreur.
Une idée serait donc de pouvoir remplacer ce token erronées par ceux suggérés.
Le problème est que cette information ne se trouve pas sous forme textuelle.
Il faudrait donc pouvoir analyser les fichiers de stratégie et les modifier pour en récupérer la liste des tokens....

% TODO surcharger des méthodes pour pouvoir être notifié des tokens manquant

\subsection{Quick fick}

%  // https://stackoverflow.com/questions/57438198/typescript-element-implicitly-has-an-any-type-because-expression-of-type-st

Actuellement il y a un seul quick fix qui a été implémenté, et cela à titre de découverte.

Ce quickfix permet de supprimer le token qui pose problème en se basant sur les ranges fourni lors de la création d'un objet Diagnostic depuis le listener.

À l'heure actuelle , il n'y a pas encore de suggestions. L'idée serait de pouvoir proposer un ou plusieurs tokens qui aurait du sens pour remplacer celui qui pose problème.
Et l'idéal serait de prendre en compte le texte venant après le curseur. % TODO

\section{Fonctionnalités triviales}
Il s'agit de fonctionnalités couramment disponibles dans un support de langage. Leur implémentation est relativement simple et rapide et n'a donc pas été mentionnée dans le cahier des charges.
Les suivantes ont été implémentées :

% TODO : fichier

\textbf{Comment}
\begin{itemize}
    \item Il est possible de commenter du code sous forme de ligne.
    \item Il est possible de commenter et décommenter du code (Toggling)
\end{itemize}

\textbf{Symbol pairs}
\begin{itemize}
    \item Il est possible de faire la correspondance pour certaines paires de symboles à l'aide d'une indication visuelle. (Matching)
    \item Certains symboles du langage doivent être automatiquement complétés si l'utilisateur saisit le premier élément de celle-ci. (Autoclosing)
\end{itemize}

\textbf{Code folding}
\begin{itemize}
    \item Cacher un bloc de code sur une ligne en fonction de son niveau d'indentation.
\end{itemize}

% TODO
% Couplage de fonctionnalité - logique
% Info on hover - autocompletion
% lint - code outline

\chapter{Planning}

% TODO : Ajouter discussion de l'avancée du travail. Êtes-vous à jour ? Avez-vous pris du retard ?
% TODO : Des implémentations pas compliqué mais peu de documentation sur antlrts dans le cadre d'une extension + faut trouver comment faire en testant parfois
% TODO :
%    Difficulté :
%    Regrouper toutes les sources.
%    Grand projet personnel....
%    Analyse des resultat

\subsection{Diagramme de gantt}
Voici le diagramme de Gantt pour ce projet.

\includepdf[page=1]{assets/planning/gantt.pdf}

\subsection{État actuel du projet}

L'état actuel du projet sur Github (au moment de la rédaction de ce rapport) correspond au commit sur la branche main, avec le hash : 584677bb5c5c215666dd7232ac4fa8fea0c453d4.

Actuellement l'extension est \href{https://marketplace.visualstudio.com/items?itemName=vitorva.vscode-uon}{disponible} sur le marketplace de VS Code.
Elle fournit du support de langage sur une grammaire fonctionnelle UON qui est capable de traiter des mappings et des séquences de type JSON et YAML, ainsi qu'un schéma de validation.

Passons maintenant en revue les tâches convenues dans le cahier des charges :

\begin{itemize}
    \item Une intégration continue (tests et publication automatisé) a été implémenté.
    \item Une coloration syntaxique à été implémenté
    \item De l'auto-complétion à été implémenté
    \item Une outline view a été implémenté
    \item Afficher des informations au survol (information on hover) d'une souris à été implémenté
\end{itemize}

Il resterait à :

\begin{itemize}
    \item Corriger quelques erreurs encore existant
    \item Implémenter les fonctionnalités de la rubrique "si le temps le permet"
\end{itemize}

\chapter{Conclusion}

% Restate your thesis.
% Synthesize or summarize your major points.
% Make the context of your argument clear.

% recherche
% UON
% parser
% ANTLR
% VS Code
%   CI/CD
% Scope de la grammaire
% YAML / JSON
% Fonctionnalites

L'objectif de ce travail de bachelor était le support du langage UON au travers de l'implémentation d'une extension VS Code.

L'idée principale de ce support était qu'il devait mettre à disposition dans l'éditeur, les éléments pour permettre aux mieux la rédaction d'un fichier UON.
Pour éviter de devoir consulter la documentation régulièrement.

Il a donc fallu découvrir et analyser un nouveau langage.
Définir quelles sont les fonctionnalités à implémenter dans un éditeur pouvant s'avérer utile pour ce langage.
Découvrir les technologies permettant leur implémentation et ensuite pouvoir les utiliser en sein de notre application.

Lors de la phase de recherche, il a été appris qu'un parser est un élément important pour fournir du support de langage.
Heursement, des outils existent pour en générer depuis une grammaire. Cela à pour intéret majeur de faire gagner du temps de développement en accélérant grandement la mise en place d'un support de langage.
ANTLR s'est donc avéré être une composante dominante du projet. De plus sa gestion des erreurs fournit par défaut permet d'avoir une meilleure tolérance concernant celles-ci
qui sont fréquentes dans un éditeur.

Nous avons fait le choix de travailler uniquement dans un environnement VS Code mais fournissant toutes les clés pour des extensibilités futures
VS Code est un environnement riche et complexe, mais qui est extrêmement bien documenter et agréable à utiliser.
Par exemple, la mise en place d'une intégration continue (tests et déployement automatisé).
Une constatation général et qu'il est parfois difficile de savoir ce que peut faire cet éditeur par défaut ou non.

Pour pouvoir fournir du support pour un langage, on est obligé de connaitre la grammaire sur laquelle ces fonctionnalités porteront.
Heursement pour nous nous avons pu reprendre une implémentation concrète que nous avons adaptée pour couvrir nos besoins.

Chaque langage ayant ses spécificités, les besoins peuvent variées les uns des autres.
Les mêmes fonctionnalités entre deux langages peuvent avoir des comportements relativement différents.
L'avantage d'avoir travaillé sur un langage de sérialisation comme UON est que cela est relativement moins complexe à gérer qu'un langage de programmation.
Cela nous oblige à découvrir et prendre conscience ce qu'il est possible de faire plus généralement.
Mais UON est aussi plus complexe que ces concurrents sur plein de points.

L'outil de complétion c3, qui peut-être utilisé avec un parseur ANTLR a aussi été une très bonne découverte.

Pour finir, je dirais que c'est un des plus gros projet sur lequel j'ai été ammené à travailler seul.
J'ai donc beaucoup appris et me servirais de ces leçons pour la suite. Travailler sur ce projet s'est révélé être très intéressant.

\section{Difficultés rencontrées}

Même si des extensions fournissant du support de langage existent et sont facilement disponible, j'ai remarqué en analysant leur code source qu'il n'y a pas de consensus sur la manière d'implémenter une fonctionnalité (ex : complétion).
C'est donc difficile de savoir si on effectue la "bonne" méthodologie.

Concernant le générateur de parser ANTLR, c'est est un outil puissant mais assez complexe à prendre en main.
Au début du projet, certaines règles de la grammaire étaient malformées est causaient des erreurs qui n'était pas facilement percéptible.
De plus, comme c'est une implémentation en Typescript qui est utilisée, peu d'exemples existent et cela a compliqué le travail lorsque des erreurs survenaient.

Adapter la grammaire a été un challenge. Il a été necessaire de modifier certaines règles car le comportement par défaut posait parfois problème. ANTLR étant différent de Lark.

La gestion des deux format (JSON et YAML) c'est revelé être assez complexe, dû à la gestion des indentations à traiter nous-même.

Du temps a été passé pour essayer d'améliorer le comportement des erreurs sans succès. Le comportement par défaut n'étant jamais réelement amélioré.

Les fonctionnalités qui ont requis le plus de travail ont été la complétion et l'outline view.
Pour la complétion, même si le moteur est extrêmement puissant et qu'un usage basique convient très bien pour notre utilisation.
Il a été nécessaire d'analyser son fonctionnement pour comprendre ce qu'il était possible de faire.
Un point complexe a été la manière de trouver le bonne indice car des situations particulières pouvait fausser le résultat attendues.

Pour l'outline view, il a fallu comprendre comment construire un objet en cours d'un parcours DFS. Et ajouter la gestion des propriétés avait rajouter une couche de complexité supplémentaires.

Un point contraignant lorsque l'on développe un support de langage, et que la grammaire est intrinsèquement liée à certaines fonctionnalités.
Un changement peut donc avoir des répercussions sur l'ensemble de celles-ci.
J'ai malheureusement, je pense, perdu du temps à corriger ce genres d'erreurs.

La rédaction du rapport a aussi représenté un challenge. Il s'agit d'un travail conséquent étant donné l'ampleur d'un tel projet.

\section{Amélioration possible}

La grammaire peut-être modifié ou étendue davantage selon les besoins des utilisateurs.
Un point intéressant serait d'avoir la possiblité d'avoir un schéma de validation directement intégré au collection pour éviter de devoir travailler sur deux fichiers différents.

Il serait intéressant de déplacer la logique des fonctionnalités de l'extension dans un langage server. Cela pour ne plus dépendre de l'API VS Code et pour permettre de réutiliser ce travail
dans des extensions d'autres éditeurs plus facilement.

Le linter a partiellement été implémenté et se base sur la détection des erreurs d'ANTLR.
Les messages d'erreurs sont ceux qu'ANTLR affiche en cas d'erreur et ne sont très "user-friendly". Il faudrait donc adapter ces messages.
Il faudrait aussi pouvoir proposer des corrections d'erreurs (ex : quickfix) plus avancées.

Le fichier \emph{UonCompletionErrorStrategy.ts} ne fait que surchager le comportement par défaut. Si le comportement doit être amélioré, c'est ce fichier qui doit être modifié.

\section{Erreurs existantes}

La liste des erreurs connues est la suivante :

\begin{itemize}
    \item Imbriquer des mapping ou sequences en format YAML ne fonctionne pas correctement. Cela vient de la gestion des indentations qu'il faudrait corriger.
    \item L'éditeur affiche un caractère erroné (ondulation rouge) au début du document lorsque le fichier est vide. Cela vient aussi de la gestion des indentations.
\end{itemize}



%% \vfil
%% \hspace{8cm}\makeatletter\@author\makeatother\par
%% \hspace{8cm}\begin{minipage}{5cm}
%%if
% Place pour signature numérique
%%\printsignature
%%fi
%% \end{minipage}
%% \clearpage

%% \let\cleardoublepage\clearpage
%% \backmatter

%% TODO : Ne fonctionne pas !!!
\label{glossaire}
\printnoidxglossary
\addcontentsline{toc}{chapter}{Glossaire}

\printbibliography
\addcontentsline{toc}{chapter}{Bibliographie}

%% \label{index}
%% \printindex

\end{document}
